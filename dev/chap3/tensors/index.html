<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tensor Operations · Scientific Computing For Physicists</title><meta name="title" content="Tensor Operations · Scientific Computing For Physicists"/><meta property="og:title" content="Tensor Operations · Scientific Computing For Physicists"/><meta property="twitter:title" content="Tensor Operations · Scientific Computing For Physicists"/><meta name="description" content="Documentation for Scientific Computing For Physicists."/><meta property="og:description" content="Documentation for Scientific Computing For Physicists."/><meta property="twitter:description" content="Documentation for Scientific Computing For Physicists."/><meta property="og:url" content="https://book.jinguo-group.science/chap3/tensors/"/><meta property="twitter:url" content="https://book.jinguo-group.science/chap3/tensors/"/><link rel="canonical" href="https://book.jinguo-group.science/chap3/tensors/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Scientific Computing For Physicists</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Become an Open-source Developer</span><ul><li><a class="tocitem" href="../../chap1/terminal/">Get a Terminal!</a></li><li><a class="tocitem" href="../../chap1/git/">Maintainability - Version Control</a></li><li><a class="tocitem" href="../../chap1/ci/">Correctness - Unit Tests</a></li></ul></li><li><span class="tocitem">Julia Programming Language</span><ul><li><a class="tocitem" href="../../chap2/julia-setup/">Setup Julia</a></li><li><a class="tocitem" href="../../chap2/julia-why/">Why Julia?</a></li><li><a class="tocitem" href="../../chap2/julia-type/">Types and Multiple-dispatch</a></li><li><a class="tocitem" href="../../chap2/julia-array/">Array and Broadcasting</a></li><li><a class="tocitem" href="../../chap2/julia-release/">My First Package</a></li><li><a class="tocitem" href="../../chap2/julia-fluid/">Project: Fluid dynamics</a></li></ul></li><li><span class="tocitem">Linear Algebra</span><ul><li><a class="tocitem" href="../linalg/">Matrix Computation</a></li><li><a class="tocitem" href="../lu/">Solving linear equations by LU factorization: Bottom-up</a></li><li><a class="tocitem" href="../qr/">QR Factorization: Bottom-up</a></li><li><a class="tocitem" href="../fft/">Fast Fourier transform</a></li><li><a class="tocitem" href="../sensitivity/">Sensitivity Analysis</a></li><li><a class="tocitem" href="../sparse/">Sparse Matrices and Graphs</a></li></ul></li><li><span class="tocitem">Tensors and Tensor Networks</span><ul><li class="is-active"><a class="tocitem" href>Tensor Operations</a><ul class="internal"><li><a class="tocitem" href="#Background"><span>Background</span></a></li><li><a class="tocitem" href="#Einsum-notation"><span>Einsum notation</span></a></li><li><a class="tocitem" href="#The-spin-glass-problem"><span>The spin-glass problem</span></a></li><li><a class="tocitem" href="#The-backward-rule-of-tensor-contraction"><span>The backward rule of tensor contraction</span></a></li><li class="toplevel"><a class="tocitem" href="#Probability-graph"><span>Probability graph</span></a></li><li><a class="tocitem" href="#The-partition-function"><span>The partition function</span></a></li></ul></li></ul></li><li><span class="tocitem">Optimization</span><ul><li><a class="tocitem" href="../../chap4/optimization/">Optimization</a></li><li><a class="tocitem" href="../../chap4/ad/">Automatic Differentiation</a></li></ul></li><li><span class="tocitem">Randomness</span><ul><li><a class="tocitem" href="../../chap5/montecarlo/">Markov Chain Monte Carlo</a></li></ul></li><li><span class="tocitem">Appendix</span><ul><li><a class="tocitem" href="../../append/plotting/">Plotting recipes with CairoMakie</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tensors and Tensor Networks</a></li><li class="is-active"><a href>Tensor Operations</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tensor Operations</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/GiggleLiu/ScientificComputingForPhysicists" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/GiggleLiu/ScientificComputingForPhysicists/blob/main/docs/src/chap3/tensors.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Tensor-Operations"><a class="docs-heading-anchor" href="#Tensor-Operations">Tensor Operations</a><a id="Tensor-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-Operations" title="Permalink"></a></h1><h2 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h2><p>Tensor networks serve as a fundamental tool for modeling and analyzing correlated systems. This section reviews the fundamental concepts of tensor networks.</p><p>A tensor is a mathematical object that generalizes scalars, vectors, and matrices. It can have multiple dimensions and is used to represent data in various mathematical and physical contexts. It is formally defined as follows:</p><p><em>Definition</em> (Tensor): A tensor <span>$T$</span> associated to a set of discrete variables <span>$V$</span> is defined as a function that maps each possible instantiation of the variables in its scope <span>$\mathcal{D}_V = \prod_{v\in V} \mathcal{D}_{v}$</span> to an element in the set <span>$\mathcal{E}$</span>, given by</p><p class="math-container">\[T_{V}: \prod_{v \in V} \mathcal{D}_{v} \rightarrow \mathcal{E}.\]</p><p>Within the context of probabilistic modeling, the elements in <span>$\mathcal{E}$</span> are non-negative real numbers, while in other scenarios, they can be of generic types.</p><p><em>Definition</em> (Tensor Network): A tensor network is a mathematical framework for defining multilinear maps, which can be represented by a triple <span>$\mathcal{N} = (\Lambda, \mathcal{T}, V_0)$</span>, where:</p><ul><li><span>$\Lambda$</span> is the set of variables present in the network <span>$\mathcal{N}$</span>.</li><li><span>$\mathcal{T} = \{ T_{V_k} \}_{k=1}^{K}$</span> is the set of input tensors, where each tensor <span>$T_{V_k}$</span> is associated with the labels <span>$V_k$</span>.</li><li><span>$V_0$</span> specifies the labels of the output tensor.</li></ul><p>Specifically, each tensor <span>$T_{V_k} \in \mathcal{T}$</span> is labeled by a set of variables <span>$V_k \subseteq \Lambda$</span>, where the cardinality <span>$|V_k|$</span> equals the rank of <span>$T_{V_k}$</span>. The multilinear map, or the \textbf{contraction}, applied to this triple is defined as</p><p class="math-container">\[T_{V_0} = \texttt{contract}(\Lambda, \mathcal{T}, V_0) \overset{\mathrm{def}}{=} \sum_{m \in \mathcal{D}_{\Lambda\setminus V_0}} \prod_{T_V \in \mathcal{T}} T_{V|M=m},\]</p><p>where <span>$M = \Lambda \setminus V_0$</span>. <span>$T_{V|M=m}$</span> denotes a slicing of the tensor <span>$T_{V}$</span> with the variables <span>$M$</span> fixed to the values <span>$m$</span>. The summation runs over all possible configurations of the variables in <span>$M$</span>.</p><p>For instance, matrix multiplication can be described as the contraction of a tensor network given by</p><p class="math-container">\[(AB)_{\{i, k\}} = \texttt{contract}\left(\{i,j,k\}, \{A_{\{i, j\}}, B_{\{j, k\}}\}, \{i, k\}\right),\]</p><p>where matrices <span>$A$</span> and <span>$B$</span> are input tensors containing the variable sets <span>$\{i, j\}, \{j, k\}$</span>, respectively, which are subsets of <span>$\Lambda = \{i, j, k\}$</span>. The output tensor is comprised of variables <span>$\{i, k\}$</span> and the summation runs over variables <span>$\Lambda \setminus \{i, k\} = \{j\}$</span>. The contraction corresponds to</p><p class="math-container">\[(A B)_{\{i, k\}} = \sum_j A_{\{i,j\}}B_{\{j, k\}}.\]</p><p>Diagrammatically, a tensor network can be represented as an <em>open hypergraph</em>, where each tensor is mapped to a vertex and each variable is mapped to a hyperedge. Two vertices are connected by the same hyperedge if and only if they share a common variable. The diagrammatic representation of the matrix multiplication is given as follows: </p><p><img src="../../assets/images/matmul.png" alt/></p><p>Here, we use different colors to denote different hyperedges. Hyperedges for <span>$i$</span> and <span>$k$</span> are left open to denote variables of the output tensor. A slightly more complex example of this is the star contraction:</p><p class="math-container">\[\texttt{contract}(\{i,j,k,l\}, \{A_{\{i, l\}}, B_{\{j, l\}}, C_{\{k, l\}}\}, \{i,j,k\}) \\
= \sum_{l}A_{\{i,l\}} B_{\{j,l\}} C_{\{k,l\}}.\]</p><p>Note that the variable <span>$l$</span> is shared by all three tensors, making regular edges, which by definition connect two nodes, insufficient for its representation. This motivates the need for hyperedges, which can connect a single variable to any number of nodes. The hypergraph representation is given as:</p><p><img src="../../assets/images/starcontract.png" alt/></p><h2 id="Einsum-notation"><a class="docs-heading-anchor" href="#Einsum-notation">Einsum notation</a><a id="Einsum-notation-1"></a><a class="docs-heading-anchor-permalink" href="#Einsum-notation" title="Permalink"></a></h2><p>The einsum notation is a compact way to specify tensor contractions with a string. In this notation, an index (subscripts) is represented by a char, and the tensors are represented by the indices. The input tensors and the output tensor are separated by an arrow <code>-&gt;</code> and input tensors are separated by comma <code>,</code>. For example, the matrix multiplication <span>$\left(\{i,j,k\}, \{A_{\{i, j\}}, B_{\{j, k\}}\}, \{i, k\}\right)$</span> can be concisely written as <code>&quot;ij,jk-&gt;ik&quot;</code>. A general contraction can be defined with pseudocode as follows:</p><pre><code class="nohighlight hljs">Let A, B, C, ... be input tensors, O be the output tensor
for indices in domain_of_unique_indices(einsum_notation)
    O[indices in O] += A[indices in A] * B[indices in B] * ...
end</code></pre><p>In the following example, we demonstrate the einsum notation for matrix multiplication and other tensor operations.</p><div class="admonition is-info"><header class="admonition-header">Example - Einsum notation</header><div class="admonition-body"><p>We first define the tensors and then demonstrate the einsum notation for various tensor operations.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using OMEinsum</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: ArgumentError: Package OMEinsum not found in current path.
- Run `import Pkg; Pkg.add(&quot;OMEinsum&quot;)` to install the OMEinsum package.</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; s = fill(1)  # scalar</code><code class="nohighlight hljs ansi" style="display:block;">0-dimensional Array{Int64, 0}:
1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; w, v = [1, 2], [4, 5];  # vectors</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; A, B = [1 2; 3 4], [5 6; 7 8]; # matrices</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; T1, T2 = reshape(1:8, 2, 2, 2), reshape(9:16, 2, 2, 2); # 3D tensor</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Unary examples:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;i-&gt;&quot;(w)  # sum of the elements of a vector.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[1]:1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij-&gt;i&quot;(A)  # sum of the rows of a matrix.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[2]:1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ii-&gt;&quot;(A)  # sum of the diagonal elements of a matrix, i.e., the trace.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[3]:1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij-&gt;&quot;(A)  # sum of the elements of a matrix.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[4]:1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;i-&gt;ii&quot;(w)  # create a diagonal matrix.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[5]:1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;i-&gt;ij&quot;(w; size_info=Dict(&#39;j&#39;=&gt;2))  # repeat a vector to form a matrix.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[6]:1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ijk-&gt;ikj&quot;(T1)  # permute the dimensions of a tensor.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[7]:1</code></pre><p>Binary examples:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij, jk -&gt; ik&quot;(A, B)  # matrix multiplication.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[1]:1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ijb,jkb-&gt;ikb&quot;(T1, T2)  # batch matrix multiplication.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[2]:1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij,ij-&gt;ij&quot;(A, B)  # element-wise multiplication.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[3]:1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij,ij-&gt;&quot;(A, B)  # sum of the element-wise multiplication.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[4]:1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij,-&gt;ij&quot;(A, s)  # element-wise multiplication by a scalar.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[5]:1</code></pre><p>Nary examples:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ai,aj,ak-&gt;ijk&quot;(A, A, B)  # star contraction.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[1]:1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ia,ajb,bkc,cld,dm-&gt;ijklm&quot;(A, T1, T2, T1, A)  # tensor train contraction.</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: LoadError: UndefVarError: `@ein_str` not defined
in expression starting at REPL[2]:1</code></pre></div></div><h2 id="The-spin-glass-problem"><a class="docs-heading-anchor" href="#The-spin-glass-problem">The spin-glass problem</a><a id="The-spin-glass-problem-1"></a><a class="docs-heading-anchor-permalink" href="#The-spin-glass-problem" title="Permalink"></a></h2><p>The spin-glass problem is a combinatorial optimization problem that is widely used in physics, computer science, and mathematics. The problem is to find the ground state of a spin-glass Hamiltonian, which is a function of the spin configuration. The Hamiltonian is defined as</p><p class="math-container">\[H(\sigma) = -\sum_{i,j} J_{ij} \sigma_i \sigma_j + \sum_i h_i \sigma_i,\]</p><p>where <span>$\sigma_i \in \{-1, 1\}$</span> is the spin variable, <span>$J_{ij}$</span> is the coupling strength between spins <span>$i$</span> and <span>$j$</span>, and <span>$h_i$</span> is the external field acting on spin <span>$i$</span>. The first term is the interaction energy between spins, and the second term is the energy due to the external field. The ground state is the spin configuration that minimizes the Hamiltonian.</p><img src="../../assets/images/spinglass.png" width="400" /><h3 id="Partition-function"><a class="docs-heading-anchor" href="#Partition-function">Partition function</a><a id="Partition-function-1"></a><a class="docs-heading-anchor-permalink" href="#Partition-function" title="Permalink"></a></h3><p>The thermal equilibrium of the spin-glass system is described by the Boltzmann distribution</p><p class="math-container">\[P(\sigma) = \frac{1}{Z} e^{-\beta H(\sigma)},\]</p><p>where <span>$\beta = 1/T$</span> is the inverse temperature, and <span>$Z$</span> is the partition function</p><p class="math-container">\[Z = \sum_{\sigma} e^{-\beta H(\sigma)}.\]</p><p>The partition function is the normalization constant that ensures the probability distribution sums to one. The partition function is a sum over all possible spin configurations, which makes it computationally intractable for large systems.</p><p>The partition function can be expressed as a tensor contraction using the einsum notation. The partition function is a sum over all possible spin configurations, which can be represented as a tensor contraction over the spins. The partition function can be written as</p><p class="math-container">\[Z = \sum_{\sigma} e^{-\beta H(\sigma)} = \sum_{\sigma} e^{\beta \sum_{i,j} J_{ij} \sigma_i \sigma_j + \sum_i h_i \sigma_i} = \sum_{\sigma} \prod_{i,j} e^{\beta J_{ij} \sigma_i \sigma_j} \prod_i e^{h_i \sigma_i}.\]</p><h3 id="Best-configuration"><a class="docs-heading-anchor" href="#Best-configuration">Best configuration</a><a id="Best-configuration-1"></a><a class="docs-heading-anchor-permalink" href="#Best-configuration" title="Permalink"></a></h3><h3 id="Landscape"><a class="docs-heading-anchor" href="#Landscape">Landscape</a><a id="Landscape-1"></a><a class="docs-heading-anchor-permalink" href="#Landscape" title="Permalink"></a></h3><h2 id="The-backward-rule-of-tensor-contraction"><a class="docs-heading-anchor" href="#The-backward-rule-of-tensor-contraction">The backward rule of tensor contraction</a><a id="The-backward-rule-of-tensor-contraction-1"></a><a class="docs-heading-anchor-permalink" href="#The-backward-rule-of-tensor-contraction" title="Permalink"></a></h2><p>The backward rule for matrix multiplication is</p><ul><li><code>C = ein&quot;ij,jk-&gt;ik&quot;(A, B)</code><ul><li><code>̄A = ein&quot;ik,jk-&gt;ij&quot;(̄C, B)</code></li><li><code>̄B = ein&quot;ik,jk-&gt;ij&quot;(A, ̄C)</code></li></ul></li><li><code>v = ein&quot;ii-&gt;i&quot;(A)</code><ul><li><code>̄A = ein&quot;?&quot;(̄v)</code></li></ul></li></ul><h1 id="Probability-graph"><a class="docs-heading-anchor" href="#Probability-graph">Probability graph</a><a id="Probability-graph-1"></a><a class="docs-heading-anchor-permalink" href="#Probability-graph" title="Permalink"></a></h1><table><tr><th style="text-align: center"><strong>Random variable</strong></th><th style="text-align: left"><strong>Meaning</strong></th></tr><tr><td style="text-align: center">A</td><td style="text-align: left">Recent trip to Asia</td></tr><tr><td style="text-align: center">T</td><td style="text-align: left">Patient has tuberculosis</td></tr><tr><td style="text-align: center">S</td><td style="text-align: left">Patient is a smoker</td></tr><tr><td style="text-align: center">L</td><td style="text-align: left">Patient has lung cancer</td></tr><tr><td style="text-align: center">B</td><td style="text-align: left">Patient has bronchitis</td></tr><tr><td style="text-align: center">E</td><td style="text-align: left">Patient hast T and/or L</td></tr><tr><td style="text-align: center">X</td><td style="text-align: left">Chest X-Ray is positive</td></tr><tr><td style="text-align: center">D</td><td style="text-align: left">Patient has dyspnoea</td></tr></table><p>A probabilistic graphical model (PGM) illustrates the mathematical modeling of reasoning in the presence of uncertainty. Bayesian networks (above) and Markov random fields are popular types of PGMs. Consider the Bayesian network shown in the figure above known as the <em>ASIA network</em>. It is a simplified example from the context of medical diagnosis that describes the probabilistic relationships between different random variables corresponding to possible diseases, symptoms, risk factors and test results. It consists of a graph <span>$G = (V,\mathcal{E})$</span> and a probability distribution <span>$P(V)$</span> where <span>$G$</span> is a directed acyclic graph, <span>$V$</span> is the set of variables and <span>$\mathcal{E}$</span> is the set of edges connecting the variables. We assume all variables to be discrete (0 or 1). Each variable <span>$v \in V$</span> is quantified with a <em>conditional probability distribution</em> <span>$P(v \mid pa(v))$</span> where <span>$pa(v)$</span> are the parents of <span>$v$</span>. These conditional probability distributions together with the graph <span>$G$</span> induce a <em>joint probability distribution</em> over <span>$P(V)$</span>, given by</p><p class="math-container">\[P(V) = \prod_{v\in V} P(v \mid pa(v)).\]</p><h2 id="The-partition-function"><a class="docs-heading-anchor" href="#The-partition-function">The partition function</a><a id="The-partition-function-1"></a><a class="docs-heading-anchor-permalink" href="#The-partition-function" title="Permalink"></a></h2><p><a href="https://uaicompetition.github.io/uci-2022/competition-entry/tasks/">https://uaicompetition.github.io/uci-2022/competition-entry/tasks/</a></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../sparse/">« Sparse Matrices and Graphs</a><a class="docs-footer-nextpage" href="../../chap4/optimization/">Optimization »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Friday 5 April 2024 16:21">Friday 5 April 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
