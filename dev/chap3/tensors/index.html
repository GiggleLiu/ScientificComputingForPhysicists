<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tensor Operations · Scientific Computing For Physicists</title><meta name="title" content="Tensor Operations · Scientific Computing For Physicists"/><meta property="og:title" content="Tensor Operations · Scientific Computing For Physicists"/><meta property="twitter:title" content="Tensor Operations · Scientific Computing For Physicists"/><meta name="description" content="Documentation for Scientific Computing For Physicists."/><meta property="og:description" content="Documentation for Scientific Computing For Physicists."/><meta property="twitter:description" content="Documentation for Scientific Computing For Physicists."/><meta property="og:url" content="https://book.jinguo-group.science/chap3/tensors/"/><meta property="twitter:url" content="https://book.jinguo-group.science/chap3/tensors/"/><link rel="canonical" href="https://book.jinguo-group.science/chap3/tensors/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Scientific Computing For Physicists</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Become an Open-source Developer</span><ul><li><a class="tocitem" href="../../chap1/terminal/">Get a Terminal!</a></li><li><a class="tocitem" href="../../chap1/git/">Maintainability - Version Control</a></li><li><a class="tocitem" href="../../chap1/ci/">Correctness - Unit Tests</a></li></ul></li><li><span class="tocitem">Julia Programming Language</span><ul><li><a class="tocitem" href="../../chap2/julia-setup/">Setup Julia</a></li><li><a class="tocitem" href="../../chap2/julia-why/">Why Julia?</a></li><li><a class="tocitem" href="../../chap2/julia-type/">Types and Multiple-dispatch</a></li><li><a class="tocitem" href="../../chap2/julia-array/">Array and Broadcasting</a></li><li><a class="tocitem" href="../../chap2/julia-release/">My First Package</a></li><li><a class="tocitem" href="../../chap2/julia-fluid/">Project: Fluid dynamics</a></li></ul></li><li><span class="tocitem">Linear Algebra</span><ul><li><a class="tocitem" href="../linalg/">Matrix Computation</a></li><li><a class="tocitem" href="../lu/">Solving linear equations by LU factorization: Bottom-up</a></li><li><a class="tocitem" href="../qr/">QR Factorization: Bottom-up</a></li><li><a class="tocitem" href="../fft/">Fast Fourier transform</a></li><li><a class="tocitem" href="../sensitivity/">Sensitivity Analysis</a></li><li><a class="tocitem" href="../sparse/">Sparse Matrices and Graphs</a></li></ul></li><li><span class="tocitem">Tensors and Tensor Networks</span><ul><li class="is-active"><a class="tocitem" href>Tensor Operations</a><ul class="internal"><li><a class="tocitem" href="#Background"><span>Background</span></a></li><li><a class="tocitem" href="#Einsum-notation"><span>Einsum notation</span></a></li><li><a class="tocitem" href="#The-spin-glass-problem"><span>The spin-glass problem</span></a></li><li><a class="tocitem" href="#Ground-state-finding"><span>Ground state finding</span></a></li><li><a class="tocitem" href="#Tensor-network-contraction"><span>Tensor network contraction</span></a></li><li><a class="tocitem" href="#The-backward-rule-of-tensor-contraction"><span>The backward rule of tensor contraction</span></a></li><li class="toplevel"><a class="tocitem" href="#Probability-graph"><span>Probability graph</span></a></li><li><a class="tocitem" href="#The-partition-function"><span>The partition function</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li></ul></li><li><span class="tocitem">Optimization</span><ul><li><a class="tocitem" href="../../chap4/optimization/">Optimization</a></li><li><a class="tocitem" href="../../chap4/ad/">Automatic Differentiation</a></li><li><a class="tocitem" href="../../chap5/complexity/">Computational complexity</a></li></ul></li><li><span class="tocitem">Randomness</span><ul><li><a class="tocitem" href="../../chap5/montecarlo/">Markov Chain Monte Carlo</a></li></ul></li><li><span class="tocitem">Appendix</span><ul><li><a class="tocitem" href="../../append/plotting/">Plotting recipes with CairoMakie</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tensors and Tensor Networks</a></li><li class="is-active"><a href>Tensor Operations</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tensor Operations</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/GiggleLiu/ScientificComputingForPhysicists" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/GiggleLiu/ScientificComputingForPhysicists/blob/main/docs/src/chap3/tensors.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Tensor-Operations"><a class="docs-heading-anchor" href="#Tensor-Operations">Tensor Operations</a><a id="Tensor-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-Operations" title="Permalink"></a></h1><h2 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h2><p>Tensor networks serve as a fundamental tool for modeling and analyzing correlated systems. This section reviews the fundamental concepts of tensor networks.</p><p>A tensor is a mathematical object that generalizes scalars, vectors, and matrices. It can have multiple dimensions and is used to represent data in various mathematical and physical contexts. It is formally defined as follows:</p><p><em>Definition</em> (Tensor): A tensor <span>$T$</span> associated to a set of discrete variables <span>$V$</span> is defined as a function that maps each possible instantiation of the variables in its scope <span>$\mathcal{D}_V = \prod_{v\in V} \mathcal{D}_{v}$</span> to an element in the set <span>$\mathcal{E}$</span>, given by</p><p class="math-container">\[T_{V}: \prod_{v \in V} \mathcal{D}_{v} \rightarrow \mathcal{E}.\]</p><p>Within the context of probabilistic modeling, the elements in <span>$\mathcal{E}$</span> are non-negative real numbers, while in other scenarios, they can be of generic types. The diagrammatic representation of a tensor is given by a node with the variables <span>$V$</span> as labels on its edges, as shown below:</p><p><img src="../../assets/images/tensors.svg" alt/></p><p><em>Definition</em> (Tensor Network): A tensor network is a mathematical framework for defining multilinear maps, which can be represented by a triple <span>$\mathcal{N} = (\Lambda, \mathcal{T}, V_0)$</span>, where:</p><ul><li><span>$\Lambda$</span> is the set of variables present in the network <span>$\mathcal{N}$</span>.</li><li><span>$\mathcal{T} = \{ T_{V_k} \}_{k=1}^{K}$</span> is the set of input tensors, where each tensor <span>$T_{V_k}$</span> is associated with the labels <span>$V_k$</span>.</li><li><span>$V_0$</span> specifies the labels of the output tensor.</li></ul><p>Specifically, each tensor <span>$T_{V_k} \in \mathcal{T}$</span> is labeled by a set of variables <span>$V_k \subseteq \Lambda$</span>, where the cardinality <span>$|V_k|$</span> equals the rank of <span>$T_{V_k}$</span>. The multilinear map, or the <strong>contraction</strong>, applied to this triple is defined as</p><p class="math-container">\[T_{V_0} = \texttt{contract}(\Lambda, \mathcal{T}, V_0) \overset{\mathrm{def}}{=} \sum_{m \in \mathcal{D}_{\Lambda\setminus V_0}} \prod_{T_V \in \mathcal{T}} T_{V|M=m},\]</p><p>where <span>$M = \Lambda \setminus V_0$</span>. <span>$T_{V|M=m}$</span> denotes a slicing of the tensor <span>$T_{V}$</span> with the variables <span>$M$</span> fixed to the values <span>$m$</span>. The summation runs over all possible configurations of the variables in <span>$M$</span>.</p><p>For instance, matrix multiplication can be described as the contraction of a tensor network given by</p><p class="math-container">\[(AB)_{\{i, k\}} = \texttt{contract}\left(\{i,j,k\}, \{A_{\{i, j\}}, B_{\{j, k\}}\}, \{i, k\}\right),\]</p><p>where matrices <span>$A$</span> and <span>$B$</span> are input tensors containing the variable sets <span>$\{i, j\}, \{j, k\}$</span>, respectively, which are subsets of <span>$\Lambda = \{i, j, k\}$</span>. The output tensor is comprised of variables <span>$\{i, k\}$</span> and the summation runs over variables <span>$\Lambda \setminus \{i, k\} = \{j\}$</span>. The contraction corresponds to</p><p class="math-container">\[(A B)_{\{i, k\}} = \sum_j A_{\{i,j\}}B_{\{j, k\}}.\]</p><p>Diagrammatically, a tensor network can be represented as an <em>open hypergraph</em>, where each tensor is mapped to a vertex and each variable is mapped to a hyperedge. Two vertices are connected by the same hyperedge if and only if they share a common variable. The diagrammatic representation of the matrix multiplication is given as follows: </p><p><img src="../../assets/images/matmul.png" alt/></p><p>Here, we use different colors to denote different hyperedges. Hyperedges for <span>$i$</span> and <span>$k$</span> are left open to denote variables of the output tensor. A slightly more complex example of this is the star contraction:</p><p class="math-container">\[\texttt{contract}(\{i,j,k,l\}, \{A_{\{i, l\}}, B_{\{j, l\}}, C_{\{k, l\}}\}, \{i,j,k\}) \\
= \sum_{l}A_{\{i,l\}} B_{\{j,l\}} C_{\{k,l\}}.\]</p><p>Note that the variable <span>$l$</span> is shared by all three tensors, making regular edges, which by definition connect two nodes, insufficient for its representation. This motivates the need for hyperedges, which can connect a single variable to any number of nodes. The hypergraph representation is given as:</p><p><img src="../../assets/images/starcontract.png" alt/></p><h2 id="Einsum-notation"><a class="docs-heading-anchor" href="#Einsum-notation">Einsum notation</a><a id="Einsum-notation-1"></a><a class="docs-heading-anchor-permalink" href="#Einsum-notation" title="Permalink"></a></h2><p>The einsum notation is a compact way to specify tensor contractions with a string. In this notation, an index (subscripts) is represented by a char, and the tensors are represented by the indices. The input tensors and the output tensor are separated by an arrow <code>-&gt;</code> and input tensors are separated by comma <code>,</code>. For example, the matrix multiplication <span>$\left(\{i,j,k\}, \{A_{\{i, j\}}, B_{\{j, k\}}\}, \{i, k\}\right)$</span> can be concisely written as <code>&quot;ij,jk-&gt;ik&quot;</code>. A general contraction can be defined with pseudocode as follows:</p><pre><code class="nohighlight hljs">Let A, B, C, ... be input tensors, O be the output tensor
for indices in domain_of_unique_indices(einsum_notation)
    O[indices in O] += A[indices in A] * B[indices in B] * ...
end</code></pre><p>In the following example, we demonstrate the einsum notation for matrix multiplication and other tensor operations.</p><div class="admonition is-info"><header class="admonition-header">Example - Einsum notation</header><div class="admonition-body"><p>We first define the tensors and then demonstrate the einsum notation for various tensor operations.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using OMEinsum</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; s = fill(1)  # scalar</code><code class="nohighlight hljs ansi" style="display:block;">0-dimensional Array{Int64, 0}:
1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; w, v = [1, 2], [4, 5];  # vectors</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; A, B = [1 2; 3 4], [5 6; 7 8]; # matrices</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; T1, T2 = reshape(1:8, 2, 2, 2), reshape(9:16, 2, 2, 2); # 3D tensor</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Unary examples:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;i-&gt;&quot;(w)  # sum of the elements of a vector.</code><code class="nohighlight hljs ansi" style="display:block;">0-dimensional Array{Int64, 0}:
3</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij-&gt;i&quot;(A)  # sum of the rows of a matrix.</code><code class="nohighlight hljs ansi" style="display:block;">2-element Vector{Int64}:
 3
 7</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ii-&gt;&quot;(A)  # sum of the diagonal elements of a matrix, i.e., the trace.</code><code class="nohighlight hljs ansi" style="display:block;">0-dimensional Array{Int64, 0}:
5</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij-&gt;&quot;(A)  # sum of the elements of a matrix.</code><code class="nohighlight hljs ansi" style="display:block;">0-dimensional Array{Int64, 0}:
10</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;i-&gt;ii&quot;(w)  # create a diagonal matrix.</code><code class="nohighlight hljs ansi" style="display:block;">2×2 Matrix{Int64}:
 1  0
 0  2</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;i-&gt;ij&quot;(w; size_info=Dict(&#39;j&#39;=&gt;2))  # repeat a vector to form a matrix.</code><code class="nohighlight hljs ansi" style="display:block;">2×2 Matrix{Int64}:
 1  1
 2  2</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ijk-&gt;ikj&quot;(T1)  # permute the dimensions of a tensor.</code><code class="nohighlight hljs ansi" style="display:block;">2×2×2 Array{Int64, 3}:
[:, :, 1] =
 1  5
 2  6

[:, :, 2] =
 3  7
 4  8</code></pre><p>Binary examples:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij, jk -&gt; ik&quot;(A, B)  # matrix multiplication.</code><code class="nohighlight hljs ansi" style="display:block;">2×2 Matrix{Int64}:
 19  22
 43  50</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ijb,jkb-&gt;ikb&quot;(T1, T2)  # batch matrix multiplication.</code><code class="nohighlight hljs ansi" style="display:block;">2×2×2 Array{Int64, 3}:
[:, :, 1] =
 39  47
 58  70

[:, :, 2] =
 163  187
 190  218</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij,ij-&gt;ij&quot;(A, B)  # element-wise multiplication.</code><code class="nohighlight hljs ansi" style="display:block;">2×2 Matrix{Int64}:
  5  12
 21  32</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij,ij-&gt;&quot;(A, B)  # sum of the element-wise multiplication.</code><code class="nohighlight hljs ansi" style="display:block;">0-dimensional Array{Int64, 0}:
70</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij,-&gt;ij&quot;(A, s)  # element-wise multiplication by a scalar.</code><code class="nohighlight hljs ansi" style="display:block;">2×2 Matrix{Int64}:
 1  2
 3  4</code></pre><p>Nary examples:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ai,aj,ak-&gt;ijk&quot;(A, A, B)  # star contraction.</code><code class="nohighlight hljs ansi" style="display:block;">2×2×2 Array{Int64, 3}:
[:, :, 1] =
 68   94
 94  132

[:, :, 2] =
  78  108
 108  152</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ia,ajb,bkc,cld,dm-&gt;ijklm&quot;(A, T1, T2, T1, A)  # tensor train contraction.</code><code class="nohighlight hljs ansi" style="display:block;">2×2×2×2×2 Array{Int64, 5}:
[:, :, 1, 1, 1] =
  9500  14564
 21604  33420

[:, :, 2, 1, 1] =
 11084  17012
 25204  39036

[:, :, 1, 2, 1] =
 13644  20916
 31028  47996

[:, :, 2, 2, 1] =
 15932  24452
 36228  56108

[:, :, 1, 1, 2] =
 13214  20258
 30050  46486

[:, :, 2, 1, 2] =
 15414  23658
 35050  54286

[:, :, 1, 2, 2] =
 19430  29786
 44186  68350

[:, :, 2, 2, 2] =
 22686  34818
 51586  79894</code></pre></div></div><div class="admonition is-info"><header class="admonition-header">Example: Trace under cyclic permutation</header><div class="admonition-body"><p>Consider 3 matrices <span>$A, B, C$</span> and the cyclic permutation of the trace <span>$\text{Tr}(ABC)$</span>. The trace of a product of matrices is invariant under cyclic permutations, i.e., <span>$\text{Tr}(ABC) = \text{Tr}(CAB) = \text{Tr}(BCA)$</span>. This can be verified using the einsum diagram.</p><p><img src="../../assets/images/perm.svg" alt/></p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; A, B, C = (randn(2, 2) for i=1:3)</code><code class="nohighlight hljs ansi" style="display:block;">Base.Generator{UnitRange{Int64}, Main.var&quot;#1#2&quot;}(Main.var&quot;#1#2&quot;(), 1:3)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij,jk,ik-&gt;&quot;(A, B, C) ≈ ein&quot;jk,ik,ij-&gt;&quot;(B, C, A)</code><code class="nohighlight hljs ansi" style="display:block;">true</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ein&quot;ij,jk,ik-&gt;&quot;(A, B, C) ≈ ein&quot;ik,ij,jk-&gt;&quot;(C, A, B)</code><code class="nohighlight hljs ansi" style="display:block;">true</code></pre></div></div><h2 id="The-spin-glass-problem"><a class="docs-heading-anchor" href="#The-spin-glass-problem">The spin-glass problem</a><a id="The-spin-glass-problem-1"></a><a class="docs-heading-anchor-permalink" href="#The-spin-glass-problem" title="Permalink"></a></h2><p>The spin-glass problem is a combinatorial optimization problem that is widely used in physics, computer science, and mathematics. The problem is to find the ground state of a spin-glass Hamiltonian, which is a function of the spin configuration. The Hamiltonian is defined as</p><p class="math-container">\[H(\sigma) = \sum_{i,j} J_{ij} \sigma_i \sigma_j + \sum_i h_i \sigma_i,\]</p><p>where <span>$\sigma_i \in \{-1, 1\}$</span> is the spin variable, <span>$J_{ij}$</span> is the coupling strength between spins <span>$i$</span> and <span>$j$</span>, and <span>$h_i$</span> is the external field acting on spin <span>$i$</span>. The first term is the interaction energy between spins, and the second term is the energy due to the external field. The ground state is the spin configuration that minimizes the Hamiltonian.</p><p>The topology of the spin-glass system is typically represented as a graph, where the spins are the nodes and the couplings are the edges. The graph can be a complete graph, where every spin is connected to every other spin, or a sparse graph, where only a few spins are connected. The spin-glass problem is NP-hard, and finding the ground state is computationally intractable for large systems.</p><img src="../../assets/images/spinglass.png" width="400" /><h3 id="Partition-function"><a class="docs-heading-anchor" href="#Partition-function">Partition function</a><a id="Partition-function-1"></a><a class="docs-heading-anchor-permalink" href="#Partition-function" title="Permalink"></a></h3><p>The thermal equilibrium of the spin-glass system is described by the Boltzmann distribution</p><p class="math-container">\[P(\sigma) = \frac{1}{Z} e^{-\beta H(\sigma)},\]</p><p>where <span>$\beta = 1/T$</span> is the inverse temperature, and <span>$Z$</span> is the partition function</p><p class="math-container">\[Z = \sum_{\sigma} e^{-\beta H(\sigma)}.\]</p><p>The partition function is the normalization constant that ensures the probability distribution sums to one. The partition function is a sum over all possible spin configurations, which makes it computationally intractable for large systems.</p><p>The partition function can be expressed as a tensor contraction using the einsum notation. The partition function is a sum over all possible spin configurations, which can be represented as a tensor contraction over the spins. The partition function can be written as</p><p class="math-container">\[Z = \sum_{\sigma} e^{-\beta H(\sigma)} = \sum_{\sigma} e^{-\beta \sum_{i,j} J_{ij} \sigma_i \sigma_j + \sum_i h_i \sigma_i} = \sum_{\sigma} \prod_{i,j} e^{-\beta J_{ij} \sigma_i \sigma_j} \prod_i e^{h_i \sigma_i}.\]</p><p>After converting the spin-glass Hamiltonian into a tensor contraction, the partition function can be computed efficiently using tensor network libraries. The topology of the tensor network is as follows: the spins are the hyper-edges, and the nodes are the tensors.</p><img src="../../assets/images/regular-200_einsum.png" width="400" /><p>To demonstrate a simpler example, we consider a spin-glass system with three spins arranged in a triangle.</p><div class="admonition is-info"><header class="admonition-header">Example: Triangle</header><div class="admonition-body"><p><img src="../../assets/images/triangle.svg" alt/></p><p>Consider a simple example of a spin-glass system with three spins arranged in a triangle. The Hamiltonian is given by</p><p class="math-container">\[H(\sigma) = J \sigma_1 \sigma_2 + J \sigma_2 \sigma_3 + J \sigma_3 \sigma_1,\]</p><p>where <span>$J$</span> is the coupling strength. The partition function is given by</p><p class="math-container">\[Z = \sum_{\sigma} e^{-\beta H(\sigma)} = \sum_{\sigma} e^{-\beta J \sigma_1 \sigma_2 - \beta J \sigma_2 \sigma_3 - \beta J \sigma_3 \sigma_1}.\]</p><p>The diagrammatic representation of the tensor network is shown below:</p><img src="../../assets/images/regular-3_einsum.png" width="300" /><p>where each node is a tensor of rank 2</p><p class="math-container">\[M_{ij} = e^{-\beta \sigma_i \sigma_j}
= \left(\begin{matrix}e^{-\beta} &amp; e^{\beta}\\e^{\beta} &amp; e^{-\beta}\end{matrix}\right)\]</p><p>The partition function can be expressed as a tensor contraction using the einsum notation.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; function partition_function(beta, J)
           M = [exp(-beta * J) exp(beta * J);
               exp(beta * J) exp(-beta * J)]
           Z = ein&quot;(ij,jk),ki-&gt;&quot;(M, M, M)[]  # () denotes the order of the contraction.
           return Z
       end</code><code class="nohighlight hljs ansi" style="display:block;">partition_function (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; J = 1.0</code><code class="nohighlight hljs ansi" style="display:block;">1.0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; partition_function(2.0, J)</code><code class="nohighlight hljs ansi" style="display:block;">44.339294097937234</code></pre><p>The ground state of the spin-glass system is the spin configuration that minimizes the Hamiltonian. From the partition function, we can compute the ground state energy and degeneracy of the ground state.</p><p class="math-container">\[E_G = \lim_{\beta\rightarrow \infty} - \frac{1}{Z}\frac{\partial Z}{\partial \beta},\]</p><p>where <span>$E_G$</span> is the ground state energy. The degeneracy of the ground state is the number of spin configurations that achieve the minimum energy.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using ForwardDiff</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; beta = 10.0</code><code class="nohighlight hljs ansi" style="display:block;">10.0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; Z = partition_function(beta, J)</code><code class="nohighlight hljs ansi" style="display:block;">132158.79476884034</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; E_G = -ForwardDiff.derivative(b -&gt; partition_function(b, J), beta)/Z</code><code class="nohighlight hljs ansi" style="display:block;">-1.0</code></pre><p>The ground state degeneracy can be computed with</p><p class="math-container">\[S_G = \lim_{\beta\rightarrow \infty} Z/e^{-\beta E_G}.\]</p><p>In Julia, the ground state energy and degeneracy can be computed as follows:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; S_G = Z/exp(-beta * E_G)</code><code class="nohighlight hljs ansi" style="display:block;">6.000000000000001</code></pre></div></div><h2 id="Ground-state-finding"><a class="docs-heading-anchor" href="#Ground-state-finding">Ground state finding</a><a id="Ground-state-finding-1"></a><a class="docs-heading-anchor-permalink" href="#Ground-state-finding" title="Permalink"></a></h2><p>The ground state finding problem is to find the spin configuration that minimizes the Hamiltonian.</p><p class="math-container">\[E_{\rm min} = \min_{\sigma} \left(\sum_{i, j} J_{ij} \sigma_i \sigma_j\right)\]</p><p>This is a well-known NP-hard problem, and finding the ground state is computationally intractable for large systems. The ground state can be found approximately using various optimization algorithms, such as simulated annealing. In the following, we introduce the Tropical algorithm for finding the ground state of the spin-glass system exactly.</p><h3 id="Tropical-algebra"><a class="docs-heading-anchor" href="#Tropical-algebra">Tropical algebra</a><a id="Tropical-algebra-1"></a><a class="docs-heading-anchor-permalink" href="#Tropical-algebra" title="Permalink"></a></h3><p>The tropical algebra is a mathematical structure that generalizes the real numbers. The tropical sum and tropical product are defined as follows:</p><p class="math-container">\[a \oplus b = \max(a, b), \quad a \otimes b = a + b\]</p><p>The tropical zero is <span>$-\infty$</span>, and the tropical one is <span>$0$</span>. The tropical sum is the maximum operation, and the tropical product is the addition operation. The tropical algebra is idempotent, commutative, and associative.</p><h3 id="Tropical-tensor-network-for-ground-state-energy-finding"><a class="docs-heading-anchor" href="#Tropical-tensor-network-for-ground-state-energy-finding">Tropical tensor network for ground state energy finding</a><a id="Tropical-tensor-network-for-ground-state-energy-finding-1"></a><a class="docs-heading-anchor-permalink" href="#Tropical-tensor-network-for-ground-state-energy-finding" title="Permalink"></a></h3><p>A tropical tensor network is a tensor network where the tensor elements are tropical numbers, such that the original sum-product network becomes a max-plus network. The tropical algebra can be related to the ground state energy of the spin-glass system, which is given by</p><p class="math-container">\[E_{\rm min} = \lim_{\beta\rightarrow \infty} -\frac{1}{\beta} \log Z = \lim_{\beta\rightarrow \infty} -\frac{1}{\beta} \log \sum_{\sigma} \prod_{i,j} e^{-\beta J_{ij}\sigma_i\sigma_j}\]</p><p>Note that</p><p class="math-container">\[\frac{1}{\beta}\log e^{\beta x}  e^{\beta y} = x+y\]</p><p class="math-container">\[\frac{1}{\beta}\lim_{\beta \rightarrow \infty}e^{\beta x} + e^{\beta y} = \max(x, y)\]</p><p>We have</p><p class="math-container">\[E_{\rm min} = -\left(\max_{\sigma} \sum_{i,j} -J_{ij}\sigma_i\sigma_j\right)\]</p><p>which corresponds to the tropical tensor network contraction.</p><p>Instead of using regular tensors, we use the following tropical tensors in the tensor network</p><p class="math-container">\[T(J_{ij}) = \left(\begin{matrix} -J_{ij} &amp; J_{ij}\\ J_{ij} &amp; -J_{ij}\end{matrix}\right)\]</p><p>Let <span>$G = (V, E)$</span> be the graph of the spin-glass system, where <span>$V$</span> is the set of spins and <span>$E$</span> is the set of couplings. The corresponding tropical tensor network is given by</p><p class="math-container">\[(\Lambda, \mathcal{T}, V_0) = (\{\sigma_i\mid i\in V\}, \{T(J_{ij})_{\sigma_i\sigma_j}\mid (i,j)\in E\}, \{\})\]</p><p>The contraction of the above tensor network corresponds to the ground state energy of the spin-glass system.</p><h2 id="Tensor-network-contraction"><a class="docs-heading-anchor" href="#Tensor-network-contraction">Tensor network contraction</a><a id="Tensor-network-contraction-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-network-contraction" title="Permalink"></a></h2><p>Optimizing a tensor network contraction order means reducing the following complexities:</p><ul><li><strong>Space complexity</strong>: the number of elements in the largest tensor.</li><li>Time complexity: the number of operations to contract a tensor network.</li><li>Read-write complexity: the number of times to read and write tensor elements.</li></ul><p>Among which, the space complexity is the most important. However, optimizing which is NP-hard. The following two problems are equivalent (Both are NP-hard to compute):</p><ul><li>Minimum possible tensor rank during the contraction</li><li>Tree width of the dual graph of the tensor network.</li></ul><h3 id="Tree-decomposition"><a class="docs-heading-anchor" href="#Tree-decomposition">Tree decomposition</a><a id="Tree-decomposition-1"></a><a class="docs-heading-anchor-permalink" href="#Tree-decomposition" title="Permalink"></a></h3><p>Give a graph <span>$G = (V, E)$</span>, a tree decomposition of <span>$G$</span> is a tree <span>$T$</span> with nodes <span>$X_1, \dots, X_n$</span>, where each <span>$X_i$</span> is a subset of <span>$V$</span>, satisfying the following properties:</p><ul><li>The union of all sets <span>$X_i$</span> equals <span>$V$</span>.</li><li>If <span>$X_i$</span> and <span>$X_j$</span> both contain a vertex <span>$v$</span>, then all nodes <span>$X_k$</span> of <span>$T$</span> in the (unique) path between <span>$X_i$</span> and <span>$X_j$</span> contain <span>$v$</span> as well.</li><li>For every edge <span>$(v, w)$</span> in the graph, there is a subset <span>$X_i$</span> that contains both <span>$v$</span> and <span>$w$</span>.</li></ul><p>To reduce the tensor network contraction complexity, we can optimize the tree width of the dual graph of the tensor network. The tree width of a graph is the smallest tree width over all possible tree decompositions of a graph.</p><ul><li><em>Tree width of a tree decomposition</em>: The largest number of nodes in a clique of a tree decomposition of a graph.</li><li><em>Tree width of a graph</em>: The smallest tree width over all possible tree decompositions of a graph.</li></ul><p>Algorithms for optimizing the tensor network contraction order could be found in the <a href="https://queracomputing.github.io/GenericTensorNetworks.jl/dev/performancetips/">manual of performance tips</a>.</p><h3 id="Landscape"><a class="docs-heading-anchor" href="#Landscape">Landscape</a><a id="Landscape-1"></a><a class="docs-heading-anchor-permalink" href="#Landscape" title="Permalink"></a></h3><p>Generic tensor network<sup class="footnote-reference"><a id="citeref-Liu2023" href="#footnote-Liu2023">[Liu2023]</a></sup> provides a unified framework for computing the solution space properties of combinatorial optimization problems. The landscape of the spin-glass problem can be analyzed using tensor networks to understand the structure of the energy landscape and the complexity of the optimization problem. Please refer to the <a href="https://queracomputing.github.io/GenericTensorNetworks.jl/dev/generated/SpinGlass/#Spin-glass-problem">manual</a> for more details.</p><h2 id="The-backward-rule-of-tensor-contraction"><a class="docs-heading-anchor" href="#The-backward-rule-of-tensor-contraction">The backward rule of tensor contraction</a><a id="The-backward-rule-of-tensor-contraction-1"></a><a class="docs-heading-anchor-permalink" href="#The-backward-rule-of-tensor-contraction" title="Permalink"></a></h2><p>The backward rule for matrix multiplication is</p><ul><li><code>C = ein&quot;ij,jk-&gt;ik&quot;(A, B)</code><ul><li><code>̄A = ein&quot;ik,jk-&gt;ij&quot;(̄C, B)</code></li><li><code>̄B = ein&quot;ik,jk-&gt;ij&quot;(A, ̄C)</code></li></ul></li><li><code>v = ein&quot;ii-&gt;i&quot;(A)</code><ul><li><code>̄A = ein&quot;?&quot;(̄v)</code></li></ul></li></ul><h1 id="Probability-graph"><a class="docs-heading-anchor" href="#Probability-graph">Probability graph</a><a id="Probability-graph-1"></a><a class="docs-heading-anchor-permalink" href="#Probability-graph" title="Permalink"></a></h1><table><tr><th style="text-align: center"><strong>Random variable</strong></th><th style="text-align: left"><strong>Meaning</strong></th></tr><tr><td style="text-align: center">A</td><td style="text-align: left">Recent trip to Asia</td></tr><tr><td style="text-align: center">T</td><td style="text-align: left">Patient has tuberculosis</td></tr><tr><td style="text-align: center">S</td><td style="text-align: left">Patient is a smoker</td></tr><tr><td style="text-align: center">L</td><td style="text-align: left">Patient has lung cancer</td></tr><tr><td style="text-align: center">B</td><td style="text-align: left">Patient has bronchitis</td></tr><tr><td style="text-align: center">E</td><td style="text-align: left">Patient hast T and/or L</td></tr><tr><td style="text-align: center">X</td><td style="text-align: left">Chest X-Ray is positive</td></tr><tr><td style="text-align: center">D</td><td style="text-align: left">Patient has dyspnoea</td></tr></table><p>A probabilistic graphical model (PGM) illustrates the mathematical modeling of reasoning in the presence of uncertainty. Bayesian networks (above) and Markov random fields are popular types of PGMs. Consider the Bayesian network shown in the figure above known as the <em>ASIA network</em>. It is a simplified example from the context of medical diagnosis that describes the probabilistic relationships between different random variables corresponding to possible diseases, symptoms, risk factors and test results. It consists of a graph <span>$G = (V,\mathcal{E})$</span> and a probability distribution <span>$P(V)$</span> where <span>$G$</span> is a directed acyclic graph, <span>$V$</span> is the set of variables and <span>$\mathcal{E}$</span> is the set of edges connecting the variables. We assume all variables to be discrete (0 or 1). Each variable <span>$v \in V$</span> is quantified with a <em>conditional probability distribution</em> <span>$P(v \mid pa(v))$</span> where <span>$pa(v)$</span> are the parents of <span>$v$</span>. These conditional probability distributions together with the graph <span>$G$</span> induce a <em>joint probability distribution</em> over <span>$P(V)$</span>, given by</p><p class="math-container">\[P(V) = \prod_{v\in V} P(v \mid pa(v)).\]</p><h2 id="The-partition-function"><a class="docs-heading-anchor" href="#The-partition-function">The partition function</a><a id="The-partition-function-1"></a><a class="docs-heading-anchor-permalink" href="#The-partition-function" title="Permalink"></a></h2><p><a href="https://uaicompetition.github.io/uci-2022/competition-entry/tasks/">https://uaicompetition.github.io/uci-2022/competition-entry/tasks/</a></p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Liu2023"><a class="tag is-link" href="#citeref-Liu2023">Liu2023</a>Liu, Jin-Guo, et al. &quot;Computing solution space properties of combinatorial optimization problems via generic tensor networks.&quot; SIAM Journal on Scientific Computing 45.3 (2023): A1239-A1270.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../sparse/">« Sparse Matrices and Graphs</a><a class="docs-footer-nextpage" href="../../chap4/optimization/">Optimization »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.0 on <span class="colophon-date" title="Wednesday 24 April 2024 10:01">Wednesday 24 April 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
