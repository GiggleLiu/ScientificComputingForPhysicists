<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Automatic Differentiation · Scientific Computing For Physicists</title><meta name="title" content="Automatic Differentiation · Scientific Computing For Physicists"/><meta property="og:title" content="Automatic Differentiation · Scientific Computing For Physicists"/><meta property="twitter:title" content="Automatic Differentiation · Scientific Computing For Physicists"/><meta name="description" content="Documentation for Scientific Computing For Physicists."/><meta property="og:description" content="Documentation for Scientific Computing For Physicists."/><meta property="twitter:description" content="Documentation for Scientific Computing For Physicists."/><meta property="og:url" content="https://book.jinguo-group.science/chap4/ad/"/><meta property="twitter:url" content="https://book.jinguo-group.science/chap4/ad/"/><link rel="canonical" href="https://book.jinguo-group.science/chap4/ad/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Scientific Computing For Physicists</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Become an Open-source Developer</span><ul><li><a class="tocitem" href="../../chap1/terminal/">Get a Terminal!</a></li><li><a class="tocitem" href="../../chap1/git/">Maintainability - Version Control</a></li><li><a class="tocitem" href="../../chap1/ci/">Correctness - Unit Tests</a></li></ul></li><li><span class="tocitem">Julia Programming Language</span><ul><li><a class="tocitem" href="../../chap2/julia-setup/">Setup Julia</a></li><li><a class="tocitem" href="../../chap2/julia-why/">Why Julia?</a></li><li><a class="tocitem" href="../../chap2/julia-type/">Types and Multiple-dispatch</a></li><li><a class="tocitem" href="../../chap2/julia-array/">Array and Broadcasting</a></li><li><a class="tocitem" href="../../chap2/julia-release/">My First Package</a></li><li><a class="tocitem" href="../../chap2/julia-fluid/">Project: Fluid dynamics</a></li></ul></li><li><span class="tocitem">Linear Algebra</span><ul><li><a class="tocitem" href="../../chap3/linalg/">Matrix Computation</a></li><li><a class="tocitem" href="../../chap3/lu/">Solving linear equations by LU factorization: Bottom-up</a></li><li><a class="tocitem" href="../../chap3/qr/">QR Factorization: Bottom-up</a></li><li><a class="tocitem" href="../../chap3/fft/">Fast Fourier transform</a></li><li><a class="tocitem" href="../../chap3/sensitivity/">Sensitivity Analysis</a></li><li><a class="tocitem" href="../../chap3/sparse/">Sparse Matrices and Graphs</a></li></ul></li><li><span class="tocitem">Tensors and Tensor Networks</span><ul><li><a class="tocitem" href="../../chap3/tensors/">Tensor Operations</a></li></ul></li><li><span class="tocitem">Optimization</span><ul><li><a class="tocitem" href="../optimization/">Optimization</a></li><li class="is-active"><a class="tocitem" href>Automatic Differentiation</a><ul class="internal"><li><a class="tocitem" href="#A-brief-history-of-autodiff"><span>A brief history of autodiff</span></a></li><li><a class="tocitem" href="#Differentiating-the-Bessel-function"><span>Differentiating the Bessel function</span></a></li><li><a class="tocitem" href="#Finite-difference"><span>Finite difference</span></a></li><li><a class="tocitem" href="#Forward-mode-automatic-differentiation"><span>Forward mode automatic differentiation</span></a></li><li><a class="tocitem" href="#Reverse-mode-automatic-differentiation"><span>Reverse mode automatic differentiation</span></a></li><li><a class="tocitem" href="#Rule-based-AD-and-source-code-transformation"><span>Rule based AD and source code transformation</span></a></li><li><a class="tocitem" href="#Example:-Hit-the-earth"><span>Example: Hit the earth</span></a></li><li><a class="tocitem" href="#Deriving-the-backward-rules-for-linear-algebra"><span>Deriving the backward rules for linear algebra</span></a></li><li><a class="tocitem" href="#Obtaining-Hessian"><span>Obtaining Hessian</span></a></li><li><a class="tocitem" href="#Optimal-checkpointing"><span>Optimal checkpointing</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li></ul></li><li><span class="tocitem">Randomness</span><ul><li><a class="tocitem" href="../../chap5/montecarlo/">Markov Chain Monte Carlo</a></li></ul></li><li><span class="tocitem">Appendix</span><ul><li><a class="tocitem" href="../../append/plotting/">Plotting recipes with CairoMakie</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Optimization</a></li><li class="is-active"><a href>Automatic Differentiation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Automatic Differentiation</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/GiggleLiu/ScientificComputingForPhysicists" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/GiggleLiu/ScientificComputingForPhysicists/blob/main/docs/src/chap4/ad.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Automatic-Differentiation"><a class="docs-heading-anchor" href="#Automatic-Differentiation">Automatic Differentiation</a><a id="Automatic-Differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Differentiation" title="Permalink"></a></h1><p>Automatic differentiation<sup class="footnote-reference"><a id="citeref-Griewank2008" href="#footnote-Griewank2008">[Griewank2008]</a></sup> is a technique to compute the derivative of a function automatically. It is a powerful tool for scientific computing, machine learning, and optimization. The automatic differentiation can be classified into two types: forward mode and backward mode. The forward mode AD computes the derivative of a function with respect to many inputs, while the backward mode AD computes the derivative of a function with respect to many outputs. The forward mode AD is efficient when the number of inputs is small, while the backward mode AD is efficient when the number of outputs is small.</p><h2 id="A-brief-history-of-autodiff"><a class="docs-heading-anchor" href="#A-brief-history-of-autodiff">A brief history of autodiff</a><a id="A-brief-history-of-autodiff-1"></a><a class="docs-heading-anchor-permalink" href="#A-brief-history-of-autodiff" title="Permalink"></a></h2><ul><li>1964 (<strong>forward mode AD</strong>) ~ Robert Edwin Wengert, A simple automatic derivative evaluation program.</li><li>1970 (<strong>backward mode AD</strong>) ~ Seppo Linnainmaa, Taylor expansion of the accumulated rounding error.</li><li>1986 (<strong>AD for machine learning</strong>) ~ Rumelhart, D. E., Hinton, G. E., and Williams, R. J., Learning representations by back-propagating errors.</li><li>1992 (<strong>optimal checkpointing</strong>) ~ Andreas Griewank, Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation.</li><li>2000s ~ The boom of tensor based AD frameworks for machine learning.</li><li>2018 ~ Re-inventing AD as differential programming (<a href="https://en.wikipedia.org/wiki/Differentiable_programming">wiki</a>.) <img src="https://qph.fs.quoracdn.net/main-qimg-fb2f8470f2120eb49c8142b08d9c4132" alt/></li><li>2020 (<strong>AD on LLVM</strong>) ~ Moses, William and Churavy, Valentin, Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients.</li></ul><h2 id="Differentiating-the-Bessel-function"><a class="docs-heading-anchor" href="#Differentiating-the-Bessel-function">Differentiating the Bessel function</a><a id="Differentiating-the-Bessel-function-1"></a><a class="docs-heading-anchor-permalink" href="#Differentiating-the-Bessel-function" title="Permalink"></a></h2><p class="math-container">\[J_\nu(z) = \sum\limits_{n=0}^{\infty} \frac{(z/2)^\nu}{\Gamma(k+1)\Gamma(k+\nu+1)} (-z^2/4)^{n}\]</p><p>A poorman&#39;s implementation of this Bessel function is as follows</p><pre><code class="language-julia hljs">function poor_besselj(ν, z::T; atol=eps(T)) where T
    k = 0
    s = (z/2)^ν / factorial(ν)
    out = s
    while abs(s) &gt; atol
        k += 1
        s *= (-1) / k / (k+ν) * (z/2)^2
        out += s
    end
    out
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">poor_besselj (generic function with 1 method)</code></pre><p>Let us plot the Bessel function</p><pre><code class="language-julia hljs">using CairoMakie

x = 0.0:0.01:10
fig = Figure()
ax = Axis(fig[1, 1]; xlabel=&quot;x&quot;, ylabel=&quot;J(ν, x)&quot;)
for i=0:5
    yi = poor_besselj.(i, x)
    lines!(ax, x, yi; label=&quot;J(ν=$i)&quot;, linewidth=2)
end
fig</code></pre><img src="f29e2ba1.png" alt="Example block output"/><p>The derivative of the Bessel function is</p><p class="math-container">\[\frac{d J_\nu(z)}{dz} = \frac{J_{\nu-1}(z) - J_{\nu+1}(z) }2\]</p><p>In the following code, we compute the gradient of the Bessel function with respect to <span>$\nu=2$</span> using different methods.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using FiniteDifferences: central_fdm</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Enzyme</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ν = 2</code><code class="nohighlight hljs ansi" style="display:block;">2</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; yi = poor_besselj.(ν, x)</code><code class="nohighlight hljs ansi" style="display:block;">1001-element Vector{Float64}:
 0.0
 1.2499895833658856e-5
 4.9998333354166534e-5
 0.00011249156273730111
 0.00019997333466663112
 0.0003124349009193845
 0.0004498650151865888
 0.0006122499341274064
 0.0007995734186575651
 0.0010118167354717647
 ⋮
 0.2544446180989598
 0.2545535274776343
 0.25463791698639315
 0.25469780120986446
 0.2547331970234957
 0.25474412359048787
 0.2547306023581344
 0.2546926570546253
 0.254630313685045</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; g_f = [Enzyme.autodiff(Enzyme.Forward, poor_besselj, Active, Enzyme.Const(ν), Enzyme.Duplicated(xi, 1.0))[1] for xi in x] # forward mode</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: Active Returns not allowed in forward mode</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; g_m = (poor_besselj.(ν-1, x) - poor_besselj.(ν+1, x)) ./ 2 # manual</code><code class="nohighlight hljs ansi" style="display:block;">1001-element Vector{Float64}:
  0.0
  0.0024999583335286453
  0.004999666672916611
  0.007498875047459988
  0.009997333533326222
  0.012494792276984322
  0.014991001518628505
  0.017485711615593105
  0.01997867306575652
  0.02246963653093209
  ⋮
  0.012117360188604584
  0.009664720400836789
  0.007213424436091794
  0.004763701525504236
  0.002315780596644404
 -0.0001301097479690025
 -0.0025737412517064726
 -0.005014886025475368
 -0.007453316568121832</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; g_b = [Enzyme.autodiff(Enzyme.Reverse, poor_besselj, Active, Enzyme.Const(ν), Enzyme.Active(xi))[1][2] for xi in x]</code><code class="nohighlight hljs ansi" style="display:block;">1001-element Vector{Float64}:
  0.0
  0.0024999583335286457
  0.004999666672916611
  0.007498875047459988
  0.009997333533326224
  0.01249479227698432
  0.014991001518628505
  0.017485711615593105
  0.01997867306575652
  0.022469636530932088
  ⋮
  0.012117360188515162
  0.00966472040073519
  0.00721342443606391
  0.004763701525451261
  0.0023157805965232994
 -0.00013010974797617036
 -0.0025737412518127695
 -0.005014886025475014
 -0.007453316568259566</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; g_c = central_fdm(5, 1).(z-&gt;poor_besselj(ν, z), x) # central finite difference</code><code class="nohighlight hljs ansi" style="display:block;">1001-element Vector{Float64}:
  0.0
  0.002499958333528643
  0.004999666672916572
  0.007498875047460022
  0.00999733353332623
  0.012494792276984247
  0.01499100151862854
  0.017485711615593313
  0.0199786730657568
  0.022469636530932088
  ⋮
  0.012117360177799304
  0.009664720382932049
  0.007213424473365034
  0.004763701486283925
  0.0023157806066726925
 -0.0001301097492729963
 -0.0025737412380602755
 -0.00501488604244338
 -0.007453316540942618</code></pre><p>Here, the forward and backward mode AD are implemented using the <a href="https://github.com/EnzymeAD/Enzyme.jl"><code>Enzyme</code></a> package. The manual method is the direct application of the derivative formula. The central finite difference is computed using the <code>central_fdm</code> function from the <a href="https://github.com/JuliaDiff/FiniteDifferences.jl"><code>FiniteDifferences</code></a> package.</p><pre><code class="language-julia hljs">fig = Figure()
ax = Axis(fig[1, 1]; xlabel=&quot;x&quot;, ylabel=&quot;y&quot;)
lines!(ax, x, yi; label=&quot;J(ν=$ν, x)&quot;, linewidth=2)
lines!(ax, x, g_b; label=&quot;g(ν=$ν, x)&quot;, linewidth=2, linestyle=:dash)
axislegend(ax)
fig</code></pre><img src="e932e64b.png" alt="Example block output"/><h2 id="Finite-difference"><a class="docs-heading-anchor" href="#Finite-difference">Finite difference</a><a id="Finite-difference-1"></a><a class="docs-heading-anchor-permalink" href="#Finite-difference" title="Permalink"></a></h2><p>The finite difference is a numerical method to approximate the derivative of a function. The finite difference is a simple and efficient method to compute the derivative of a function. The finite difference can be classified into three types: forward, backward, and central.</p><p>For example, the first order forward difference is</p><p class="math-container">\[\frac{\partial f}{\partial x} \approx \frac{f(x+\Delta) - f(x)}{\Delta}\]</p><p>The first order backward difference is</p><p class="math-container">\[\frac{\partial f}{\partial x} \approx \frac{f(x) - f(x-\Delta)}{\Delta}\]</p><p>The first order central difference is</p><p class="math-container">\[\frac{\partial f}{\partial x} \approx \frac{f(x+\Delta) - f(x-\Delta)}{2\Delta}\]</p><p>Among these three methods, the central difference is the most accurate. It has an error of <span>$O(\Delta^2)$</span>, while the forward and backward differences have an error of <span>$O(\Delta)$</span>.</p><p>Higher order finite differences can be found in the <a href="https://en.wikipedia.org/wiki/Finite_difference_coefficient">wiki page</a>.</p><div class="admonition is-info"><header class="admonition-header">Example: central finite difference to the 4th order</header><div class="admonition-body"><p>The coefficients of the central finite difference to the 4th order are</p><table><tr><th style="text-align: right">-2</th><th style="text-align: right">-1</th><th style="text-align: right">0</th><th style="text-align: right">1</th><th style="text-align: right">2</th></tr><tr><td style="text-align: right">1/12</td><td style="text-align: right">−2/3</td><td style="text-align: right">0</td><td style="text-align: right">2/3</td><td style="text-align: right">−1/12</td></tr></table><p>The induced formula is</p><p class="math-container">\[\frac{\partial f}{\partial x} \approx \frac{f(x-2\Delta) - 8f(x-\Delta) + 8f(x+\Delta) - f(x+2\Delta)}{12\Delta}\]</p><p>In the following, we will derive this formula using the Taylor expansion.</p><p class="math-container">\[\left(\begin{matrix}
f(x-2\Delta)\\f(x-\Delta)\\f(x)\\f(x+\Delta)\\f(x+2\Delta)
\end{matrix}\right) \approx \left(\begin{matrix}
1 &amp; (-2)^1 &amp; (-2)^{2} &amp; (-2)^3 &amp;  &amp; (-2)^4\\
1 &amp; (-1)^1 &amp; (-1)^{2} &amp; (-1)^3 &amp;  &amp; (-1)^4\\
1 &amp; 0 &amp; 0 &amp; 0 &amp;  &amp; 0\\
1 &amp; (1)^1 &amp; (1)^{2} &amp; (1)^3 &amp;  &amp; (1)^4\\
1 &amp; (2)^1 &amp; (2)^{2} &amp; (2)^3 &amp;  &amp; (2)^4
\end{matrix}\right)\left(\begin{matrix}
f(x)\\f&#39;(x)\Delta\\f&#39;&#39;(x)\Delta^2/2\\f&#39;&#39;&#39;(x)\Delta^3/6\\f&#39;&#39;&#39;&#39;(x)\Delta^4/24
\end{matrix}\right)\]</p><p>Let us denote the matrix on the right-hand side as <span>$A$</span>. Then we want to find the coefficients <span>$\vec \alpha = (\alpha_{-2}, \alpha_{-1}, \alpha_{0}, \alpha_{1}, \alpha_{2})^T$</span> such that</p><p class="math-container">\[\begin{align*}
&amp;\alpha_{-2}f(x-2\Delta) + \alpha_{-1}f(x-\Delta) + \alpha_{0}f(x) + \alpha_{1}f(x+\Delta) + \alpha_{2}f(x+2\Delta)\\
&amp; = f&#39;(x)\Delta + O(\Delta^5),
\end{align*}\]</p><p>which can be computed by solving the linear system</p><p class="math-container">\[A \vec \alpha = (0, 1, 0, 0, 0)^T.\]</p><p>The following code demonstrates the central finite difference to the 4th order.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; b = [0.0, 1, 0, 0, 0]</code><code class="nohighlight hljs ansi" style="display:block;">5-element Vector{Float64}:
 0.0
 1.0
 0.0
 0.0
 0.0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; A = [i^j for i=-2:2, j=0:4]</code><code class="nohighlight hljs ansi" style="display:block;">5×5 Matrix{Int64}:
 1  -2  4  -8  16
 1  -1  1  -1   1
 1   0  0   0   0
 1   1  1   1   1
 1   2  4   8  16</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; A&#39; \ b  # the central_fdm(5, 1) coefficients</code><code class="nohighlight hljs ansi" style="display:block;">5-element Vector{Float64}:
  0.08333333333333333
 -0.6666666666666666
  0.0
  0.6666666666666666
 -0.08333333333333333</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; central_fdm(5, 1)(x-&gt;poor_besselj(2, x), 0.5)</code><code class="nohighlight hljs ansi" style="display:block;">0.11985236384013791</code></pre><pre><code class="language-julia-repl hljs">julia&gt; using BenchmarkTools

julia&gt; @benchmark central_fdm(5, 1)(y-&gt;poor_besselj(2, y), x) setup=(x=0.5)
BenchmarkTools.Trial: 10000 samples with 9 evaluations.
Range (min … max):  2.588 μs … 434.102 μs  ┊ GC (min … max): 0.00% … 98.68%
Time  (median):     2.708 μs               ┊ GC (median):    0.00%
Time  (mean ± σ):   2.832 μs ±   5.422 μs  ┊ GC (mean ± σ):  3.49% ±  1.96%

▁▂▅▆▇██▆▅▄▂▁                                               ▂
▇███████████████▆▆▆▅▄▅▅▄▆▄▅▇██▆▆▆▄▆▆▆▆▅▆▄▄▄▄▃▄▄▄▃▁▄▄▄▁▁▄▁▃▄ █
2.59 μs      Histogram: log(frequency) by time      3.62 μs &lt;

Memory estimate: 2.47 KiB, allocs estimate: 36.</code></pre></div></div><p>The central finite difference can be generalized to the <span>$n$</span>th order. The <span>$n$</span>th order central finite difference has an error of <span>$O(\Delta^{n+1})$</span>.</p><h2 id="Forward-mode-automatic-differentiation"><a class="docs-heading-anchor" href="#Forward-mode-automatic-differentiation">Forward mode automatic differentiation</a><a id="Forward-mode-automatic-differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-mode-automatic-differentiation" title="Permalink"></a></h2><p>Forward mode AD attaches a infitesimal number <span>$\epsilon$</span> to a variable, when applying a function <span>$f$</span>, it does the following transformation</p><p class="math-container">\[f(x+g \epsilon) = f(x) + f&#39;(x) g\epsilon + \mathcal{O}(\epsilon^2)\]</p><p>The higher order infinitesimal is ignored. </p><p><strong>In the program</strong>, we can define a <em>dual number</em> with two fields, just like a complex number</p><pre><code class="nohighlight hljs">f((x, g)) = (f(x), f&#39;(x)*g)</code></pre><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using ForwardDiff</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; res = sin(ForwardDiff.Dual(π/4, 2.0))</code><code class="nohighlight hljs ansi" style="display:block;">Dual{Nothing}(0.7071067811865475,1.4142135623730951)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; res === ForwardDiff.Dual(sin(π/4), cos(π/4)*2.0)</code><code class="nohighlight hljs ansi" style="display:block;">true</code></pre><p>We can apply this transformation consecutively, it reflects the chain rule.</p><p class="math-container">\[\begin{align*}
\frac{\partial \vec y_{i+1}}{\partial x} &amp;= \boxed{\frac{\partial \vec y_{i+1}}{\partial \vec y_i}}\frac{\partial \vec y_i}{\partial x}\\
&amp;\text{local Jacobian}
\end{align*}\]</p><p><strong>Example:</strong> Computing two gradients <span>$\frac{\partial z\sin x}{\partial x}$</span> and <span>$\frac{\partial \sin^2x}{\partial x}$</span> at one sweep</p><pre><code class="language-julia-repl hljs">julia&gt; autodiff(Forward, poor_besselj, 2, Duplicated(0.5, 1.0))[1]
0.11985236384014333

julia&gt; @benchmark autodiff(Forward, poor_besselj, 2, Duplicated(x, 1.0))[1] setup=(x=0.5)
BenchmarkTools.Trial: 10000 samples with 996 evaluations.
 Range (min … max):  22.256 ns … 66.349 ns  ┊ GC (min … max): 0.00% … 0.00%
 Time  (median):     23.050 ns              ┊ GC (median):    0.00%
 Time  (mean ± σ):   23.290 ns ±  1.986 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%

  ▅▅  █▆▂▂▄▂▂▂▁                                               ▁
  ██▅▇██████████▅▅▄▅▅▅▅▃▄▄▄▅▆▅▅▃▄▃▄▅▄▅▄▆▅▅▄▃▂▄▃▃▂▃▄▃▄▄▃▂▂▃▂▃▃ █
  22.3 ns      Histogram: log(frequency) by time      31.8 ns &lt;

 Memory estimate: 0 bytes, allocs estimate: 0.</code></pre><p>The computing time grows <strong>linearly</strong> as the number of variables that we want to differentiate. But does not grow significantly with the number of outputs.</p><h2 id="Reverse-mode-automatic-differentiation"><a class="docs-heading-anchor" href="#Reverse-mode-automatic-differentiation">Reverse mode automatic differentiation</a><a id="Reverse-mode-automatic-differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-mode-automatic-differentiation" title="Permalink"></a></h2><p>On the other side, the back-propagation can differentiate <strong>many inputs</strong> with respect to a <strong>single output</strong> efficiently</p><p class="math-container">\[\begin{align*}
    \frac{\partial \mathcal{L}}{\partial \vec y_i} = \frac{\partial \mathcal{L}}{\partial \vec y_{i+1}}&amp;\boxed{\frac{\partial \vec y_{i+1}}{\partial \vec y_i}}\\
&amp;\text{local jacobian?}
\end{align*}\]</p><pre><code class="language-julia-repl hljs">julia&gt; autodiff(Enzyme.Reverse, poor_besselj, 2, Enzyme.Active(0.5))[1]
(nothing, 0.11985236384014332)

julia&gt; @benchmark autodiff(Enzyme.Reverse, poor_besselj, 2, Enzyme.Active(x))[1] setup=(x=0.5)
BenchmarkTools.Trial: 10000 samples with 685 evaluations.
 Range (min … max):  182.482 ns … 503.771 ns  ┊ GC (min … max): 0.00% … 0.00%
 Time  (median):     208.880 ns               ┊ GC (median):    0.00%
 Time  (mean ± σ):   210.059 ns ±  17.016 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%

  ▃                 ▁▁█▃                                        
  █▂▁▂▃▁▁▁▁▂▂▁▁▁▁▁▁▃████▄▃▄▄▃▂▂▃▂▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▂
  182 ns           Histogram: frequency by time          260 ns &lt;

 Memory estimate: 0 bytes, allocs estimate: 0.</code></pre><p>Computing local Jacobian directly can be expensive. In practice, we can use the back-propagation rules to update the adjoint of the variables directly. It requires the forward pass storing the intermediate variables.</p><h2 id="Rule-based-AD-and-source-code-transformation"><a class="docs-heading-anchor" href="#Rule-based-AD-and-source-code-transformation">Rule based AD and source code transformation</a><a id="Rule-based-AD-and-source-code-transformation-1"></a><a class="docs-heading-anchor-permalink" href="#Rule-based-AD-and-source-code-transformation" title="Permalink"></a></h2><p>Rule based AD is a technique to define the backward rules of the functions. The backward rules are the derivatives of the functions with respect to the inputs. The backward rules are crucial for the reverse mode AD. For example, the backward rule of the Bessel function is</p><p class="math-container">\[\begin{align*}
&amp;J&#39;_{\nu}(z) =  \frac{J_{\nu-1}(z) - J_{\nu+1}(z) }2\\
&amp;J&#39;_{0}(z) =  - J_{1}(z)
\end{align*}\]</p><pre><code class="language-julia-repl hljs">julia&gt; 0.5 * (poor_besselj(1, 0.5) - poor_besselj(3, 0.5))
0.11985236384014333


julia&gt; @benchmark 0.5 * (poor_besselj(1, x) - poor_besselj(3, x)) setup=(x=0.5)
BenchmarkTools.Trial: 10000 samples with 998 evaluations.
 Range (min … max):  17.576 ns … 56.947 ns  ┊ GC (min … max): 0.00% … 0.00%
 Time  (median):     17.702 ns              ┊ GC (median):    0.00%
 Time  (mean ± σ):   17.999 ns ±  1.796 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%

  ▇█▃ ▁     ▁                                                 ▁
  ███▆██▆▃▅▆█▆▆▅▅▄▅▆▄▄▃▄▂▄▂▄▃▄▃▄▄▄▄▅▄▃▃▅▃▅▅▆▅▅▂▄▄▂▅▄▅▃▅▄▄▅▅▆▆ █
  17.6 ns      Histogram: log(frequency) by time      24.6 ns &lt;

 Memory estimate: 0 bytes, allocs estimate: 0.</code></pre><h2 id="Example:-Hit-the-earth"><a class="docs-heading-anchor" href="#Example:-Hit-the-earth">Example: Hit the earth</a><a id="Example:-Hit-the-earth-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Hit-the-earth" title="Permalink"></a></h2><p>Let us consider a simple example of throwing a stone on Pluto to hit the earth. To goal is to find the initial velocity <span>$v_0$</span> such that the stone hits the earth.</p><p>The following code is based on the <code>PhysicsSimulation</code> package in the <a href="https://github.com/GiggleLiu/ScientificComputingDemos">demo repository</a>. Clone the repository to local and run the following command in a terminal to start the Julia REPL:</p><pre><code class="language-bash hljs">$ make init-PhysicsSimulation  # install dependencies
$ make example-PhysicsSimulation  # run the example (optional)
$ cd PhysicsSimulation
$ julia --project=examples</code></pre><p>The solar system is defined as</p><pre><code class="language-julia-repl hljs">julia&gt; using PhysicsSimulation

julia&gt; data = PhysicsSimulation.Bodies.solar_system_data()
10×8 DataFrame
 Row │ name     mass        x             y             z            vx           vy            vz           
     │ String7  Float64     Float64       Float64       Float64      Float64      Float64       Float64      
─────┼───────────────────────────────────────────────────────────────────────────────────────────────────────
   1 │ sun      1.98854e30   -0.00343003    0.00176188   1.24669e-5   3.43312e-6  -5.2313e-6    -2.97297e-8
   2 │ mercury  3.302e23      0.0888799    -0.442615    -0.0447572    0.0219088    0.00716157   -0.00142593
  ⋮  │    ⋮         ⋮            ⋮             ⋮             ⋮            ⋮            ⋮             ⋮
  10 │ pluto    1.307e22    -21.2907      -18.9663       8.18796      0.0022763   -0.00267048   -0.000366955
                                                                                               7 rows omitted

julia&gt; const solar = PhysicsSimulation.Bodies.newton_system_from_data(data);
WARNING: redefinition of constant Main.solar. This may fail, cause incorrect answers, or produce other errors.

julia&gt; const Δt, num_steps = 0.01, 1000
(0.01, 1000)

julia&gt; states = leapfrog_simulation(solar; dt=Δt, nsteps=num_steps); # simulation</code></pre><p>Then we modify the solar system by adding a location close to the pluto. The loss function is defined as the distance between the stone and the earth after the simulation.</p><pre><code class="language-julia-repl hljs">julia&gt; function modified_solar_system(v0)
           # I throw a stone on Pluto, with velocity v0
           newbody = Body(solar.bodies[end].r + PhysicsSimulation.Point(0.01, 0.0, 0.0), v0, 1e-16)
           bds = copy(solar.bodies)
           push!(bds, newbody)
           return NewtonSystem(bds)
       end
modified_solar_system (generic function with 1 method)

julia&gt; function loss_hit_earth(v0)
           # simulate the system
           lf = LeapFrogSystem(modified_solar_system(PhysicsSimulation.Point(v0...)))
           for _ = 1:num_steps
               step!(lf, Δt)
           end
           return PhysicsSimulation.distance(lf.sys.bodies[end].r, lf.sys.bodies[4].r)  # final distance to earth
       end
loss_hit_earth (generic function with 1 method)

julia&gt; v0 = solar.bodies[end].v * 2
Point3D{Float64}((1.6628340497679406, -1.9507869905754016, -0.26806028935392806))

julia&gt; y0 = loss_hit_earth(v0)
36.347500179711304</code></pre><p>The trajectory of the stone is shown in the following video, where the stone is in red and the earth is in yellow.</p><video width="580" height="420" controls style="margin-bottom:30px">
  <source src="../../assets/images/solar-system-hit-earth-beforeopt.mp4" type="video/mp4">
</video><p>We use Enzyme to obtain the gradient of the loss function.</p><pre><code class="language-julia-repl hljs">julia&gt; using Enzyme

julia&gt; function gradient_hit_earth!(g, v)
           g .= Enzyme.autodiff(Enzyme.Reverse, loss_hit_earth, Active, Active(PhysicsSimulation.Point(v...)))[1][1]
           return g
       end
gradient_hit_earth! (generic function with 1 method)

julia&gt; gradient_hit_earth!(zeros(3), v0)
┌ Warning: TODO forward zero-set of arraycopy used memset rather than runtime type
└ @ Enzyme.Compiler ~/.julia/packages/GPUCompiler/U36Ed/src/utils.jl:59
3-element Vector{Float64}:
  -1.1166522031161832
 -10.153844722578679
   1.4730813620938634</code></pre><p>The gradients can be verified by the finite difference method</p><pre><code class="language-julia-repl hljs">using Test, FiniteDifferences

julia&gt; @testset &quot;grad&quot; begin
           enzyme_gradient = gradient_hit_earth!(zeros(3), v0)
           finitediff_gradient, = FiniteDifferences.jacobian(central_fdm(5,1), loss_hit_earth, v0)
           # compare the jacobian
           @test sum(abs2, enzyme_gradient - finitediff_gradient&#39;) &lt; 1e-6
       end
Test Summary: | Pass  Total  Time
grad          |    1      1  2.6s
Test.DefaultTestSet(&quot;grad&quot;, Any[], 1, false, false, true, 1.712588089584372e9, 1.712588092166565e9, false, &quot;REPL[17]&quot;)</code></pre><p>We use the <code>Optim</code> package to optimize the loss function with respect to the initial velocity. The optimizer is LBFGS.</p><pre><code class="language-julia-repl hljs">julia&gt; using Optim

julia&gt; function hit_earth(v0)
           # optimize the velocity, such that the stone hits the earth
           # the optimizer is LBFGS
           return Optim.optimize(loss_hit_earth, gradient_hit_earth!, [v0...], LBFGS()).minimizer
       end
hit_earth (generic function with 1 method)

julia&gt; vopt = hit_earth(v0)
3-element Vector{Float64}:
  1.6330750836153596
  1.3140656278615053
 -0.6512732118117887

julia&gt; yopt = loss_hit_earth(vopt)
2.9515292346456305e-15</code></pre><p>Finally, we visualize the result as a video.</p><video width="580" height="420" controls style="margin-bottom:30px">
  <source src="../../assets/images/solar-system-hit-earth.mp4" type="video/mp4">
</video><p>Yeah, the stone hits the earth!</p><h3 id="Rule-based-or-not?"><a class="docs-heading-anchor" href="#Rule-based-or-not?">Rule based or not?</a><a id="Rule-based-or-not?-1"></a><a class="docs-heading-anchor-permalink" href="#Rule-based-or-not?" title="Permalink"></a></h3><table>
<tr>
<th width=200></th>
<th width=300>Rule based</th>
<th width=300>Source code transformation</th>
</tr>
<tr style="vertical-align:top">
<td></td>
<td>defining backward rules manully for functions on tensors</td>
<td>defining backward rules on a limited set of basic scalar operations, and generate gradient code using source code transformation</td>
</tr>
<tr style="vertical-align:top">
<td>pros and cons</td>
<td>
<ol>
<li style="color:green">Good tensor performance</li>
<li style="color:green">Mature machine learning ecosystem</li>
</ol>
</td>
<td>
<ol>
<li style="color:green">Reasonalbe scalar performance</li>
<li style="color:red">Automatically generated backward rules</li>
</ol>
</td>
<td>
</td>
</tr>
<tr style="vertical-align:top">
<td>packages</td>
<td><a href="https://jax.readthedocs.io/en/latest/">JAX</a><br>
<a href="https://pytorch.org/">PyTorch</a>
</td>
<td><a href="http://tapenade.inria.fr:8080/tapenade/">Tapenade</a><br>
<a href="http://www.met.reading.ac.uk/clouds/adept/">Adept</a><br>
<a href="https://github.com/EnzymeAD/Enzyme">Enzyme</a>
</td>
</tr>
</table><h2 id="Deriving-the-backward-rules-for-linear-algebra"><a class="docs-heading-anchor" href="#Deriving-the-backward-rules-for-linear-algebra">Deriving the backward rules for linear algebra</a><a id="Deriving-the-backward-rules-for-linear-algebra-1"></a><a class="docs-heading-anchor-permalink" href="#Deriving-the-backward-rules-for-linear-algebra" title="Permalink"></a></h2><p>Many backward rules could be found in the notes<sup class="footnote-reference"><a id="citeref-Giles2008" href="#footnote-Giles2008">[Giles2008]</a></sup><sup class="footnote-reference"><a id="citeref-Seeger2017" href="#footnote-Seeger2017">[Seeger2017]</a></sup>. Here we list some of the latest improvements.</p><h3 id="Matrix-multiplication"><a class="docs-heading-anchor" href="#Matrix-multiplication">Matrix multiplication</a><a id="Matrix-multiplication-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-multiplication" title="Permalink"></a></h3><p>Let <span>$\mathcal{T}$</span> be a stack, and <span>$x \rightarrow \mathcal{T}$</span> and <span>$x\leftarrow \mathcal{T}$</span> be the operation of pushing and poping an element from this stack. Given <span>$A \in R^{l\times m}$</span> and <span>$B\in R^{m\times n}$</span>, the forward pass computation of matrix multiplication is</p><p class="math-container">\[\begin{align*}
&amp;C = A B\\
&amp;A \rightarrow \mathcal{T}\\
&amp;B \rightarrow \mathcal{T}\\
&amp;\ldots
\end{align*}\]</p><p>Let the adjoint of <span>$x$</span> be <span>$\overline{x} = \frac{\partial \mathcal{L}}{\partial x}$</span>, where <span>$\mathcal{L}$</span> is a real loss as the final output. The backward pass computes</p><p class="math-container">\[\begin{align*}
&amp;\ldots\\
&amp;B \leftarrow \mathcal{T}\\
&amp;\overline{A} = \overline{C}B\\
&amp;A \leftarrow \mathcal{T}\\
&amp;\overline{B} = A\overline{C}
\end{align*}\]</p><p>The rules to compute <span>$\overline{A}$</span> and <span>$\overline{B}$</span> are called the backward rules for matrix multiplication. They are crucial for rule based automatic differentiation.</p><p>Let us introduce a small perturbation <span>$\delta A$</span> on <span>$A$</span> and <span>$\delta B$</span> on <span>$B$</span>,</p><p class="math-container">\[\delta C = \delta A B + A \delta B\]</p><p class="math-container">\[\delta \mathcal{L} = {\rm tr}(\delta C^T \overline{C}) = 
{\rm tr}(\delta A^T \overline{A}) + {\rm tr}(\delta B^T \overline{B})\]</p><p>It is easy to see</p><p class="math-container">\[\delta L = {\rm tr}((\delta A B)^T \overline C) + {\rm tr}((A \delta B)^T \overline C) = 
{\rm tr}(\delta A^T \overline{A}) + {\rm tr}(\delta B^T \overline{B})\]</p><p>We have the backward rules for matrix multiplication as</p><p class="math-container">\[\begin{align*}
&amp;\overline{A} = \overline{C}B^T\\
&amp;\overline{B} = A^T\overline{C}
\end{align*}\]</p><h3 id="Einsum"><a class="docs-heading-anchor" href="#Einsum">Einsum</a><a id="Einsum-1"></a><a class="docs-heading-anchor-permalink" href="#Einsum" title="Permalink"></a></h3><h3 id="Symmetric-Eigen-decomposition"><a class="docs-heading-anchor" href="#Symmetric-Eigen-decomposition">Symmetric Eigen decomposition</a><a id="Symmetric-Eigen-decomposition-1"></a><a class="docs-heading-anchor-permalink" href="#Symmetric-Eigen-decomposition" title="Permalink"></a></h3><p>Given a symmetric matrix <span>$A$</span>, the eigen decomposition is</p><p class="math-container">\[A = UEU^\dagger\]</p><p>Where <span>$U$</span> is the eigenvector matrix, and <span>$E$</span> is the eigenvalue matrix. The backward rules for the symmetric eigen decomposition are<sup class="footnote-reference"><a id="citeref-Seeger2017" href="#footnote-Seeger2017">[Seeger2017]</a></sup></p><p class="math-container">\[\overline{A} = U\left[\overline{E} + \frac{1}{2}\left(\overline{U}^\dagger U \circ F + h.c.\right)\right]U^\dagger\]</p><p>Where <span>$F_{ij}=(E_j- E_i)^{-1}$</span>.</p><p>If <span>$E$</span> is continuous, we define the density <span>$\rho(E) = \sum\limits_k \delta(E-E_k)=-\frac{1}{\pi}\int_k \Im[G^r(E, k)] $ (check sign!). Where $G^r(E, k) = \frac{1}{E-E_k+i\delta}$</span>.</p><p>We have</p><p class="math-container">\[\overline{A} = U\left[\overline{E} + \frac{1}{2}\left(\overline{U}^\dagger U \circ \Re [G(E_i, E_j)] + h.c.\right)\right]U^\dagger\]</p><h3 id="Singular-Value-Decomposition-(SVD)"><a class="docs-heading-anchor" href="#Singular-Value-Decomposition-(SVD)">Singular Value Decomposition (SVD)</a><a id="Singular-Value-Decomposition-(SVD)-1"></a><a class="docs-heading-anchor-permalink" href="#Singular-Value-Decomposition-(SVD)" title="Permalink"></a></h3><p><em>references</em>:</p><p>Complex valued SVD is defined as <span>$A = USV^\dagger$</span>. For simplicity, we consider a <strong>full rank square matrix</strong> <span>$A$</span>. Differentiating the SVD<sup class="footnote-reference"><a id="citeref-Wan2019" href="#footnote-Wan2019">[Wan2019]</a></sup><sup class="footnote-reference"><a id="citeref-Francuz2023" href="#footnote-Francuz2023">[Francuz2023]</a></sup>, we have</p><p class="math-container">\[dA = dUSV^\dagger + U dS V^\dagger + USdV^\dagger\]</p><p class="math-container">\[U^\dagger dA V = U^\dagger dU S + dS + SdV^\dagger V\]</p><p>Defining matrices <span>$dC=U^\dagger dU$</span> and <span>$dD = dV^\dagger V$</span> and <span>$dP = U^\dagger dA V$</span>, then we have</p><p class="math-container">\[\begin{cases}dC^\dagger+dC=0,\\dD^\dagger +dD=0\end{cases}\]</p><p>We have</p><p class="math-container">\[dP = dC S + dS + SdD\]</p><p>where <span>$dCS$</span> and <span>$SdD$</span> has zero real part in diagonal elements. So that <span>$dS = \Re[{\rm diag}(dP)]$</span>. </p><p class="math-container">\[\begin{align*}
d\mathcal{L} &amp;= {\rm Tr}\left[\overline{A}^TdA+\overline{A^*}^TdA^*\right]\\
&amp;= {\rm Tr}\left[\overline{A}^TdA+dA^\dagger\overline{A}^*\right] ~~~~~~~\#rule~3
\end{align*}\]</p><p>Easy to show <span>$\overline A_s = U^*\overline SV^T$</span>. Notice here, <span>$\overline A$</span> is the <strong>derivative</strong> rather than <strong>gradient</strong>, they are different by a conjugate, this is why we have transpose rather than conjugate here. see my <a href="https://giggleliu.github.io/2018/02/01/complex_bp.html">complex valued autodiff blog</a> for detail.</p><p>Using the relations <span>$dC^\dagger+dC=0$</span> and <span>$dD^\dagger+dD=0$</span> </p><pre><code class="nohighlight hljs">\begin{cases}
dPS + SdP^\dagger &amp;= dC S^2-S^2dC\\
SdP + dP^\dagger S &amp;= S^2dD-dD S^2
\end{cases}</code></pre><p class="math-container">\[\begin{cases}
dC = F\circ(dPS+SdP^\dagger)\\
dD = -F\circ (SdP+dP^\dagger S)
\end{cases}\]</p><p>where <span>$F_{ij} = \frac{1}{s_j^2-s_i^2}$</span>, easy to verify <span>$F^T = -F$</span>. Notice here, the relation between the imaginary diagonal parts  is lost</p><p class="math-container">\[\color{red}{\Im[I\circ dP] = \Im[I\circ(dC+dD)]}\]</p><p>This <strong>the missing diagonal imaginary part</strong> is definitely not trivial, but has been ignored for a long time until <a href="https://github.com/tensorflow/tensorflow/issues/13641#issuecomment-526976200">@refraction-ray</a> (Shixin Zhang) mentioned and solved it. Let&#39;s first focus on the off-diagonal contributions from <span>$dU$</span></p><p class="math-container">\[\begin{align*}
{\rm Tr}\overline U^TdU &amp;= {\rm Tr} \overline U ^TU dC + \overline U^T (I-UU^\dagger) dAVS^{-1}\\
&amp;= {\rm Tr}\overline U^T U (F\circ(dPS+SdP^\dagger))\\
 &amp;=  {\rm Tr}(dPS+SdP^\dagger)(-F\circ (\overline U^T U)) \# rule~1,2\\
 &amp;= {\rm Tr}(dPS+SdP^\dagger)J^T
\end{align*}\]</p><p>Here, we defined <span>$J=F\circ(U^T\overline U)$</span>.</p><p class="math-container">\[\begin{align*}
d\mathcal L &amp;= {\rm Tr} (dPS+SdP^\dagger)(J+J^\dagger)^T\\
&amp;= {\rm Tr} dPS(J+J^\dagger)^T+h.c.\\
&amp;= {\rm Tr} U^\dagger dA V S(J+J^\dagger)^T+h.c.\\
&amp;= {\rm Tr}\left[ VS(J+J^\dagger)^TU^\dagger\right] dA+h.c.
\end{align*}\]</p><p>By comparing with <span>$d\mathcal L = {\rm Tr}\left[\overline{A}^TdA+h.c. \right]$</span>, we have</p><p class="math-container">\[\bar A_U^{(\rm real)} =  \left[VS(J+J^\dagger)^TU^\dagger\right]^T\\
=U^*(J+J^\dagger)SV^T\]</p><h4 id="Update:-The-missing-diagonal-imaginary-part"><a class="docs-heading-anchor" href="#Update:-The-missing-diagonal-imaginary-part">Update: The missing diagonal imaginary part</a><a id="Update:-The-missing-diagonal-imaginary-part-1"></a><a class="docs-heading-anchor-permalink" href="#Update:-The-missing-diagonal-imaginary-part" title="Permalink"></a></h4><p>Now let&#39;s inspect the diagonal imaginary parts of <span>$dC$</span> and <span>$dD$</span> in Eq. 16. At a first glance, it is not sufficient to derive <span>$dC$</span> and <span>$dD$</span> from <span>$dP$</span>, but consider there is still an information not used, <strong>the loss must be gauge invariant</strong>, which means</p><p class="math-container">\[\mathcal{L}(U\Lambda, S, V\Lambda)\]</p><p>Should be independent of the choice of gauge <span>$\Lambda$</span>, which is defined as <span>${\rm diag}(e^i\phi, ...)$</span></p><p class="math-container">\[\begin{align*}
d\mathcal{L} &amp;={\rm Tr}[ \overline{U\Lambda}^T d(U\Lambda) +\overline  SdS+\overline{V\Lambda}^Td(V\Lambda)] + h.c.\\
&amp;={\rm Tr}[ \overline {U\Lambda}^T (dU\Lambda+Ud\Lambda) +\overline{S}dS+  \overline{V\Lambda}^T(Vd\Lambda +dV\Lambda)] + h.c.\\
&amp;= {\rm Tr}[(\overline{U\Lambda}^TU+\overline{V\Lambda}^TV )d\Lambda ] + \ldots + h.c.
\end{align*}\]</p><p>Gauge invariance refers to</p><p class="math-container">\[\overline{\Lambda} =  I\circ(\overline{U\Lambda}^TU+\overline{V\Lambda}^TV) = 0\]</p><p>For any <span>$\Lambda$</span>, where <span>$I$</span> refers to the diagonal mask matrix. It is of cause valid when <span>$\Lambda\rightarrow1$</span>, <span>$I\circ(\overline{U}^TU+\overline V^TV) = 0$</span>.</p><p>Consider the contribution from the <strong>diagonal imaginary part</strong>, we have</p><p class="math-container">\[\begin{align*}
&amp;{\rm Tr} [\overline U^T U (I \circ \Im [dC])+\overline V^T V (I \circ \Im [dD^\dagger])] + h.c.\\
&amp;={\rm Tr} [ I \circ (\overline U^T U)\Im [dC]-I\circ (\overline V^T V) \Im [dD]] +h.c. ~~~~~~~~~~~~~~\#  rule 1\\
&amp;={\rm Tr} [ I \circ (\overline U^T U)(\Im [dC]+ \Im [dD])] \\
&amp;={\rm Tr}[I\circ (\overline U^T U) \Im[dP]S^{-1}]  \\
&amp;={\rm Tr}[S^{-1}\Lambda_J U^{\dagger}dA V]\\
\end{align*}\]</p><p>where <span>$\Lambda_J  = \Im[I\circ(\overline U^TU)]= \frac 1 2I\circ(\overline U^TU)-h.c.$</span>, with <span>$I$</span> the mask for diagonal part. Since only the real part contribute to <span>$\delta \mathcal{L}$</span> (the imaginary part will be canceled by the Hermitian conjugate counterpart), we can safely move <span>$\Im$</span> from right to left.</p><p class="math-container">\[\color{red}{\bar A_{U+V}^{(\rm imag)} = U^*\Lambda_J S^{-1}V^T}\]</p><p>When <span>$U$</span> is <strong>not full rank</strong>, this formula should take an extra term (Ref. 2)</p><p class="math-container">\[\bar A_U^{(\rm real)} =U^*(J+J^\dagger)SV^T + (VS^{-1}\overline U^T(I-UU^\dagger))^T\]</p><p>Similarly, for <span>$V​$</span> we have</p><p class="math-container">\[\overline A_V^{(\rm real)} =U^*S(K+K^\dagger)V^T + (U S^{-1} \overline V^T (I - VV^\dagger))^*,\]</p><p>where <span>$K=F\circ(V^T\overline V)​$</span>.</p><p>To wrap up</p><p class="math-container">\[\overline A = \overline A_U^{\rm (real)} + \overline A_S + \overline A_V^{\rm (real)} +  \overline A_{U+V}^{\rm (imag)}\]</p><p>This result can be directly used in <strong>autograd</strong>.</p><p>For the <strong>gradient</strong> used in training, one should change the convention</p><p class="math-container">\[\mathcal{\overline A} = \overline A^*,\\ \mathcal{\overline U} = \overline U^*,\\ \mathcal{\overline V}= \overline V^*.\]</p><p>This convention is used in <strong>tensorflow</strong>, <strong>Zygote.jl</strong>. Which is</p><p class="math-container">\[\begin{align*}
\mathcal{\overline A} =&amp; U(\mathcal{J}+\mathcal{J}^\dagger)SV^\dagger + (I-UU^\dagger)\mathcal{\overline U}S^{-1}V^\dagger\\
&amp;+ U\overline SV^\dagger\\
&amp;+US(\mathcal{K}+\mathcal{K}^\dagger)V^\dagger + U S^{-1} \mathcal{\overline V}^\dagger (I - VV^\dagger)\\
&amp;\color{red}{+\frac 1 2 U (I\circ(U^\dagger\overline U)-h.c.)S^{-1}V^\dagger}
\end{align*}\]</p><p>where <span>$J=F\circ(U^\dagger\mathcal{\overline U})$</span> and <span>$K=F\circ(V^\dagger \mathcal{\overline V})$</span>.</p><h4 id="Rules"><a class="docs-heading-anchor" href="#Rules">Rules</a><a id="Rules-1"></a><a class="docs-heading-anchor-permalink" href="#Rules" title="Permalink"></a></h4><p>rule 1. <span>${\rm Tr} \left[A(C\circ B\right)] = \sum A^T\circ C\circ B = {\rm Tr} ((C\circ A^T)^TB)={\rm Tr}(C^T\circ A)B$</span></p><p>rule2. <span>$(C\circ A)^T = C^T \circ A^T$</span></p><p>rule3. When <span>$\mathcal L$</span> is real, </p><p class="math-container">\[\frac{\partial \mathcal{L}}{\partial x^*} =  \left(\frac{\partial \mathcal{L}}{\partial x}\right)^*\]</p><h3 id="QR-decomposition"><a class="docs-heading-anchor" href="#QR-decomposition">QR decomposition</a><a id="QR-decomposition-1"></a><a class="docs-heading-anchor-permalink" href="#QR-decomposition" title="Permalink"></a></h3><p>Let <span>$A$</span> be a full rank matrix, the QR decomposition is defined as</p><p class="math-container">\[A = QR\]</p><p>with <span>$Q^\dagger Q = \mathbb{I}$</span>, so that <span>$dQ^\dagger Q+Q^\dagger dQ=0$</span>. <span>$R$</span> is a complex upper triangular matrix, with diagonal part real.</p><p>The backward rules for QR decomposition are derived in multiple references, including <sup class="footnote-reference"><a id="citeref-Hubig2019" href="#footnote-Hubig2019">[Hubig2019]</a></sup> and <sup class="footnote-reference"><a id="citeref-Liao2019" href="#footnote-Liao2019">[Liao2019]</a></sup>. To derive the backward rules, we first consider differentiating the QR decomposition</p><p class="math-container">\[dA = dQR+QdR\]</p><p class="math-container">\[dQ = dAR^{-1}-QdRR^{-1}\]</p><p class="math-container">\[\begin{cases}
Q^\dagger dQ = dC - dRR^{-1}\\
dQ^\dagger Q =dC^\dagger - R^{-\dagger}dR^\dagger
\end{cases}\]</p><p>where <span>$dC=Q^\dagger dAR^{-1}$</span>.</p><p>Then</p><p class="math-container">\[dC+dC^\dagger = dRR^{-1} +(dRR^{-1})^\dagger\]</p><p>Notice <span>$dR$</span> is upper triangular and its diag is lower triangular, this restriction gives</p><p class="math-container">\[U\circ(dC+dC^\dagger) = dRR^{-1}\]</p><p>where <span>$U$</span> is a mask operator that its element value is <span>$1$</span> for upper triangular part, <span>$0.5$</span> for diagonal part and <span>$0$</span> for lower triangular part. One should also notice here both <span>$R$</span> and <span>$dR$</span> has real diagonal parts, as well as the product <span>$dRR^{-1}$</span>.</p><p>Now let&#39;s wrap up using the Zygote convension of gradient</p><p class="math-container">\[\begin{align*}
d\mathcal L &amp;= {\rm Tr}\left[\overline{\mathcal{Q}}^\dagger dQ+\overline{\mathcal{R}}^\dagger dR +h.c. \right]\\
&amp;={\rm Tr}\left[\overline{\mathcal{Q}}^\dagger dA R^{-1}-\overline{\mathcal{Q}}^\dagger QdR
R^{-1}+\overline{\mathcal{R}}^\dagger dR +h.c. \right]\\
&amp;={\rm Tr}\left[ R^{-1}\overline{\mathcal{Q}}^\dagger dA+ R^{-1}(-\overline{\mathcal{Q}}^\dagger Q +R\overline{\mathcal{R}}^\dagger) dR +h.c. \right]\\
&amp;={\rm Tr}\left[ R^{-1}\overline{\mathcal{Q}}^\dagger dA+ R^{-1}M dR +h.c. \right]
\end{align*}\]</p><p>here, <span>$M=R\overline{\mathcal{R}}^\dagger-\overline{\mathcal{Q}}^\dagger Q$</span>. Plug in <span>$dR$</span> we have</p><p class="math-container">\[\begin{align*}
d\mathcal{L}&amp;={\rm Tr}\left[ R^{-1}\overline{\mathcal{Q}}^\dagger dA + M \left[U\circ(dC+dC^\dagger)\right] +h.c. \right]\\
&amp;={\rm Tr}\left[ R^{-1}\overline{\mathcal{Q}}^\dagger dA + (M\circ L)(dC+dC^\dagger) +h.c. \right]  \;\;\# rule\; 1\\
&amp;={\rm Tr}\left[ (R^{-1}\overline{\mathcal{Q}}^\dagger dA+h.c.) + (M\circ L)(dC + dC^\dagger)+ (M\circ L)^\dagger (dC + dC^\dagger)\right]\\
&amp;={\rm Tr}\left[ R^{-1}\overline{\mathcal{Q}}^\dagger dA + (M\circ L+h.c.)dC + h.c.\right]\\
&amp;={\rm Tr}\left[ R^{-1}\overline{\mathcal{Q}}^\dagger dA + (M\circ L+h.c.)Q^\dagger dAR^{-1}\right]+h.c.\\
\end{align*}\]</p><p>where <span>$L =U^\dagger = 1-U$</span> is the mask of lower triangular part of a matrix.</p><p class="math-container">\[\begin{align*}
\mathcal{\overline A}^\dagger &amp;= R^{-1}\left[\overline{\mathcal{Q}}^\dagger + (M\circ L+h.c.)Q^\dagger\right]\\
\mathcal{\overline A} &amp;= \left[\overline{\mathcal{Q}} + Q(M\circ L+h.c.)\right]R^{-\dagger}\\
&amp;=\left[\overline{\mathcal{Q}} + Q \texttt{copyltu}(M)\right]R^{-\dagger}
\end{align*}\]</p><p>Here, the <span>$\texttt{copyltu}​$</span> takes conjugate when copying elements to upper triangular part.</p><h2 id="Obtaining-Hessian"><a class="docs-heading-anchor" href="#Obtaining-Hessian">Obtaining Hessian</a><a id="Obtaining-Hessian-1"></a><a class="docs-heading-anchor-permalink" href="#Obtaining-Hessian" title="Permalink"></a></h2><p>The second order gradient, Hessian, is also recognized as the Jacobian of the gradient. In practice, we can compute the Hessian by differentiating the gradient function with forward mode AD, which is also known as the forward-over-reverse mode AD.</p><h2 id="Optimal-checkpointing"><a class="docs-heading-anchor" href="#Optimal-checkpointing">Optimal checkpointing</a><a id="Optimal-checkpointing-1"></a><a class="docs-heading-anchor-permalink" href="#Optimal-checkpointing" title="Permalink"></a></h2><p>The main drawback of the reverse mode AD is the memory usage. The memory usage of the reverse mode AD is proportional to the number of intermediate variables, which scales linearly with the number of operations. The optimal checkpointing<sup class="footnote-reference"><a id="citeref-Griewank2008" href="#footnote-Griewank2008">[Griewank2008]</a></sup> is a technique to reduce the memory usage of the reverse mode AD. It is a trade-off between the memory and the computational cost. The optimal checkpointing is a step towards solving the memory wall problem</p><p>Given the binomial function <span>$\eta(\tau, \delta) = \frac{(\tau + \delta)!}{\tau!\delta!}$</span>, show that the following statement is true.</p><p class="math-container">\[\eta(\tau,\delta) = \sum_{k=0}^\delta \eta(\tau-1,k)\]</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Griewank2008"><a class="tag is-link" href="#citeref-Griewank2008">Griewank2008</a>Griewank A, Walther A. Evaluating derivatives: principles and techniques of algorithmic differentiation. Society for industrial and applied mathematics, 2008.</li><li class="footnote" id="footnote-Wan2019"><a class="tag is-link" href="#citeref-Wan2019">Wan2019</a>Wan, Zhou-Quan, and Shi-Xin Zhang. &quot;Automatic differentiation for complex valued SVD.&quot; arXiv preprint arXiv:1909.02659 (2019).</li><li class="footnote" id="footnote-Francuz2023"><a class="tag is-link" href="#citeref-Francuz2023">Francuz2023</a>Francuz, Anna, Norbert Schuch, and Bram Vanhecke. &quot;Stable and efficient differentiation of tensor network algorithms.&quot; arXiv preprint arXiv:2311.11894 (2023).</li><li class="footnote" id="footnote-Seeger2017"><a class="tag is-link" href="#citeref-Seeger2017">Seeger2017</a>Seeger, Matthias, et al. &quot;Auto-differentiating linear algebra.&quot; arXiv preprint arXiv:1710.08717 (2017).</li><li class="footnote" id="footnote-Giles2008"><a class="tag is-link" href="#citeref-Giles2008">Giles2008</a>Giles, Mike. &quot;An extended collection of matrix derivative results for forward and reverse mode automatic differentiation.&quot; (2008).</li><li class="footnote" id="footnote-Hubig2019"><a class="tag is-link" href="#citeref-Hubig2019">Hubig2019</a>Hubig, Claudius. &quot;Use and implementation of autodifferentiation in tensor network methods with complex scalars.&quot; arXiv preprint arXiv:1907.13422 (2019).</li><li class="footnote" id="footnote-Liao2019"><a class="tag is-link" href="#citeref-Liao2019">Liao2019</a>Liao, Hai-Jun, et al. &quot;Differentiable programming tensor networks.&quot; Physical Review X 9.3 (2019): 031041.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../optimization/">« Optimization</a><a class="docs-footer-nextpage" href="../../chap5/montecarlo/">Markov Chain Monte Carlo »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.0 on <span class="colophon-date" title="Wednesday 17 April 2024 08:56">Wednesday 17 April 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
