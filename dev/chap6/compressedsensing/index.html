<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Compressed sensing · Scientific Computing For Physicists</title><meta name="title" content="Compressed sensing · Scientific Computing For Physicists"/><meta property="og:title" content="Compressed sensing · Scientific Computing For Physicists"/><meta property="twitter:title" content="Compressed sensing · Scientific Computing For Physicists"/><meta name="description" content="Documentation for Scientific Computing For Physicists."/><meta property="og:description" content="Documentation for Scientific Computing For Physicists."/><meta property="twitter:description" content="Documentation for Scientific Computing For Physicists."/><meta property="og:url" content="https://book.jinguo-group.science/chap6/compressedsensing/"/><meta property="twitter:url" content="https://book.jinguo-group.science/chap6/compressedsensing/"/><link rel="canonical" href="https://book.jinguo-group.science/chap6/compressedsensing/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Scientific Computing For Physicists</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Become an Open-source Developer</span><ul><li><a class="tocitem" href="../../chap1/terminal/">Get a Terminal!</a></li><li><a class="tocitem" href="../../chap1/git/">Maintainability - Version Control</a></li><li><a class="tocitem" href="../../chap1/ci/">Correctness - Unit Tests</a></li></ul></li><li><span class="tocitem">Julia Programming Language</span><ul><li><a class="tocitem" href="../../chap2/julia-setup/">Setup Julia</a></li><li><a class="tocitem" href="../../chap2/julia-why/">Why Julia?</a></li><li><a class="tocitem" href="../../chap2/julia-type/">Types and Multiple-dispatch</a></li><li><a class="tocitem" href="../../chap2/julia-array/">Array and Broadcasting</a></li><li><a class="tocitem" href="../../chap2/julia-release/">My First Package</a></li><li><a class="tocitem" href="../../chap2/julia-fluid/">Project: Fluid dynamics</a></li></ul></li><li><span class="tocitem">Linear Algebra</span><ul><li><a class="tocitem" href="../../chap3/linalg/">Matrix Computation</a></li><li><a class="tocitem" href="../../chap3/lu/">Solving linear equations by LU factorization: Bottom-up</a></li><li><a class="tocitem" href="../../chap3/qr/">QR Factorization: Bottom-up</a></li><li><a class="tocitem" href="../../chap3/fft/">Fast Fourier transform</a></li><li><a class="tocitem" href="../../chap3/sensitivity/">Sensitivity Analysis</a></li><li><a class="tocitem" href="../../chap3/sparse/">Sparse Matrices and Graphs</a></li></ul></li><li><span class="tocitem">Tensors and Tensor Networks</span><ul><li><a class="tocitem" href="../../chap3/tensors/">Tensor Operations</a></li></ul></li><li><span class="tocitem">Optimization</span><ul><li><a class="tocitem" href="../../chap4/optimization/">Optimization</a></li><li><a class="tocitem" href="../../chap4/ad/">Automatic Differentiation</a></li><li><a class="tocitem" href="../../chap5/complexity/">Computational complexity</a></li></ul></li><li><span class="tocitem">Randomness</span><ul><li><a class="tocitem" href="../../chap5/montecarlo/">Markov Chain Monte Carlo</a></li></ul></li><li><span class="tocitem">Appendix</span><ul><li><a class="tocitem" href="../../append/plotting/">Plotting recipes with CairoMakie</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Compressed sensing</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Compressed sensing</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/GiggleLiu/ScientificComputingForPhysicists" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/GiggleLiu/ScientificComputingForPhysicists/blob/main/docs/src/chap6/compressedsensing.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Compressed-sensing"><a class="docs-heading-anchor" href="#Compressed-sensing">Compressed sensing</a><a id="Compressed-sensing-1"></a><a class="docs-heading-anchor-permalink" href="#Compressed-sensing" title="Permalink"></a></h1><p>using DataStructures using Plots using StatsBase using Interpolations using JuMP, SCS using LinearAlgebra</p><p>using Images</p><p>using NLSolversBase</p><p>using Optim</p><p>using FiniteDifferences</p><h1 id="Sparsity-Detection"><a class="docs-heading-anchor" href="#Sparsity-Detection">Sparsity Detection</a><a id="Sparsity-Detection-1"></a><a class="docs-heading-anchor-permalink" href="#Sparsity-Detection" title="Permalink"></a></h1><p>Beyond sparse matrices and Principle Component Analysis (PCA)!&quot;</p><h1 id="Information"><a class="docs-heading-anchor" href="#Information">Information</a><a id="Information-1"></a><a class="docs-heading-anchor-permalink" href="#Information" title="Permalink"></a></h1><p>A measure of randomness, usually measured by the entropy</p><p class="math-container">\[S = -\sum_k p_k\log p_k.\]</p><p>Quiz: Which knowledge bellow removes more information?</p><ol><li>When I toss a coin, its head side will be up,</li><li>Tomorrow will rain,</li><li>Today&#39;s lecture will be a successful one.</li></ol><h1 id="Huffman-coding"><a class="docs-heading-anchor" href="#Huffman-coding">Huffman coding</a><a id="Huffman-coding-1"></a><a class="docs-heading-anchor-permalink" href="#Huffman-coding" title="Permalink"></a></h1><p>In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding or using such a code proceeds by means of Huffman coding.</p><p>Ref: <a href="https://www.programiz.com/dsa/huffman-coding">https://www.programiz.com/dsa/huffman-coding</a>&quot;</p><p><strong>Task</strong>: describe the following image in computer.&quot;</p><p>Mondrian - Trafalgar Square, 1939-43 - a picture having little information from various perspective.&quot;</p><p>LocalResource(&quot;images/trafalgar-square.jpg&quot;, :width=&gt;300)</p><h2 id="The-naive-approach"><a class="docs-heading-anchor" href="#The-naive-approach">The naive approach</a><a id="The-naive-approach-1"></a><a class="docs-heading-anchor-permalink" href="#The-naive-approach" title="Permalink"></a></h2><ul><li>R: 000</li><li>Y: 001</li><li>B: 010</li><li>K: 011</li><li>W: 100</li></ul><p>We need <span>$3mn$</span> bits to store this image. Can we do better?&quot;</p><h2 id="Observation"><a class="docs-heading-anchor" href="#Observation">Observation</a><a id="Observation-1"></a><a class="docs-heading-anchor-permalink" href="#Observation" title="Permalink"></a></h2><p>Calculate the frequency of each color in the image.</p><ul><li>R: 3%</li><li>Y: 7%</li><li>B: 1%</li><li>K: 10%</li><li>W: 79%</li></ul><h2 id="Formalized-description"><a class="docs-heading-anchor" href="#Formalized-description">Formalized description</a><a id="Formalized-description-1"></a><a class="docs-heading-anchor-permalink" href="#Formalized-description" title="Permalink"></a></h2><p><strong>Input</strong></p><p>Alphabet <span>$A=(a_{1},a_{2},\dots ,a_{n})$</span>, which is the symbol alphabet of size <span>$n$</span>. Tuple <span>${\displaystyle W=(w_{1},w_{2},\dots ,w_{n})}$</span>, which is the tuple of the (positive) symbol weights (usually proportional to probabilities), i.e. <span>${\displaystyle w_{i}=\operatorname {weight} \left(a_{i}\right),\,i\in \{1,2,\dots ,n\}}$</span>.</p><p><strong>Output</strong></p><p>Code  <span>${\displaystyle C\left(W\right)=(c_{1},c_{2},\dots ,c_{n})}$</span>, which is the tuple of (binary) codewords, where <span>$c_{i}$</span> is the codeword for <span>${\displaystyle a_{i},\,i\in \{1,2,\dots ,n\}}$</span>.</p><p><strong>Goal</strong></p><p>Let <span>${\textstyle L\left(C\left(W\right)\right)=\sum _{i=1}^{n}{w_{i}\operatorname {length} \left(c_{i}\right)}}$</span> be the weighted path length of code <span>$C$</span>. Condition: <span>${\displaystyle L\left(C\left(W\right)\right)\leq L\left(T\left(W\right)\right)}$</span> for any code <span>${\displaystyle T\left(W\right)}$</span>.</p><h2 id="Algorithm"><a class="docs-heading-anchor" href="#Algorithm">Algorithm</a><a id="Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Algorithm" title="Permalink"></a></h2><p>The simplest construction algorithm uses a priority queue where the node with lowest probability is given highest priority:</p><ol><li>Create a leaf node for each symbol and add it to the priority queue.</li><li>While there is more than one node in the queue:<ol><li>Remove the two nodes of highest priority (lowest probability) from the queue</li><li>Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes&#39; probabilities.</li><li>Add the new node to the queue.</li></ol></li><li>The remaining node is the root node and the tree is complete.</li></ol><p>Since efficient priority queue data structures require <span>$O(\log n)$</span> time per insertion, and a tree with n leaves has <span>$2n−1$</span> nodes, this algorithm operates in <span>$O(n \log n)$</span> time, where <span>$n$</span> is the number of symbols.</p><h2 id="Implementation&quot;"><a class="docs-heading-anchor" href="#Implementation&quot;">Implementation&quot;</a><a id="Implementation&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation&quot;" title="Permalink"></a></h2><p>Build a huffman tree</p><p>struct Node{VT, PT}     value::Union{VT,Nothing} 	prob::PT     left::Union{Node{VT,PT}, Nothing}     right::Union{Node{VT,PT}, Nothing} end</p><p>function huffman_tree(symbols, probs) 	isempty(symbols) &amp;&amp; error(&quot;empty input!&quot;) 	# priority queue can keep the items ordered with log(# of items) effort. 	nodes = PriorityQueue(Base.Order.Forward, 		[Node(c, f, nothing, nothing)=&gt;f for (c, f) in zip(symbols, probs)])     while length(nodes) &gt; 1         left = dequeue!(nodes)         right = dequeue!(nodes)         parent = Node(nothing, left.prob + right.prob, left, right)         enqueue!(nodes, parent=&gt;left.prob + right.prob)     end 	return dequeue!(nodes) end</p><p>ht = huffman_tree(&quot;RYBKW&quot;, [0.03, 0.07, 0.01, 0.1, 0.79])</p><p>From the tree, we generate the binary code.&quot;</p><p>function decent!(tree::Node{VT}, prefix::String=&quot;&quot;, d::Dict = Dict{VT,String}()) where VT 	if tree.left === nothing # leaft 		d[tree.value] = prefix 	else   # non-leaf 		decent!(tree.left, prefix<em>&quot;0&quot;, d) 		decent!(tree.right, prefix</em>&quot;1&quot;, d) 	end 	return d end</p><p>code_dict = decent!(ht)</p><p>mean<em>code</em>length = let 	code<em>length = 0.0 	for (symbol, prob) in zip(&quot;RYBKW&quot;, [0.03, 0.07, 0.01, 0.1, 0.79]) 		code</em>length += length(code<em>dict[symbol]) * prob 	end 	code</em>length end</p><p>We only need <span>$1.36mn$</span> bits to represent the Mondrian&#39;s Trafalgar Square!</p><h2 id="The-optimality&quot;"><a class="docs-heading-anchor" href="#The-optimality&quot;">The optimality&quot;</a><a id="The-optimality&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#The-optimality&quot;" title="Permalink"></a></h2><p><em>Lemma</em>: Huffman Encoding produces an optimal tree.</p><p>The compressed text has a minimum size of <span>$Sn$</span>, where</p><p class="math-container">\[S = -\sum_k p_k\log p_k.\]</p><p>It is reached when all non-leaf nodes in the tree are ballanced, i.e. having the same weight for left and right children.</p><p>S_trafalgar = StatsBase.entropy([0.03, 0.07, 0.01, 0.1, 0.79], 2)</p><h1 id="Matrix-Product-State/Tensor-Train"><a class="docs-heading-anchor" href="#Matrix-Product-State/Tensor-Train">Matrix Product State/Tensor Train</a><a id="Matrix-Product-State/Tensor-Train-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-Product-State/Tensor-Train" title="Permalink"></a></h1><p>Calculate the compression ratio.</p><h1 id="Compressed-Sensing"><a class="docs-heading-anchor" href="#Compressed-Sensing">Compressed Sensing</a><a id="Compressed-Sensing-1"></a><a class="docs-heading-anchor-permalink" href="#Compressed-Sensing" title="Permalink"></a></h1><p>Reference: <a href="https://www.pyrunner.com/weblog/B/index.html">https://www.pyrunner.com/weblog/B/index.html</a></p><h2 id="Example-1:-Two-sinusoids&quot;"><a class="docs-heading-anchor" href="#Example-1:-Two-sinusoids&quot;">Example 1: Two sinusoids&quot;</a><a id="Example-1:-Two-sinusoids&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#Example-1:-Two-sinusoids&quot;" title="Permalink"></a></h2><p>import FFTW</p><p>Let us define a function of adding two sinusoids.&quot;</p><h1 id="sum-of-two-sinusoids"><a class="docs-heading-anchor" href="#sum-of-two-sinusoids">sum of two sinusoids</a><a id="sum-of-two-sinusoids-1"></a><a class="docs-heading-anchor-permalink" href="#sum-of-two-sinusoids" title="Permalink"></a></h1><p>n = 5000</p><h1 id="time-sequence"><a class="docs-heading-anchor" href="#time-sequence">time sequence</a><a id="time-sequence-1"></a><a class="docs-heading-anchor-permalink" href="#time-sequence" title="Permalink"></a></h1><p>t = LinRange(0, 1/8, n)</p><h1 id="the-function"><a class="docs-heading-anchor" href="#the-function">the function</a><a id="the-function-1"></a><a class="docs-heading-anchor-permalink" href="#the-function" title="Permalink"></a></h1><p>y = sin.(1394π .* t) + sin.(3266π .* t)</p><p>plot(t, y; label=&quot;the original function&quot;)</p><h1 id="the-function-in-the-spectrum-domain"><a class="docs-heading-anchor" href="#the-function-in-the-spectrum-domain">the function in the spectrum domain</a><a id="the-function-in-the-spectrum-domain-1"></a><a class="docs-heading-anchor-permalink" href="#the-function-in-the-spectrum-domain" title="Permalink"></a></h1><p>yt = FFTW.dct(y)</p><p>plot(t, yt; label=&quot;the function in frequency domain&quot;)</p><p>Let us extract 10% samples from it.</p><p>m = 500</p><h1 id="not-allowing-repeated-indices"><a class="docs-heading-anchor" href="#not-allowing-repeated-indices">not allowing repeated indices</a><a id="not-allowing-repeated-indices-1"></a><a class="docs-heading-anchor-permalink" href="#not-allowing-repeated-indices" title="Permalink"></a></h1><p>samples = sort!(StatsBase.sample(1:n, m, replace=false))</p><p>t2 = t[samples]</p><p>y2 = y[samples]</p><p>let 	plt = plot(t, y; label=&quot;the samples&quot;) 	scatter!(plt, t2, y2; label=&quot;the generated samples&quot;, markersize=2) end</p><p>If we plot it directly, it looks not so good&quot;</p><p>interp<em>linear = linear</em>interpolation(t2, y2; extrapolation_bc=Line())</p><p>plot(t, interp_linear.(t))</p><p>Why? Because we haven&#39;t used a prior that it is sparse in the frequency domain.&quot;</p><p>plot(t, FFTW.dct(interp_linear.(t)))</p><p>Instead, we rephrase the problem as the following convex optimization problem</p><p class="math-container">\[\begin{align}
&amp;\min\sum_i |x_i|\\
&amp;s.t. A x = b
\end{align}\]</p><p>model = Model(SCS.Optimizer)</p><p>A = FFTW.idct(Matrix{Float64}(I, n, n), 1)[samples, :]</p><h1 id="do-L1-optimization"><a class="docs-heading-anchor" href="#do-L1-optimization">do L1 optimization</a><a id="do-L1-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#do-L1-optimization" title="Permalink"></a></h1><p>@variable model x[1:n];</p><p>@variable(model, norm1);</p><p>@constraint model A * x .== y2;</p><p>We use <span>$l_1$</span> norm because <span>$l_0$</span> is very hard to optimize.&quot;</p><p>@constraint(model, [norm1; x] in MOI.NormOneCone(1 + length(x)));</p><p>@objective(model, Min, norm1);</p><p>@bind run_optimize CheckBox()</p><p>if run_optimize 	optimize!(model) 	plot([JuMP.value.(x), FFTW.idct(JuMP.value.(x))]; layout=(2, 1), xlim=(0, 1000), labels=[&quot;spectrum&quot;, &quot;recovered&quot;]) end</p><h1 id="A*JuMP.value.(x)-y2"><a class="docs-heading-anchor" href="#A*JuMP.value.(x)-y2">A*JuMP.value.(x) - y2</a><a id="A*JuMP.value.(x)-y2-1"></a><a class="docs-heading-anchor-permalink" href="#A*JuMP.value.(x)-y2" title="Permalink"></a></h1><p>A * FFTW.dct(interp_linear.(t)) - y2</p><h1 id="norm(JuMP.value.(x),-1)"><a class="docs-heading-anchor" href="#norm(JuMP.value.(x),-1)">norm(JuMP.value.(x), 1)</a><a id="norm(JuMP.value.(x),-1)-1"></a><a class="docs-heading-anchor-permalink" href="#norm(JuMP.value.(x),-1)" title="Permalink"></a></h1><p>norm(FFTW.dct(interp_linear.(t)), 1)</p><h2 id="Example-2:-Recovering-an-image&quot;"><a class="docs-heading-anchor" href="#Example-2:-Recovering-an-image&quot;">Example 2: Recovering an image&quot;</a><a id="Example-2:-Recovering-an-image&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#Example-2:-Recovering-an-image&quot;" title="Permalink"></a></h2><p><img src="https://www.pyrunner.com/media/uploads/zinnia/2016/05/26/compressed_sensing_slide_2.png" alt/></p><h2 id="Creating-a-Julia-Package"><a class="docs-heading-anchor" href="#Creating-a-Julia-Package">Creating a Julia Package</a><a id="Creating-a-Julia-Package-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-a-Julia-Package" title="Permalink"></a></h2><ol><li>Go to the folder for package development,</li></ol><pre><code class="language-bash hljs">cd path/to/julia/dev/folder</code></pre><ol><li>Type the following command</li></ol><pre><code class="language-julia hljs">julia&gt; using PkgTemplates

julia&gt; tpl = Template(; user=&quot;GiggleLiu&quot;, plugins=[
           GitHubActions(; extra_versions=[&quot;nightly&quot;]),
           Git(),
           Codecov()
	], dir=pwd())

julia&gt; tpl(&quot;CompressedSensingTutorial&quot;)</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Please replace <code>GiggleLiu</code> with your own user name, the <code>CompressedSensingTutorial</code> with your own package name! Please check the <a href="https://juliaci.github.io/PkgTemplates.jl/stable/user/">document of PkgTemplates</a>. Now you should see a new package in your current folder.</p></div></div><ol><li>develop your project with, e.g., <a href="https://www.julia-vscode.org/">VSCode</a>.</li></ol><pre><code class="language-bash hljs">cd CompressedSensingTutorial

code .</code></pre><p>In the VSCode, please use your project environment as your julia project environment, check <a href="https://www.julia-vscode.org/docs/dev/userguide/env/">here</a>.</p><ol><li>Please configure your project dependency by typing in the <code>pkg&gt;</code> mode.</li></ol><pre><code class="language-julia hljs">(CompressedSensingTutorial) pkg&gt; add FFTW FiniteDifferences Images Optim ...</code></pre><p>https://github.com/timholy/Revise.jl&quot;</p><p>source_img = Gray.(Images.load(joinpath(@<strong>DIR</strong>, &quot;images/waterfall.jpeg&quot;)))</p><p>img = Float64.(source_img);</p><p>size(img)</p><p>Gray.(FFTW.dct(img))</p><h1 id="We-have-to-use-the-Pluto-ingredients-for-loading-a-local-project"><a class="docs-heading-anchor" href="#We-have-to-use-the-Pluto-ingredients-for-loading-a-local-project">We have to use the Pluto ingredients for loading a local project</a><a id="We-have-to-use-the-Pluto-ingredients-for-loading-a-local-project-1"></a><a class="docs-heading-anchor-permalink" href="#We-have-to-use-the-Pluto-ingredients-for-loading-a-local-project" title="Permalink"></a></h1><h1 id="Please-check-the-issue:-https://github.com/fonsp/Pluto.jl/issues/115#issuecomment-661722426"><a class="docs-heading-anchor" href="#Please-check-the-issue:-https://github.com/fonsp/Pluto.jl/issues/115#issuecomment-661722426">Please check the issue: https://github.com/fonsp/Pluto.jl/issues/115#issuecomment-661722426</a><a id="Please-check-the-issue:-https://github.com/fonsp/Pluto.jl/issues/115#issuecomment-661722426-1"></a><a class="docs-heading-anchor-permalink" href="#Please-check-the-issue:-https://github.com/fonsp/Pluto.jl/issues/115#issuecomment-661722426" title="Permalink"></a></h1><p>CT = mod.CompressedSensingTutorial</p><p>Let us check the project!&quot;</p><p>pixels = CT.sample<em>image</em>pixels(img, 0.1)</p><p>The objective function.&quot;</p><p>Gray.(CT.zero_padded(pixels, pixels.values))</p><p>@bind do<em>compressed</em>sensing CheckBox()</p><p>@doc CT.sensing_image</p><p>newimg = if do<em>compressed</em>sensing 	CT.sensing<em>image(pixels; C=0.005, optimizer=:LBFGS, show</em>trace=false, linesearch=Optim.HagerZhang()) else 	rand(size(img)...) end;</p><p>Gray.(newimg)</p><p>FFTW.dct(newimg)</p><p>newimg[pixels.indices] .- pixels.values</p><h2 id="Related-research-works"><a class="docs-heading-anchor" href="#Related-research-works">Related research works</a><a id="Related-research-works-1"></a><a class="docs-heading-anchor-permalink" href="#Related-research-works" title="Permalink"></a></h2><ul><li><a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.105.150401">Quantum State Tomography via Compressed Sensing.</a> David Gross, Yi-Kai Liu, Steven T. Flammia, Stephen Becker, and Jens Eisert. Phys. Rev. Lett. 105, 150401 – Published 4 October 2010</li></ul><h1 id="Kernel-PCA"><a class="docs-heading-anchor" href="#Kernel-PCA">Kernel PCA</a><a id="Kernel-PCA-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-PCA" title="Permalink"></a></h1><h2 id="References:"><a class="docs-heading-anchor" href="#References:">References:</a><a id="References:-1"></a><a class="docs-heading-anchor-permalink" href="#References:" title="Permalink"></a></h2><ul><li><a href="https://www.jmlr.org/papers/v7/micchelli06a.html">Universal Kernels</a>, Charles A. Micchelli, Yuesheng Xu, Haizhang Zhang; 2006.</li><li><a href="https://link.springer.com/chapter/10.1007/BFb0020217">Kernel Principal Component Analysis</a>, Bernhard Scholkopf, Alexander Smola, Klaus Robert Muller, 1997</li><li><a href="https://jmlr.org/papers/v12/sriperumbudur11a.html">Universality, Characteristic Kernels and RKHS Embedding of Measures</a> Sriperumbudur, B. K., Fukumizu, K. &amp; Lanckriet, G. R. G. Journal of Machine Learning Research 12, 2389–2410 (2011).</li></ul><h2 id="Kernel-Method"><a class="docs-heading-anchor" href="#Kernel-Method">Kernel Method</a><a id="Kernel-Method-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-Method" title="Permalink"></a></h2><h3 id="From-dot-product-to-distance"><a class="docs-heading-anchor" href="#From-dot-product-to-distance">From dot product to distance</a><a id="From-dot-product-to-distance-1"></a><a class="docs-heading-anchor-permalink" href="#From-dot-product-to-distance" title="Permalink"></a></h3><p>Let <span>$x, y \in R^n$</span> be two vectors, their distance is defined by</p><p class="math-container">\[{\rm dist}(x, y) = \|x - y\|^2 = x^T x + y^T y - 2y^T x\]</p><p>If we can defined an inner product between two vectors, we can defined a measure of distance.</p><h3 id="Kernel-functions"><a class="docs-heading-anchor" href="#Kernel-functions">Kernel functions</a><a id="Kernel-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-functions" title="Permalink"></a></h3><p>By extending the dot product by an arbitrary symmetric positive definite kernel funciton.</p><p class="math-container">\[x^T y \rightarrow \kappa(x, y)\]</p><p>We have a new measure of distance as</p><p class="math-container">\[{\rm dist}_\kappa(x, y) = \kappa(x, x) + \kappa(y, y) - 2\kappa(x, y)\]</p><p>Given a kernel function, there is a mapping from the original vector space to reproducing kernel Hilbert space (RKHS) associated with it. The kernel function of two vectors can be defined as an inner product of their images in the RKHS.</p><p class="math-container">\[\kappa(x, y) = \phi(x)^T\phi(y)\]</p><p>It is important to note that <span>$\kappa(\cdot, x)$</span> is nolonger a linear function under this definition of inner product.&quot;</p><h3 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h3><p>Let <span>$x, y\in \mathbb{R}^2$</span>, a polynomial kernel of order 2 can be defined as</p><p class="math-container">\[\kappa(x, y) = (x^T y)^2\]</p><p>Then we can express the mapping from a vector to RKHS as</p><p class="math-container">\[\phi(x) = \left(\begin{matrix}x_1^2\\x_2^2\\x_1x_2\\x_2x_1\end{matrix}\right)\]</p><h3 id="Universality-of-a-kernel"><a class="docs-heading-anchor" href="#Universality-of-a-kernel">Universality of a kernel</a><a id="Universality-of-a-kernel-1"></a><a class="docs-heading-anchor-permalink" href="#Universality-of-a-kernel" title="Permalink"></a></h3><p>A kernel <span>$\kappa$</span> is universal if and only if the following equation is a universal function approximator.</p><p class="math-container">\[f = \sum_{j=1}^n c_j \kappa(\cdot, x_j),\]</p><p>where <span>$c_j \in \mathbb{R}$</span> and <span>$x_j$</span> can be either a number of a vector.</p><p>As noted in Micchelli et al. (2006), one can ask whether the function, <span>$f$</span> in the above euqation approximates any real-valued target function arbitrarily well as the number of summands increases without bound. This is an important question to consider because if the answer is affirmative, then the kernel-based learning algorithm can be consistent in the sense that for any target function, <span>$f^⋆$</span>, the discrepancy between <span>$f$</span> (which is learned from the training data) and <span>$f^⋆$</span> goes to zero (in some appropriate sense) as the sample size goes to infinity.</p><h2 id="Kernel-Principle-Component-Analysis-(PCA)"><a class="docs-heading-anchor" href="#Kernel-Principle-Component-Analysis-(PCA)">Kernel Principle Component Analysis (PCA)</a><a id="Kernel-Principle-Component-Analysis-(PCA)-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-Principle-Component-Analysis-(PCA)" title="Permalink"></a></h2><p>The linear PCA starts from computing a convariance matrix of the data <span>$x_k \in \mathbb{R}^N$</span>, for <span>$k=1,\ldots, l$</span>. We assume the data is centralized, i.e. <span>$\sum_{k} x_k=0$</span>. Then the covariance matrix is defined as</p><p class="math-container">\[C = \frac{1}{l}\sum_{k=1}^l x_k x_k^T\]</p><p>The new coordinates in the eigenvector basis of <span>$C$</span> are called principle components.</p><p>The kernel PCA is defined as the PCA in the RKHS, i.e. </p><p class="math-container">\[\overline{C} = \frac{1}{l}\sum_{k=1}^l \phi(x_k)\phi(x_k)^T\]</p><p>The eigenvalue problem to solve is</p><p class="math-container">\[\lambda V = \overline{C} V\]</p><p>where <span>$V=\sum_{k=1}^l \alpha_k \phi(x_k)$</span></p><p>By projecting this eigenvalue problem into this subspace, we obtain the following equivalent form</p><p class="math-container">\[l\lambda \alpha = K \alpha\]</p><p>where <span>$K_{ij} = K(x_i, x_j)$</span>.</p><p>let 	video = html&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/hmBTACBGWJs&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen&gt;&lt;/iframe&gt;</p><h1 id="Homework"><a class="docs-heading-anchor" href="#Homework">Homework</a><a id="Homework-1"></a><a class="docs-heading-anchor-permalink" href="#Homework" title="Permalink"></a></h1><h3 id="1.-Autodiff"><a class="docs-heading-anchor" href="#1.-Autodiff">1. Autodiff</a><a id="1.-Autodiff-1"></a><a class="docs-heading-anchor-permalink" href="#1.-Autodiff" title="Permalink"></a></h3><p>Given <span>$A\in \mathbb{R}^{n\times n}$</span> and <span>$x, b\in \mathbb{R}^n$</span>. Please derive the backward rule of <span>$\mathcal{L} = \|Ax - b\|_2$</span> either using the chain rules or the perturbative approach (from the last lecture).</p><h3 id="2.-Sparsity-detection"><a class="docs-heading-anchor" href="#2.-Sparsity-detection">2. Sparsity detection</a><a id="2.-Sparsity-detection-1"></a><a class="docs-heading-anchor-permalink" href="#2.-Sparsity-detection" title="Permalink"></a></h3><p>Choose one.</p><h5 id="(a).-Text-compression"><a class="docs-heading-anchor" href="#(a).-Text-compression">(a). Text compression</a><a id="(a).-Text-compression-1"></a><a class="docs-heading-anchor-permalink" href="#(a).-Text-compression" title="Permalink"></a></h5><p>Given a text to be compressed:</p><pre><code class="nohighlight hljs">Compressed sensing (also known as compressive sensing, compressive sampling, or sparse sampling) is a signal processing technique for efficiently acquiring and reconstructing a signal, by finding solutions to underdetermined linear systems. This is based on the principle that, through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the Nyquist–Shannon sampling theorem. There are two conditions under which recovery is possible. The first one is sparsity, which requires the signal to be sparse in some domain. The second one is incoherence, which is applied through the isometric property, which is sufficient for sparse signals.</code></pre><p>Please</p><ol><li>Analyse the frequency of each char</li><li>Create an optimal Huffman coding for each char</li><li>Encode the text and count the length of total coding (not including the deliminators).</li></ol><h5 id="(b).-Compressed-Sensing"><a class="docs-heading-anchor" href="#(b).-Compressed-Sensing">(b). Compressed Sensing</a><a id="(b).-Compressed-Sensing-1"></a><a class="docs-heading-anchor-permalink" href="#(b).-Compressed-Sensing" title="Permalink"></a></h5><p>Go through the video clip <a href="https://youtu.be/hmBTACBGWJs">Compressed Sensing: When It Works</a></p><p>video</p><p>Please summarize this video clip, and explain when does compressed sensing work and when not.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If you are interested in knowing more about compressed sensing, please do not miss this youtube video paylist: <a href="https://youtube.com/playlist?list=PLMrJAkhIeNNRHP5UA-gIimsXLQyHXxRty">https://youtube.com/playlist?list=PLMrJAkhIeNNRHP5UA-gIimsXLQyHXxRty</a></p></div></div><p>end</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.0 on <span class="colophon-date" title="Wednesday 24 April 2024 10:01">Wednesday 24 April 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
