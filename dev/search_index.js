var documenterSearchIndex = {"docs":
[{"location":"chap4/hosvd/#Higher-order-SVD","page":"-","title":"Higher order SVD","text":"","category":"section"},{"location":"chap4/hosvd/#Tucker-decomposition","page":"-","title":"Tucker decomposition","text":"","category":"section"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"Suppose A in R^n_1times n_2times n_3 and assume that r leq rm rank(A)with inequality in at least one component. Prompted by the optimality properties of the matrix SVD, let us consider the following optimization problem:","category":"page"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"min_X  A - X _F","category":"page"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"such that ","category":"page"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"X_lmn = sum_j_1=j_2=j_3=1^r_1 r_2 r_3 S_j_1j_2j_3 (U_1)_lj_1(U_2)_mj_2(U_3)_nj_3","category":"page"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"We refer to this as the Tucker approximation problem.","category":"page"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"The pseudocode for Tucker decomposition algorithm:","category":"page"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"beginalign*\ntextttRepeat\ntextttfor k = lldotsd\ntextCompute the SVD\nA(k) (U_d otimes ldots otimes U_k+1 otimes U_k-1 otimes ldots otimes U_1) = tildeU_kSigma_kV_k^T\nU_k = tildeU_k(1r_k)\ntextttend\nendalign*","category":"page"},{"location":"chap4/hosvd/#CP-decomposition","page":"-","title":"CP decomposition","text":"","category":"section"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"Given X in R^n_1 times n_2times n_3 and an integer r, we consider the problem","category":"page"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"min_X A - X","category":"page"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"such that ","category":"page"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"X_lmn = sum_j=1^rlambda_j F_lj G_mj H_nj","category":"page"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"where Fin R^n_1times r, Gin R^n_2times r, and Hin R^n_3times r. This is an example of the CP approximation problem. We assume that the columns of F, G, and H have unit 2-norm.","category":"page"},{"location":"chap4/hosvd/","page":"-","title":"-","text":"beginalign*\ntextttRepeat\ntextttfork= ld \ntextMinimize  A_(k) - tildeF^(k) (F^(d) odot ldotsodot F^(k+ l) odot F^(k-1) odot ldots odot F^(1))_F\ntext with respect to tildeF(k)\ntextttforj = lr\nlambda_j = tildeF_(k)( j)\nF^(k)(j) = tildeF_k ( j)lambda_j\ntextttend\ntextttend\nendalign*","category":"page"},{"location":"chap3/linalg/#Matrix-Computation","page":"Matrix Computation","title":"Matrix Computation","text":"","category":"section"},{"location":"chap3/linalg/#Matrix-multiplication","page":"Matrix Computation","title":"Matrix multiplication","text":"","category":"section"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"Matrix multiplication is a fundamental operation in linear algebra. Given two matrices Ain mathbbC^mtimes n and Bin mathbbC^ntimes p, the product C = AB is defined as","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"C_ij = sum_k=1^n A_ikB_kj","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"The time complexity of matrix multiplication is O(mnp).","category":"page"},{"location":"chap3/linalg/#System-of-Linear-Equations-and-LU-Decomposition","page":"Matrix Computation","title":"System of Linear Equations and LU Decomposition","text":"","category":"section"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"Let Ain mathbbC^ntimes n be a invertible square matrix and b in mathbbC^n be a vector. Solving a linear equation means finding a vector xinmathbbC^n such that","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"A x = b","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"note: Example\nLet us consider the following system of linear equationsbeginalign*\n2 x_1 + 3 x_2 - 2 x_3 = 1 \n3 x_1 + 2 x_2 + 3 x_3 = 2 \n4 x_1 - 3 x_2 + 2 x_3 = 3\nendalign*The system of linear equations can be written in matrix form asbeginbmatrix\n2  3  -2 \n3  2  3 \n4  -3  2\nendbmatrix\nbeginbmatrix\nx_1 \nx_2 \nx_3\nendbmatrix\n=\nbeginbmatrix\n1 \n2 \n3\nendbmatrixIn Julia, we can solve the system of linear equations using the backslash operator \\ function.A = [2 3 -2; 3 2 3; 4 -3 2]\nb = [1, 2, 3]\nx = A \\ b\nA * xThe \\ method is implemented with the LU decomposition. It is equivalent to the following code.using LinearAlgebra\nlures = lu(A)  # pivot rows by default\nlures.L * lures.U ≈ lures.P * A\n\nUpperTriangular(lures.U) \\ (LowerTriangular(lures.L) \\ (lures.P * b))","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"The LU decomposition of a matrix Ain mathbbC^ntimes n is a factorization of the form","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"PA = LU","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"where P is a permutation matrix for pivoting the rows of A, L is a lower triangular matrix, and U is an upper triangular matrix. Pivoting rows are used to avoid division by zero to ensure numerical stability. In Julia, linear equations with UpperTriangular or LowerTriangular matrices will be solved with forward and backward substitution.","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"To summarize, the algorithm to solve a linear equation contains following steps:","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"Decompose the matrix PA in mathbbC^ntimes n into L in mathbbC^ntimes n and U in mathbbC^ntimes n matrices using a method such as Gaussian elimination or Crout's method.\nRewrite the equation Ax = b as LUx = Pb.\nSolve for y in Ly = b by Forward-substitution. This involves substituting the values of y into the equation one at a time, starting with the first row and working downwards.\nSolve for x in Ux = y by Back-substitution (link TBA). This involves substituting the values of x into the equation one at a time, starting with the last row and working upwards.","category":"page"},{"location":"chap3/linalg/#Least-Squares-Problem-and-QR-Decomposition","page":"Matrix Computation","title":"Least Squares Problem and QR Decomposition","text":"","category":"section"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"The least squares problem is to find a vector xinmathbbC^n that minimizes the residual","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"Ax - b_2","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"where Ain mathbbC^mtimes n and bin mathbbC^m. A solution to the least squares problem involves finding the QR decomposition of the matrix A. The QR decomposition of A is a factorization of the form","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"A = QR","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"where Qin mathbbC^mtimes m is an orthogonal matrix and Rin mathbbC^mtimes n is an upper triangular matrix. The QR decomposition is used to solve the linear least squares problem and to find the eigenvalues of a matrix.","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"In Julia, we can find the QR decomposition of a matrix using the qr function.","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"A = [1 2; 3 4; 5 6]\nqr(A)","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"note: Example: data fitting\nSuppose we have a set of data pointst_i 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5\ny_i 2.9 2.7 4.8 5.3 7.1 7.6 7.7 7.6 9.4 9.0<img src=\"../../assets/images/fitting-data.png\" alt=\"fitting data\" width=\"400\"/>We can fit a quadratic function of the form y = c_0 + c_1 t + c_2 t^2 to the data by solving the least squares problem. We can solve the least squares problem by finding the values of c_0, c_1, and c_2 that minimize the sum of the squared residualssum_i=1^n (y_i - (c_0 + c_1 t_i + c_2 t_i^2))^2In matrix form, the least squares problem can be written asmin_x Ax - b_2whereA = beginbmatrix\n1  t_1  t_1^2 \n1  t_2  t_2^2 \nvdots  vdots  vdots \n1  t_n  t_n^2\nendbmatrix\nx = beginbmatrix\nc_0 \nc_1 \nc_2\nendbmatrix\nb = beginbmatrix\ny_1 \ny_2 \nvdots \ny_n\nendbmatrixBy expanding the expression Ax - b_2, we can see that the solution to the least squares problem is given byx = (A^dagger A)^-1 A^dagger bwhen A^dagger A is invertible, where A^dagger is the Hermitian conjugate of A, which is the same as transpose given A is real. using LinearAlgebra\ntime = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5];\ny = [2.9, 2.7, 4.8, 5.3, 7.1, 7.6, 7.7, 7.6, 9.4, 9.0];\n\nA = hcat(ones(length(time)), time, time.^2)\nx = (A' * A) \\ (A' * y)The fitted quadratic function is as follows.<img src=\"../../assets/images/fitting-data2.png\" alt=\"fitting data\" width=\"400\"/>However, this approach is not recommended for large matrices due to the poor numerical stability. The condition number of A^dagger A is the square of the condition number of A, which can be very large. Instead, we can use the qr function to solve the least squares problem.Q, R = qr(A)\nx = R \\ (Matrix(Q)' * y)An alternative way is to use the pseudoinverse of A, which invokes the more costly SVD decomposition.LinearAlgebra.pinv(A) * (y)  # an alternative way","category":"page"},{"location":"chap3/linalg/#Eigenvalues-and-Eigenvectors","page":"Matrix Computation","title":"Eigenvalues and Eigenvectors","text":"","category":"section"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"The eigenvalues and eigenvectors of a matrix Ain mathbbC^ntimes n are the solutions to the equation","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"A x = lambda x","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"where lambda is a scalar and x is a non-zero vector. The eigenvalues of a matrix can be found by solving the characteristic equation","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"det(A - lambda I) = 0","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"where I is the identity matrix. The eigenvectors can be found by solving the equation (A - lambda I)x = 0.","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"In Julia, we can find the eigenvalues and eigenvectors of a matrix using the eigen function.","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"A = [1 2; 3 4]\neigen(A)","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"note: Example: eigenmodes of a vibrating string (or atomic chain)\nThis example is about solving the dynamics of a vibrating string. (Image: )Image source and main referenceThe dynamics of a one dimensional vibrating string can be described by the Newton's second lawM ddotu = C(u_i+1 - u_i) - C(u_i - u_i-1)where M is the mass matrix, C is the stiffness, and u_i is the displacement of the ith atom. The end atoms are fixed, so we have u_0 = u_n+1 = 0. We assume all atoms have the same eigenfrequency omega and the displacement of the ith atom is given byu_i(t) = A_i cos(omega t + phi_i)where phi_i is the phase of the ith atom. Then we transform the equation into the eigenvalue problembeginbmatrix\n-C  C  0  cdots  0 \nC  -2C  C  cdots  0 \n0  C  -2C  cdots  0 \nvdots  vdots  vdots  ddots  vdots \n0  0  0  cdots  -C\nendbmatrix\nbeginbmatrix\nA_1 \nA_2 \nA_3 \nvdots \nA_n\nendbmatrix\n= -omega^2M\nbeginbmatrix\nA_1 \nA_2 \nA_3 \nvdots \nA_n\nendbmatrixThe eigenvalues omega^2 are the eigenfrequencies of the vibrating string and the eigenvectors are the eigenmodes of the vibrating string.Let us consider a 5-atom vibrating string with M = C = 10. We can find the eigenvalues and eigenvectors of the mass matrix using the eigen function.M = C = 1.0\nC_matrix = [-C C 0 0 0; C -2C C 0 0; 0 C -2C C 0; 0 0 C -2C C; 0 0 0 C -C]\nevals, evecs = LinearAlgebra.eigen(C_matrix);\nsecond_omega = sqrt(-evals[2]/M)\nsecond_mode = evecs[:, 2]\nu(t) = second_mode .* cos.(-second_omega .* t) # (ϕi=0)\nu(1.0)  # atom locations offsets at t=1.0By comparing the eigenmodes with the simulation, we can see that the second mode matches the simulation.<img src=\"../../assets/images/springs-demo.gif\" alt=\"eigenmodes\" width=\"400\"/>For any given initial condition, the displacement of the atoms can be expressed as a linear combination of the eigenmodes. To find a more generic implementation, please check the source code.","category":"page"},{"location":"chap3/linalg/#Matrix-functions","page":"Matrix Computation","title":"Matrix functions","text":"","category":"section"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"Suppose we have a matrix A in mathbbC^ntimes n and an analytic function f defined with a power series","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"f(A) = sum_i=0^infty a_i A^i","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"To compute a matrix function, e.g. f(A) = e^A, we can use the following steps:","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"Diagonalize the matrix A as A = PDP^-1, where D is a diagonal matrix and P is a matrix whose columns are the eigenvectors of A.\nCompute the matrix function f(A) as f(A) = Pf(D)P^-1.\nCompute the matrix function f(D) by applying the function f to the diagonal elements of D.\nCompute the matrix function f(A) by multiplying the matrices P, f(D), and P^-1, i.e. f(A) = P f(D) P^-1.","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"note: Example\nLet us consider the matrixA = beginbmatrix\n1  2 \n3  4\nendbmatrixWe can compute the matrix function e^A using the exp function.A = [1 2; 3 4]\nexp(A)It is consistent with the result from the eigenvalue decomposition.D, P = LinearAlgebra.eigen(A)\nP * LinearAlgebra.Diagonal(exp.(D)) * inv(P)","category":"page"},{"location":"chap3/linalg/#Singular-Value-Decomposition","page":"Matrix Computation","title":"Singular Value Decomposition","text":"","category":"section"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"The singular value decomposition (SVD) of a matrix Ain mathbbC^mtimes n is a factorization of the form","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"A = U Sigma V^dagger","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"where Uin mathbbC^mtimes m and Vin mathbbC^ntimes n are orthogonal matrices and Sigmain mathbbC^mtimes n is a diagonal matrix with non-negative real numbers on the diagonal. The singular value decomposition is a generalization of the eigenvalue decomposition for non-square matrices.","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"In Julia, we can find the singular value decomposition of a matrix using the svd function.","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"A = [1 2; 3 4; 5 6]\nsvd(A)","category":"page"},{"location":"chap3/linalg/#Cholesky-Decomposition","page":"Matrix Computation","title":"Cholesky Decomposition","text":"","category":"section"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"The Cholesky decomposition of a positive definite matrix Ain mathbbC^ntimes n is a factorization of the form","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"A = LL^dagger","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"where Lin mathbbC^ntimes n is a lower triangular matrix. The Cholesky decomposition is used to solve the linear system of equations Ax = b when A is symmetric and positive definite.","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"In Julia, we can find the Cholesky decomposition of a matrix using the cholesky function.","category":"page"},{"location":"chap3/linalg/","page":"Matrix Computation","title":"Matrix Computation","text":"A = [2 1; 1 3]\ncholesky(A)","category":"page"},{"location":"chap3/chol/#Data-Fitting-and-Cholesky-Decomposition","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"","category":"section"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"Given m data points (t_i y_i), we wish to find the n-vector x of parameters that gives the \"best fit\" to the data by the model function f(t x), with","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"f mathbbR^n+1 rightarrow mathbbR","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"min_xsum_i=1^m (y_i - f(t_i x))^2","category":"page"},{"location":"chap3/chol/#Example","page":"Data Fitting and Cholesky Decomposition","title":"Example","text":"","category":"section"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"f(x) = x_0 + x_1 t + x_2 t^2","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"Ax = left(beginmatrix\n1  t_1  t_1^2\n1  t_2  t_2^2\n1  t_3  t_3^2\n1  t_4  t_4^2\n1  t_5  t_5^2\nvdots  vdots  vdots\nendmatrixright)\nleft(beginmatrix x_1  x_2  x_3endmatrixright) approx\nleft(beginmatrixy_1 y_2 y_3  y_4  y_5vdotsendmatrixright) = b","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"The goal is to minimize Ax - b_2^2, we obtain the normal equations","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"A^T Ax = A^T b","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"To solve the normal equations, we can use the pseudoinverse","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"beginalign*\nA^+ = (A^T A)^-1A^T\nx = A^+ b\nendalign*","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"where A^+ is the pseudoinverse of A.","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"The julia version","category":"page"},{"location":"chap3/chol/#The-geometric-interpretation","page":"Data Fitting and Cholesky Decomposition","title":"The geometric interpretation","text":"","category":"section"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"The residual is b-Ax","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"A^T(b - Ax) = 0","category":"page"},{"location":"chap3/chol/#Solving-Normal-Equations-with-Cholesky-decomposition","page":"Data Fitting and Cholesky Decomposition","title":"Solving Normal Equations with Cholesky decomposition","text":"","category":"section"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"Step 1: Rectangular → Square","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"A^TAx = A^T b","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"Step 2: Square → Triangular","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"A^T A = LL^T","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"Step 3: Solve the triangular linear equation \"\"\"","category":"page"},{"location":"chap3/chol/#Issue:-The-Condition-Squaring-Effect","page":"Data Fitting and Cholesky Decomposition","title":"Issue: The Condition-Squaring Effect","text":"","category":"section"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"The conditioning of a square linear system Ax = b depends only on the matrix, while the conditioning of a least squares problem Ax approx b depends on both A and b.","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"A = left(beginmatrix1  1 epsilon  0  0  epsilon endmatrixright)","category":"page"},{"location":"chap3/chol/#Cholesky-decomposition","page":"Data Fitting and Cholesky Decomposition","title":"Cholesky decomposition","text":"","category":"section"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"Cholesky decomposition is a method of decomposing a positive-definite matrix into a product of a lower triangular matrix and its transpose. It is named after the mathematician André-Louis Cholesky, who developed the method in the early 1900s. The Cholesky decomposition is useful in many areas of mathematics and science, including linear algebra, numerical analysis, and statistics. It is often used in solving systems of linear equations, computing the inverse of a matrix, and generating random numbers with a given covariance matrix. The Cholesky decomposition is computationally efficient and numerically stable, making it a popular choice in many applications.","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"Given a positive definite symmetric matrix Ain mathbbR^ntimes n, the Cholesky decomposition is formally defined as","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"A = LL^T","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"where L is an upper triangular matrix.","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"The implementation of Cholesky decomposition is similar to LU decomposition.","category":"page"},{"location":"chap3/chol/","page":"Data Fitting and Cholesky Decomposition","title":"Data Fitting and Cholesky Decomposition","text":"function chol!(a::AbstractMatrix)\n\tn = size(a, 1)\n\t@assert size(a, 2) == n\n\tfor k=1:n\n\t\ta[k, k] = sqrt(a[k, k])\n\t\tfor i=k+1:n\n\t\t\ta[i, k] = a[i, k] / a[k, k]\n\t\tend\n\t\tfor j=k+1:n\n\t\t\tfor i=k+1:n\n\t\t\t\ta[i,j] = a[i,j] - a[i, k] * a[j, k]\n\t\t\tend\n\t\tend\n\tend\n\treturn a\nend\n\n@testset \"cholesky\" begin\n\tn = 10\n\tQ, R = qr(randn(10, 10))\n\ta = Q * Diagonal(rand(10)) * Q'\n\tL = chol!(copy(a))\n\t@test tril(L) * tril(L)' ≈ a\n\t# cholesky(a) in Julia\nend","category":"page"},{"location":"chap1/git/#Maintainability-Version-Control","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"","category":"section"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"Maintaining a software project is not easy, especially when it comes to multiple developers working on the same piece of code. When adding a new feature to the project, maintainers may encounter the following problems:","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"Multiple developers modify the same file at the same time, works can not be merged easily.\nNew code breaks an existing feature, downstream users are affected.","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"The solution to the above problems is version-control. Among all version control software, git is the most popular one.","category":"page"},{"location":"chap1/git/#Install-git","page":"Maintainability - Version Control","title":"Install git","text":"","category":"section"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"In Ubuntu (or WSL), you can install git with the following command:","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"sudo apt-get install git","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"In MacOS, you can install git with the following command:","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"brew install git","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"Then you should configure your git with your name and email:","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"git config --global user.name \"Your Name\"\ngit config --global user.email \"xxx@example.com\"","category":"page"},{"location":"chap1/git/#Create-a-git-repository","page":"Maintainability - Version Control","title":"Create a git repository","text":"","category":"section"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"A git repository, also known as a repo, is a directory that managed by git. To get started, you start with a terminal and type","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"cd path/to/working/directory\ngit init\necho \"Hello, World\" > README.md\ngit add -A\ngit commit -m 'this is my initial commit'\ngit status","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"Line 1: changes the directory to the working directory, which can be either an existing directory or a new directory.\nLine 2: initializes the working directory as a git repository, which will create a .git directory that containing all data generated by git.\nLine 3: creates a file README.md with the content Hello, World. The file README.md is a markdown file, which is a lightweight markup language with plain-text-formatting syntax. You can learn more about markdown from the markdown tutorial. This line can be omitted if the working directory already contains files.\nLine 4: line add files to the staging area (area that caches changes that to be committed).\nLine 5: commits the changes to the repository, which will create a snapshot of your current work.\nLine 6: shows the status of the working directory, staging area, and repository. If the above commands are executed correctly, the output should be nothing to commit, working tree clean.","category":"page"},{"location":"chap1/git/#Track-the-changes-checkout,-diff,-log","page":"Maintainability - Version Control","title":"Track the changes - checkout, diff, log","text":"","category":"section"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"Git enables developers to track changes in their codebase. Continuing the previous example, we can analyze the repository with the following commands:","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"echo \"Bye Bye, World\" > README.md\ngit diff\ngit add -A\ngit commit -m 'a second commit'\ngit log\ngit checkout HEAD~1\ngit checkout main","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"Line 1: makes changes to the file README.md.\nLine 2: shows the changes made to the file README.md.\nLine 3-4: adds the changes to the staging area and commits the changes to the repository.\nLine 5: shows the history of commits. The output should be something like this:","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"commit 02cd535b6d78fca1713784c61eec86e67ce9010c (HEAD -> main)\nAuthor: GiggleLiu <cacate0129@gmail.com>\nDate:   Mon Feb 5 14:34:20 2024 +0800\n\n    a second commit\n\ncommit 570e390759617a7021b0e069a3fbe612841b3e50\nAuthor: GiggleLiu <cacate0129@gmail.com>\nDate:   Mon Feb 5 14:23:41 2024 +0800\n\n    this is my initial commit","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"Line 6: Checkout the previous snapshot. Note HEAD is your current snapshot and HEAD~n is the nth snapshot counting back from the current snapshot.\nLine 7: Return to the main branch, which points to the latest snapshot. We will discuss more about branch later in this tutorial.","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"You can use git reset to reset the current HEAD to the specified snapshot, which can be useful when you committed something bad by accident.","category":"page"},{"location":"chap1/git/#Upload-your-repository-to-the-cloud-remote","page":"Maintainability - Version Control","title":"Upload your repository to the cloud - remote","text":"","category":"section"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"A server to store git repository, or remote in git terminology, is required for the collaboration purpose. Remote repositories can be hosted on git hosting services such as GitHub and GitLab.","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"To start, you need to create an empty repository (no README files) on a git hosting service (Tutorial: How to create a new GitHub repo), a URL for cloning the repo will show up, which typically use the SSH or HTTPS protocol. To secure your repository, you must configure some security settings, such as SSH or two-factor authentication (2FA).","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"After creating the repository, you can upload your local repository to the remote repository with the following commands:","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"git remote add origin <url>\ngit remote -v\ngit push origin main","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"Line 1: add a remote repository, where origin is a tag for the added remote repository, and the <url> is the URL of the remote repository.\nLine 2: shows the URL of all remotes, including the origin remote we just added.\nLine 3: push commits to the main branch of the remote repository origin. When collaborating with others, this command could fail due to another commit pushed earlier from your collaborators. To resolve the issue, you should use git pull origin main to fetch the latest snapshot and merge two commits manually.","category":"page"},{"location":"chap1/git/#Develop-features-safely-branches","page":"Maintainability - Version Control","title":"Develop features safely - branches","text":"","category":"section"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"So far, we worked with a single branch main, which is not recommended. A branch in git is a lightweight pointer to a specific commit. Working on the main branch may cause the following reasons:","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"No usable code. Developers usually develop features based on the current main branch, so the main branch is expected to always usable. However, working on a single branch can easily break this rule.\nHard to resolve conflicts. When multiple developers modify the same file at the same time, works can not be synchronized easily due to the conflicts. Multiple branches can make the feature development process relatively independent of each other.\nHard to discard a feature. For some experimental features, you may want to discard it after testing. However, a commit on the main branch can not be easily reverted.","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"Understanding the branches is extremely useful when, multiple developers are working on the same project. In the following example, we will create a new branch me/feature and develop a feature there.","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"git checkout -b me/feature\necho \"Hello, World - Version 2\" > README.md\ngit add -A\ngit commit -m 'this is my feature'\ngit push origin me/feature","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"Line 1: create and switch to the new branch me/feature, which is a copy of the current branch, e.g. the main branch. The branch name me/feature follows the convention <username>/<feature>, which is useful when working with others.\nLine 2-5: makes some changes to the file README.md and commits the changes to the repository. Finally, the changes are pushed to the remote repository origin. The remote branch me/feature is created automatically.","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"While developing a feature, you or another developer may want to develop another feature based on the current main branch. You can create another branch other/feature and develop the feature there.","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"git checkout main\ngit checkout -b other/feature\necho \"Bye Bye, World - Version 2\" > feature.md\ngit add -A\ngit commit -m 'this is another feature'\ngit push origin other/feature","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"In the above example, we created a new branch other/feature based on the main branch, and made some changes to the file feature.md.","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"Finally, when the feature is ready, you can merge the feature branch to the main branch.","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"git checkout main\ngit merge me/feature\ngit push origin main","category":"page"},{"location":"chap1/git/#Working-with-others-issues-and-pull-requests","page":"Maintainability - Version Control","title":"Working with others - issues and pull requests","text":"","category":"section"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"When working with others, you may want to propose changes to a repository and discuss them with others. This is where issues and pull requests come in. Issues and pull requests are features of git hosting services like GitHub and GitLab.","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"Issue is relatively simple, it is a way to report a bug or request a feature.\nPull request (resource: how to create a pull request) is a way to propose changes to a repository and discuss them with others. It is also a way to merge code from source branch to target branch. The source branch can be a branch in the same repository or a branch in a forked repository - a copy of the repository in your account. Forking a repository is needed when you want to propose changes to a repository that you do not have write access to.","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"note: Should I make a pull requests or push directly to main branch?\nTo update the main branch, one should use pull requests as much as possible, even if you have write access to the repository. It is a good practice to discuss the changes with others before merging them to the main branch. A pull request also makes the changes more traceable, which is useful when you want to revert the changes.","category":"page"},{"location":"chap1/git/#Git-cheat-sheet","page":"Maintainability - Version Control","title":"Git cheat sheet","text":"","category":"section"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"It is not possible to cover all the feature of git. We will list a few useful commands and resources for git learning.","category":"page"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"# global config\ngit config  # Get and set repository or global options\n\n# initialize a repo\ngit init    # Create an empty Git repo or reinitialize an existing one\ngit clone   # Clone repository into new directory\n\n# info\ngit status  # Show the working tree status\ngit log     # Show commit logs\ngit diff    # Show changes between commits, commit and working tree, etc\n\n# work on a branch\ngit add     # Add file contents to the index\ngit rm      # Remove files from the working tree and from the index\ngit commit  # Record changes to the repository\ngit reset   # Reset current HEAD to the specified state\n\n# branch manipulation\ngit checkout # Switch branches or restore working tree files\ngit branch  # List, create, or delete branches\ngit merge   # Join two or more development histories together\n\n# remote synchronization\ngit remote  # Manage set of tracked repositories\ngit pull  # Fetch from and integrate with another repo or a local branch\ngit fetch   # Download objects and refs from another repository\ngit push    # Update remote refs along with associated objects","category":"page"},{"location":"chap1/git/#Resources","page":"Maintainability - Version Control","title":"Resources","text":"","category":"section"},{"location":"chap1/git/","page":"Maintainability - Version Control","title":"Maintainability - Version Control","text":"The Official GitHub Training Manual\nMIT online course missing semester.","category":"page"},{"location":"chap6/compressedsensing/#Compressed-sensing","page":"Compressed sensing","title":"Compressed sensing","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"using DataStructures using Plots using StatsBase using Interpolations using JuMP, SCS using LinearAlgebra","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"using Images","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"using NLSolversBase","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"using Optim","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"using FiniteDifferences","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"begin \tfunction ingredients(path::String) \t\t# this is from the Julia source code (evalfile in base/loading.jl) \t\t# but with the modification that it returns the module instead of the last object \t\tname = Symbol(basename(path)) \t\tm = Module(name) \t\tCore.eval(m, \t        Expr(:toplevel, \t             :(eval(x) = (Expr(core eval))(name, x)), \t             :(include(x) = (Expr(top include))(name, x)), \t             :(include(mapexpr::Function, x) = (Expr(top include))(mapexpr name x)) \t             (include(path)))) \t\tm \tend \tfunction highlight(str) \t    HTML(<span style=\"background-color:yellow\">str</span>) \tend end;","category":"page"},{"location":"chap6/compressedsensing/#Sparsity-Detection","page":"Compressed sensing","title":"Sparsity Detection","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Beyond sparse matrices and Principle Component Analysis (PCA)!\"","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"TableOfContents()","category":"page"},{"location":"chap6/compressedsensing/#Information","page":"Compressed sensing","title":"Information","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"A measure of highlight(\"randomness\"), usually measured by the entropy","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"S = -sum_k p_klog p_k","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Quiz: Which knowledge bellow removes more information?","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"When I toss a coin, its head side will be up,\nTomorrow will rain,\nToday's lecture will be a successful one.","category":"page"},{"location":"chap6/compressedsensing/#Huffman-coding","page":"Compressed sensing","title":"Huffman coding","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for highlight(\"lossless data compression\"). The process of finding or using such a code proceeds by means of Huffman coding.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Ref: https://www.programiz.com/dsa/huffman-coding\"","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Task: describe the following image in computer.\"","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Mondrian - Trafalgar Square, 1939-43 - a picture having little information from various perspective.\"","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"LocalResource(\"images/trafalgar-square.jpg\", :width=>300)","category":"page"},{"location":"chap6/compressedsensing/#The-naive-approach","page":"Compressed sensing","title":"The naive approach","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"R: 000\nY: 001\nB: 010\nK: 011\nW: 100","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"We need 3mn bits to store this image. Can we do better?\"","category":"page"},{"location":"chap6/compressedsensing/#Observation","page":"Compressed sensing","title":"Observation","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Calculate the frequency of each color in the image.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"R: 3%\nY: 7%\nB: 1%\nK: 10%\nW: 79%","category":"page"},{"location":"chap6/compressedsensing/#Formalized-description","page":"Compressed sensing","title":"Formalized description","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Input","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Alphabet A=(a_1a_2dots a_n), which is the symbol alphabet of size n. Tuple displaystyle W=(w_1w_2dots w_n), which is the tuple of the (positive) symbol weights (usually proportional to probabilities), i.e. displaystyle w_i=operatorname weight left(a_iright)iin 12dots n.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Output","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Code  displaystyle Cleft(Wright)=(c_1c_2dots c_n), which is the tuple of (binary) codewords, where c_i is the codeword for displaystyle a_iiin 12dots n.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Goal","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Let textstyle Lleft(Cleft(Wright)right)=sum _i=1^nw_ioperatorname length left(c_iright) be the weighted path length of code C. Condition: displaystyle Lleft(Cleft(Wright)right)leq Lleft(Tleft(Wright)right) for any code displaystyle Tleft(Wright).","category":"page"},{"location":"chap6/compressedsensing/#Algorithm","page":"Compressed sensing","title":"Algorithm","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"The simplest construction algorithm uses a priority queue where the node with lowest probability is given highest priority:","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Create a leaf node for each symbol and add it to the priority queue.\nWhile there is more than one node in the queue:\nRemove the two nodes of highest priority (lowest probability) from the queue\nCreate a new internal node with these two nodes as children and with probability equal to the sum of the two nodes' probabilities.\nAdd the new node to the queue.\nThe remaining node is the root node and the tree is complete.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Since efficient priority queue data structures require O(log n) time per insertion, and a tree with n leaves has 2n1 nodes, this algorithm operates in O(n log n) time, where n is the number of symbols.","category":"page"},{"location":"chap6/compressedsensing/#Implementation\"","page":"Compressed sensing","title":"Implementation\"","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Build a huffman tree","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"struct Node{VT, PT}     value::Union{VT,Nothing} \tprob::PT     left::Union{Node{VT,PT}, Nothing}     right::Union{Node{VT,PT}, Nothing} end","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"function huffman_tree(symbols, probs) \tisempty(symbols) && error(\"empty input!\") \t# priority queue can keep the items ordered with log(# of items) effort. \tnodes = PriorityQueue(Base.Order.Forward, \t\t[Node(c, f, nothing, nothing)=>f for (c, f) in zip(symbols, probs)])     while length(nodes) > 1         left = dequeue!(nodes)         right = dequeue!(nodes)         parent = Node(nothing, left.prob + right.prob, left, right)         enqueue!(nodes, parent=>left.prob + right.prob)     end \treturn dequeue!(nodes) end","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"ht = huffman_tree(\"RYBKW\", [0.03, 0.07, 0.01, 0.1, 0.79])","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"From the tree, we generate the binary code.\"","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"function decent!(tree::Node{VT}, prefix::String=\"\", d::Dict = Dict{VT,String}()) where VT \tif tree.left === nothing # leaft \t\td[tree.value] = prefix \telse   # non-leaf \t\tdecent!(tree.left, prefix\"0\", d) \t\tdecent!(tree.right, prefix\"1\", d) \tend \treturn d end","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"code_dict = decent!(ht)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"meancodelength = let \tcodelength = 0.0 \tfor (symbol, prob) in zip(\"RYBKW\", [0.03, 0.07, 0.01, 0.1, 0.79]) \t\tcodelength += length(codedict[symbol]) * prob \tend \tcodelength end","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"We only need 136mn bits to represent the Mondrian's Trafalgar Square!","category":"page"},{"location":"chap6/compressedsensing/#The-optimality\"","page":"Compressed sensing","title":"The optimality\"","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Lemma: Huffman Encoding produces an optimal tree.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"The compressed text has a minimum size of Sn, where","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"S = -sum_k p_klog p_k","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"It is reached when all non-leaf nodes in the tree are ballanced, i.e. having the same weight for left and right children.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"S_trafalgar = StatsBase.entropy([0.03, 0.07, 0.01, 0.1, 0.79], 2)","category":"page"},{"location":"chap6/compressedsensing/#Matrix-Product-State/Tensor-Train","page":"Compressed sensing","title":"Matrix Product State/Tensor Train","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Calculate the compression ratio.","category":"page"},{"location":"chap6/compressedsensing/#Compressed-Sensing","page":"Compressed sensing","title":"Compressed Sensing","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Reference: https://www.pyrunner.com/weblog/B/index.html","category":"page"},{"location":"chap6/compressedsensing/#Example-1:-Two-sinusoids\"","page":"Compressed sensing","title":"Example 1: Two sinusoids\"","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"import FFTW","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Let us define a function of adding two sinusoids.\"","category":"page"},{"location":"chap6/compressedsensing/#sum-of-two-sinusoids","page":"Compressed sensing","title":"sum of two sinusoids","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"n = 5000","category":"page"},{"location":"chap6/compressedsensing/#time-sequence","page":"Compressed sensing","title":"time sequence","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"t = LinRange(0, 1/8, n)","category":"page"},{"location":"chap6/compressedsensing/#the-function","page":"Compressed sensing","title":"the function","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"y = sin.(1394π .* t) + sin.(3266π .* t)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"plot(t, y; label=\"the original function\")","category":"page"},{"location":"chap6/compressedsensing/#the-function-in-the-spectrum-domain","page":"Compressed sensing","title":"the function in the spectrum domain","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"yt = FFTW.dct(y)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"plot(t, yt; label=\"the function in frequency domain\")","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Let us extract 10% samples from it.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"m = 500","category":"page"},{"location":"chap6/compressedsensing/#not-allowing-repeated-indices","page":"Compressed sensing","title":"not allowing repeated indices","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"samples = sort!(StatsBase.sample(1:n, m, replace=false))","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"t2 = t[samples]","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"y2 = y[samples]","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"let \tplt = plot(t, y; label=\"the samples\") \tscatter!(plt, t2, y2; label=\"the generated samples\", markersize=2) end","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"If we plot it directly, it looks not so good\"","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"interplinear = linearinterpolation(t2, y2; extrapolation_bc=Line())","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"plot(t, interp_linear.(t))","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Why? Because we haven't used a prior that it is sparse in the frequency domain.\"","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"plot(t, FFTW.dct(interp_linear.(t)))","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Instead, we rephrase the problem as the following convex optimization problem","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"beginalign\nminsum_i x_i\nst A x = b\nendalign","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"model = Model(SCS.Optimizer)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"A = FFTW.idct(Matrix{Float64}(I, n, n), 1)[samples, :]","category":"page"},{"location":"chap6/compressedsensing/#do-L1-optimization","page":"Compressed sensing","title":"do L1 optimization","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"@variable model x[1:n];","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"@variable(model, norm1);","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"@constraint model A * x .== y2;","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"We use l_1 norm because l_0 is very hard to optimize.\"","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"@constraint(model, [norm1; x] in MOI.NormOneCone(1 + length(x)));","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"@objective(model, Min, norm1);","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"@bind run_optimize CheckBox()","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"if run_optimize \toptimize!(model) \tplot([JuMP.value.(x), FFTW.idct(JuMP.value.(x))]; layout=(2, 1), xlim=(0, 1000), labels=[\"spectrum\", \"recovered\"]) end","category":"page"},{"location":"chap6/compressedsensing/#A*JuMP.value.(x)-y2","page":"Compressed sensing","title":"A*JuMP.value.(x) - y2","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"A * FFTW.dct(interp_linear.(t)) - y2","category":"page"},{"location":"chap6/compressedsensing/#norm(JuMP.value.(x),-1)","page":"Compressed sensing","title":"norm(JuMP.value.(x), 1)","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"norm(FFTW.dct(interp_linear.(t)), 1)","category":"page"},{"location":"chap6/compressedsensing/#Example-2:-Recovering-an-image\"","page":"Compressed sensing","title":"Example 2: Recovering an image\"","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"(Image: )","category":"page"},{"location":"chap6/compressedsensing/#Creating-a-Julia-Package","page":"Compressed sensing","title":"Creating a Julia Package","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Go to the folder for package development,","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"cd path/to/julia/dev/folder","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Type the following command","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"julia> using PkgTemplates\n\njulia> tpl = Template(; user=\"GiggleLiu\", plugins=[\n           GitHubActions(; extra_versions=[\"nightly\"]),\n           Git(),\n           Codecov()\n\t], dir=pwd())\n\njulia> tpl(\"CompressedSensingTutorial\")","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"note: Note\nPlease replace GiggleLiu with your own user name, the CompressedSensingTutorial with your own package name! Please check the document of PkgTemplates. Now you should see a new package in your current folder.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"develop your project with, e.g., VSCode.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"cd CompressedSensingTutorial\n\ncode .","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"In the VSCode, please use your project environment as your julia project environment, check here.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Please configure your project dependency by typing in the pkg> mode.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"(CompressedSensingTutorial) pkg> add FFTW FiniteDifferences Images Optim ...","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"https://github.com/timholy/Revise.jl\"","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"source_img = Gray.(Images.load(joinpath(@DIR, \"images/waterfall.jpeg\")))","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"img = Float64.(source_img);","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"size(img)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Gray.(FFTW.dct(img))","category":"page"},{"location":"chap6/compressedsensing/#We-have-to-use-the-Pluto-ingredients-for-loading-a-local-project","page":"Compressed sensing","title":"We have to use the Pluto ingredients for loading a local project","text":"","category":"section"},{"location":"chap6/compressedsensing/#Please-check-the-issue:-https://github.com/fonsp/Pluto.jl/issues/115#issuecomment-661722426","page":"Compressed sensing","title":"Please check the issue: https://github.com/fonsp/Pluto.jl/issues/115#issuecomment-661722426","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"mod = ingredients(\"../lib/CompressedSensingTutorial/src/CompressedSensingTutorial.jl\")","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"CT = mod.CompressedSensingTutorial","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Let us check the project!\"","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"pixels = CT.sampleimagepixels(img, 0.1)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"The objective function.\"","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Gray.(CT.zero_padded(pixels, pixels.values))","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"@bind docompressedsensing CheckBox()","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"@doc CT.sensing_image","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"newimg = if docompressedsensing \tCT.sensingimage(pixels; C=0.005, optimizer=:LBFGS, showtrace=false, linesearch=Optim.HagerZhang()) else \trand(size(img)...) end;","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Gray.(newimg)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"FFTW.dct(newimg)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"newimg[pixels.indices] .- pixels.values","category":"page"},{"location":"chap6/compressedsensing/#Related-research-works","page":"Compressed sensing","title":"Related research works","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Quantum State Tomography via Compressed Sensing. David Gross, Yi-Kai Liu, Steven T. Flammia, Stephen Becker, and Jens Eisert. Phys. Rev. Lett. 105, 150401 – Published 4 October 2010","category":"page"},{"location":"chap6/compressedsensing/#Kernel-PCA","page":"Compressed sensing","title":"Kernel PCA","text":"","category":"section"},{"location":"chap6/compressedsensing/#References:","page":"Compressed sensing","title":"References:","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Universal Kernels, Charles A. Micchelli, Yuesheng Xu, Haizhang Zhang; 2006.\nKernel Principal Component Analysis, Bernhard Scholkopf, Alexander Smola, Klaus Robert Muller, 1997\nUniversality, Characteristic Kernels and RKHS Embedding of Measures Sriperumbudur, B. K., Fukumizu, K. & Lanckriet, G. R. G. Journal of Machine Learning Research 12, 2389–2410 (2011).","category":"page"},{"location":"chap6/compressedsensing/#Kernel-Method","page":"Compressed sensing","title":"Kernel Method","text":"","category":"section"},{"location":"chap6/compressedsensing/#From-dot-product-to-distance","page":"Compressed sensing","title":"From dot product to distance","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Let x y in R^n be two vectors, their distance is defined by","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"rm dist(x y) = x - y^2 = x^T x + y^T y - 2y^T x","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"If we can defined an inner product between two vectors, we can defined a measure of distance.","category":"page"},{"location":"chap6/compressedsensing/#Kernel-functions","page":"Compressed sensing","title":"Kernel functions","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"By extending the dot product by an arbitrary highlight(\"symmetric positive definite\") kernel funciton.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"x^T y rightarrow kappa(x y)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"We have a new measure of distance as","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"rm dist_kappa(x y) = kappa(x x) + kappa(y y) - 2kappa(x y)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Given a kernel function, there is a mapping from the original vector space to reproducing kernel Hilbert space (RKHS) associated with it. The kernel function of two vectors can be defined as an inner product of their images in the RKHS.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"kappa(x y) = phi(x)^Tphi(y)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"It is important to note that kappa(cdot x) is nolonger a linear function under this definition of inner product.\"","category":"page"},{"location":"chap6/compressedsensing/#Example","page":"Compressed sensing","title":"Example","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Let x yin mathbbR^2, a polynomial kernel of order 2 can be defined as","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"kappa(x y) = (x^T y)^2","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Then we can express the mapping from a vector to RKHS as","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"phi(x) = left(beginmatrixx_1^2x_2^2x_1x_2x_2x_1endmatrixright)","category":"page"},{"location":"chap6/compressedsensing/#Universality-of-a-kernel","page":"Compressed sensing","title":"Universality of a kernel","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"A kernel kappa is universal if and only if the following equation is a universal function approximator.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"f = sum_j=1^n c_j kappa(cdot x_j)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"where c_j in mathbbR and x_j can be either a number of a vector.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"As noted in Micchelli et al. (2006), one can ask whether the function, f in the above euqation approximates any real-valued target function arbitrarily well as the number of summands increases without bound. This is an important question to consider because if the answer is affirmative, then the kernel-based learning algorithm can be consistent in the sense that for any target function, f^, the discrepancy between f (which is learned from the training data) and f^ goes to zero (in some appropriate sense) as the sample size goes to infinity.","category":"page"},{"location":"chap6/compressedsensing/#Kernel-Principle-Component-Analysis-(PCA)","page":"Compressed sensing","title":"Kernel Principle Component Analysis (PCA)","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"The linear PCA starts from computing a convariance matrix of the data x_k in mathbbR^N, for k=1ldots l. We assume the data is centralized, i.e. sum_k x_k=0. Then the covariance matrix is defined as","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"C = frac1lsum_k=1^l x_k x_k^T","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"The new coordinates in the eigenvector basis of C are called principle components.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"The kernel PCA is defined as the PCA in the RKHS, i.e. ","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"overlineC = frac1lsum_k=1^l phi(x_k)phi(x_k)^T","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"The eigenvalue problem to solve is","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"lambda V = overlineC V","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"where V=sum_k=1^l alpha_k phi(x_k)","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"By projecting this eigenvalue problem into this subspace, we obtain the following equivalent form","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"llambda alpha = K alpha","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"where K_ij = K(x_i x_j).","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"let \tvideo = html<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hmBTACBGWJs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>","category":"page"},{"location":"chap6/compressedsensing/#Homework","page":"Compressed sensing","title":"Homework","text":"","category":"section"},{"location":"chap6/compressedsensing/#1.-Autodiff","page":"Compressed sensing","title":"1. Autodiff","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Given Ain mathbbR^ntimes n and x bin mathbbR^n. Please derive the backward rule of mathcalL = Ax - b_2 either using the chain rules or the perturbative approach (from the last lecture).","category":"page"},{"location":"chap6/compressedsensing/#2.-Sparsity-detection","page":"Compressed sensing","title":"2. Sparsity detection","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Choose one.","category":"page"},{"location":"chap6/compressedsensing/#(a).-Text-compression","page":"Compressed sensing","title":"(a). Text compression","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Given a text to be compressed:","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Compressed sensing (also known as compressive sensing, compressive sampling, or sparse sampling) is a signal processing technique for efficiently acquiring and reconstructing a signal, by finding solutions to underdetermined linear systems. This is based on the principle that, through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the Nyquist–Shannon sampling theorem. There are two conditions under which recovery is possible. The first one is sparsity, which requires the signal to be sparse in some domain. The second one is incoherence, which is applied through the isometric property, which is sufficient for sparse signals.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Please","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Analyse the frequency of each char\nCreate an optimal Huffman coding for each char\nEncode the text and count the length of total coding (not including the deliminators).","category":"page"},{"location":"chap6/compressedsensing/#(b).-Compressed-Sensing","page":"Compressed sensing","title":"(b). Compressed Sensing","text":"","category":"section"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Go through the video clip Compressed Sensing: When It Works","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"video","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"Please summarize this video clip, and explain when does compressed sensing work and when not.","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"note: Note\nIf you are interested in knowing more about compressed sensing, please do not miss this youtube video paylist: https://youtube.com/playlist?list=PLMrJAkhIeNNRHP5UA-gIimsXLQyHXxRty","category":"page"},{"location":"chap6/compressedsensing/","page":"Compressed sensing","title":"Compressed sensing","text":"end","category":"page"},{"location":"chap1/ci/#Correctness-Unit-Tests","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"","category":"section"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"In terms of scientific computing, accuracy of your result is most certainly more important than anything else. Checking the correctness is definitely one of the most challenging tasks in software development. Consider the following scenario:","category":"page"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"question: The problem of code review\nSuppose you are one of the maintainers of the Julia programming language. One day, a GitHub user Oscar Smith submitted a 6k-line PR to the JuliaLang/julia repository: (Image: )You want to check if this huge PR did something expected, requiring the following conditions to be satisfied:The build is successfully on Linux, macOS and Windows.\nNo existing feature breaks.\nThe added feature does something expected.What would you do?Checking to the 128 changed files line-by-line with human eye.\nHire a part-time worker, try installing the PR on three fresh machines, and try using as many features as possible and see if anything breaks.\nSomething more efficient.","category":"page"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"In the above scenario, the first option is not reliable for a software project expected to be used by millions of users. The second option is too expensive and time-consuming. Clever software engineers have come up with a more efficient way to check the correctness of the code, which is to use Unit Tests and CI/CD.","category":"page"},{"location":"chap1/ci/#Unit-Test","page":"Correctness - Unit Tests","title":"Unit Test","text":"","category":"section"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"Unit Tests is a software a testing method for the smallest testable unit of an application, e.g. functions. Unit tests are composed of a series of individual test cases, each of which is composed of:","category":"page"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"a collection of inputs and expected outputs for a function.\nan assertion statement to verify the function returns the expected output for a given input.","category":"page"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"To verify the correctness of the code, we run the unit tests. If the tests pass and the coverage is high, we can be confident that the code is working as expected.","category":"page"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"Tests pass: all assertions in the test cases are true.\nTest coverage: the percentage of the code that is covered by tests, i.e. the higher the coverage, the more robust the code is.","category":"page"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"In Julia, Test is a built-in package for writing and running unit tests. We will learn how to write and run unit tests in the section My First Package.","category":"page"},{"location":"chap1/ci/#Automate-your-workflow-CI/CD","page":"Correctness - Unit Tests","title":"Automate your workflow - CI/CD","text":"","category":"section"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"You still need to set up three clean machines to run the tests. What if you do not have three machines? The key to solving this problem is to automate the workflow on the cloud with the containerization technology, e.g. Docker. You do not need to configure the dockers on the cloud manually. Instead, you can use a Continuous Integration/Continuous Deployment (CI/CD) service to automate the workflow of","category":"page"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"(CI) build, test and merge the code changes whenever a developer commits code to the repository.\n(CD) deploy the code or documentation to a cloud service and register the package to the package registry.","category":"page"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"CI/CD are often integrated with git hosting services, e.g. Github Actions. A typical CI/CD pipeline include the following steps:","category":"page"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"code updated detected,\nfor each task, initialize a virtual machine on the cloud,\nthe virtual machine initializes the environment and runs the tests,\nthe virtual machine reports the test results.","category":"page"},{"location":"chap1/ci/","page":"Correctness - Unit Tests","title":"Correctness - Unit Tests","text":"The tasks of CI/CD are often defined in a configuration file, e.g. .github/workflows/ci.yml. We will learn how to set up a CI/CD pipeline in the section My First Package.","category":"page"},{"location":"chap2/julia-release/#My-First-Package","page":"My First Package","title":"My First Package","text":"","category":"section"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"One of the most important features of Julia is its package manager. It allows one to create, manage, and publish his own packages. In this section, we will learn how to create a package and publish it to the Julia registry.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Julia package manager can install the correct version of a package and its dependencies because it knows the exact versions of all the packages that are compatible with each other. This information was stored in the General registry - a central GitHub repository of metadata about all registered Julia packages.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Everyone can register a package in the General registry. To do so, you need to:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Create a package.\nSpecify the dependency of your package in the Project.toml file, like which version of a package your package depends on.\nDevelop the package by writing the source code, tests, and documentation.\nOpen-source the package by pushing the package to a public repository on GitHub. GitHub Actions can be used to automate the process of testing, building the documentation, and tagging a release so that other developers can contribute to the package easily.\nRegister the package in the General registry by creating a pull request to the General registry.","category":"page"},{"location":"chap2/julia-release/#Create-a-package","page":"My First Package","title":"Create a package","text":"","category":"section"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"We use PkgTemplate. Open a Julia REPL and type the following commands to initialize a new package named MyFirstPackage:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"julia> using PkgTemplates\n\njulia> tpl = Template(;\n    user=\"GiggleLiu\",  # replace!\n    authors=\"GiggleLiu\",  # replace!\n    julia=v\"1.10\",\n    plugins=[\n        License(; name=\"MIT\"),\n        Git(; ssh=true),\n        GitHubActions(; x86=true),\n        Codecov(),\n        Documenter{GitHubActions}(),\n    ],\n)\n\njulia> tpl(\"MyFirstPackage\")","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"where the username \"GiggleLiu\" should be replaced with your GitHub username. Many plugins are used in the above example:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"License: to choose a license for the package. Here we use the MIT license, which is a permissive free software license. Popular licenses include:\nMIT: a permissive free software license, featured with a short and simple permissive license with conditions only requiring preservation of copyright and license notices.\nApache2: a permissive free software license, featured with a contributor license agreement and a patent grant.\nGPL: a copyleft free software license, featured with a strong copyleft license that requires derived works to be available under the same license.\nGit: to initialize a Git repository for the package. Here we use the SSH protocol for Git for convenience. Using two-factor authentication (2FA) can make your GitHub account more secure.\nGitHubActions: to enable continuous integration (CI) with GitHub Actions.\nCodecov: to enable code coverage tracking with Codecov. It is a tool that helps you to measure the test coverage of your code. A package with high test coverage is more reliable.\nDocumenter: to enable documentation building and deployment with Documenter.jl and GitHub pages.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"After running the above commands, a new directory named MyFirstPackage will be created in the folder ~/.julia/dev/ - the default location for Julia packages.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"note: What makes a good package name?\nFor a package that is intended to be registered in the General registry, it is recommended to use a name that follows the Julia package naming guidelines. Although the same registry may not have two packages with the same name, a package use the UUID rather than the name as its unique identifier, because name may not be unique when multiple registries are used together.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"The file structure of the package is as follows:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"tree .   \n.\n├── .git\n│   ...\n├── .github\n│   ├── dependabot.yml\n│   └── workflows\n│       ├── CI.yml\n│       ├── CompatHelper.yml\n│       └── TagBot.yml\n├── .gitignore\n├── LICENSE\n├── Manifest.toml\n├── Project.toml\n├── README.md\n├── docs\n│   ├── Manifest.toml\n│   ├── Project.toml\n│   ├── make.jl\n│   └── src\n│       └── index.md\n├── src\n│   └── MyFirstPackage.jl\n└── test\n    └── runtests.jl","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":".git and .gitignore: the files that are used by Git. The .gitingore file contains the files that should be ignored by Git. By default, the .gitignore file contains the following lines:\n*.jl.*.cov\n*.jl.cov\n*.jl.mem\n/Manifest.toml\n/docs/Manifest.toml\n/docs/build/\n.github: the folder that contains the GitHub Actions configuration files.\nLICENSE: the file that contains the license of the package. The MIT license is used in this package.\nREADME.md: the manual that shows up in the GitHub repository of the package, which contains the description of the package.\nProject.toml: the file that contains the metadata of the package, including the name, UUID, version, dependencies and compatibility of the package.\nManifest.toml: the file that contains the exact versions of all the packages that are compatible with each other. It is usually automatically resolved from the Project.toml file, and it is not recommended pushing it to the remote repository.\ndocs: the folder that contains the documentation of the package. It has its own Project.toml and Manifest.toml files, which are used to manage the documentation environment. The make.jl file is used to build the documentation and the src folder contains the source code of the documentation.\nsrc: the folder that contains the source code of the package.\ntest: the folder that contains the test code of the package, which contains the main test file runtests.jl.","category":"page"},{"location":"chap2/julia-release/#Specify-the-dependency","page":"My First Package","title":"Specify the dependency","text":"","category":"section"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"The file that contains the metadata of the package, including the name, UUID, version, dependencies and compatibility of the package. To add a new dependency, you can use the following command in the package path:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"$ cd ~/.julia/dev/MyFirstPackage\n\n$ julia --project","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"This will open a Julia REPL in the package environment. To check the package environment, you can type the following commands in the package mode (press ]) of the REPL:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"(MyFirstPackage) pkg> st\nProject MyFirstPackage v1.0.0-DEV\nStatus `~/.julia/dev/MyFirstPackage/Project.toml` (empty project)","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"After that, you can add a new dependency by typing:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"(MyFirstPackage) pkg> add LinearAlgebra\n\n(MyFirstPackage) pkg> st\nProject MyFirstPackage v1.0.0-DEV\nStatus `~/jcode/ScientificComputingForPhysicists/lib/MyFirstPackage/Project.toml`\n  [37e2e46d] LinearAlgebra","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"The dependency is added correctly if no error is thrown. Press backspace to exit the package mode.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Type ; to enter the shell mode and then type","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"shell> cat Project.toml\nname = \"MyFirstPackage\"\nuuid = \"594718ca-da39-4ff3-a299-6d8961b2aa49\"\nauthors = [\"GiggleLiu\"]\nversion = \"1.0.0-DEV\"\n\n[deps]\nLinearAlgebra = \"37e2e46d-f89d-539d-b4ee-838fcccc9c8e\"\n\n[compat]\njulia = \"1.10\"\n\n[extras]\nTest = \"8dfed614-e22c-5e08-85e1-65c5234f0b40\"\n\n[targets]\ntest = [\"Test\"]","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"You will see that the dependency LinearAlgebra is added to the [deps] section of the Project.toml file.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"We also need to specify which version of LinearAlgebra is compatible with the current package. To do so, you need to edit the [compat] section of the Project.toml file with your favorite editor.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"[compat]\njulia = \"1.10\"\nLinearAlgebra = \"1\"  # added line","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Here, we have used the most widely used dependency version specifier =, which means matching the first nonzero component of the version number. For example:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"1 matches 1.0.0, 1.1.0, 1.1.1, but not 2.0.0.\n0.8 matches 0.8.0, 0.8.1, 0.8.2, but not 0.9.0 or 0.7.0.\n1.2 matches 1.2.0, 1.3.1, but not 1.1.0 or 2.0.0.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"The validity of specifying compatibility is based on the consensus among the developers:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"whenever an exported function is changed in a package, the first nonzero component of the version number should be increased.\nversion number starts with 0 is considered as a development version, and it is not stable.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Please check the Julia documentation about package compatibility for advanced usage.","category":"page"},{"location":"chap2/julia-release/#Develop-the-package","page":"My First Package","title":"Develop the package","text":"","category":"section"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Developers develop packages in the package environment. The package development process includes:","category":"page"},{"location":"chap2/julia-release/#1.-Edit-the-source-code","page":"My First Package","title":"1. Edit the source code","text":"","category":"section"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"The source code of the package is located in the src folder of the package path.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Let us add a simple function to the package. The source code of the package is as follows:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"File: src/MyFirstPackage.jl","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"module MyFirstPackage\n# import packages\nusing LinearAlgebra\n\n# export interfaces\nexport Lorenz, integrate_step\nexport Point, Point2D, Point3D\nexport RungeKutta, Euclidean\n\n# `include` other source files into this module\ninclude(\"lorenz.jl\")\n\nend","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"The include(\"lorenz.jl\") line includes the source code of the Lorenz system, which can be defined in the lorenz.jl file in the same folder. The source code of is as follows:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"File: src/lorenz.jl","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"\"\"\"\n    Point{D, T}\n\nA point in D-dimensional space, with coordinates of type T.\n\n# Examples\n```jldoctest\njulia> p1 = Point(1.0, 2.0)\nPoint{2, Float64}((1.0, 2.0))\n\njulia> p2 = Point(3.0, 4.0)\nPoint{2, Float64}((3.0, 4.0))\n\njulia> p1 + p2\nPoint{2, Float64}((4.0, 6.0))\n```\n\"\"\"\n# define a point in D-dimensional space\nstruct Point{D, T <: Real}\n    data::NTuple{D, T}  # a tuple of D elements of type T\nend\nPoint(x::Real...) = Point((x...,))  # `...` is the splat operator\n# define 2D and 3D points\nconst Point2D{T} = Point{2, T}\nconst Point3D{T} = Point{3, T}\n\n# define the dot product of two coordinate vectors\n# `mapreduce` is a high-order function that applies a function to each element \n# of an iterable and then reduces the result to a single value.\nLinearAlgebra.dot(x::Point, y::Point) = mapreduce(*, +, x.data, y.data)\n# implement the operations of the point\n# `Base` is the standard library of Julia\n# `Base.isapprox` is used to define a new method for the function `isapprox` in the `Base` module\n# for arithmetic operations like `*`, `*`, `+`, an extra `:` is required to avoid ambiguity\nBase.:*(x::Real, y::Point) = Point(x .* y.data) # `.` is the broadcast operator\nBase.:/(y::Point, x::Real) = Point(y.data ./ x)\nBase.:+(x::Point, y::Point) = Point(x.data .+ y.data)\nBase.isapprox(x::Point, y::Point; kwargs...) = all(isapprox.(x.data, y.data; kwargs...))\n# `all` returns true if all elements of the iterable are true\n\n# define the index and broadcastable functions\nBase.getindex(p::Point, i::Int) = p.data[i] # for `p[i]` like operations\nBase.broadcastable(p::Point) = p.data # for `x .+ y` like operations\nBase.iterate(p::Point, args...) = iterate(p.data, args...) # for `[p...]` like operations\n\n# the Lorenz system\nstruct Lorenz\n    σ::Float64\n    ρ::Float64\n    β::Float64\nend\n\n# the differential equation of the Lorenz system\nfunction field(p::Lorenz, u)\n    x, y, z = u\n    Point(p.σ*(y-x), x*(p.ρ-z)-y, x*y-p.β*z)\nend\n\n# abstract type for integrators, which allows us to switch between different integration methods\nabstract type AbstractIntegrator end\n# Runge-Kutta 4th order method\n# https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods\nstruct RungeKutta{K} <: AbstractIntegrator end\n# Euclidean method\nstruct Euclidean <: AbstractIntegrator end\n\nfunction integrate_step(f, ::RungeKutta{4}, t, y, Δt)\n    k1 = Δt * f(t, y)\n    k2 = Δt * f(t+Δt/2, y + k1 / 2)\n    k3 = Δt * f(t+Δt/2, y + k2 / 2)\n    k4 = Δt * f(t+Δt, y + k3)\n    return y + k1/6 + k2/3 + k3/3 + k4/6\nend\n\n# Euclidean integration\nfunction integrate_step(f, ::Euclidean, t, y, Δt)\n    return y + Δt * f(t, y)\nend\n\nfunction integrate_step(lz::Lorenz, int::AbstractIntegrator, u, Δt)\n    return integrate_step((t, u) -> field(lz, u), int, zero(Δt), u, Δt)\nend","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"To use this function, you can type the following commands in the package environment:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"julia> using MyFirstPackage\n\njulia> Point(2.0, 3.0)\nPoint2D{Float64}((2.0, 3.0))","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"note: How to enter package environment?\nIn shell: type\n$ cd ~/.julia/dev/MyFirstPackage\n$ julia --project\nIn REPL: press ] to enter the package mode and then type\npkg> activate path/to/package\nto enter the package environment. To deactivate the package environment, type\npkg> activate\nIn VSCode: Click the Julia env: ... button in the bottom of the window and then select the package path.","category":"page"},{"location":"chap2/julia-release/#2.-Write-tests","page":"My First Package","title":"2. Write tests","text":"","category":"section"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"We always need to write tests for the package. The test code of the package is located in the test folder of the package path.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"File: test/runtests.jl","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"using Test\nusing MyFirstPackage\n\n@testset \"lorenz\" begin\n    include(\"lorenz.jl\")\nend","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"File: test/lorenz.jl","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"using Test, MyFirstPackage\n\n@testset \"Point\" begin\n    p1 = Point(1.0, 2.0)\n    p2 = Point(3.0, 4.0)\n    @test p1 + p2 ≈ Point(4.0, 6.0)\nend\n\n@testset \"step\" begin\n    lz = Lorenz(10.0, 28.0, 8/3)\n    int = RungeKutta{4}()\n    r1 = integrate_step(lz, int, Point(1.0, 1.0, 1.0), 0.0001)\n    eu = Euclidean()\n    r2 = integrate_step(lz, eu, Point(1.0, 1.0, 1.0), 0.0001)\n    @test isapprox(r1, r2; rtol=1e-5)\nend","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"To run the tests, you can use the following command in the package environment:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"(MyFirstPackage) pkg> test\n  ... \n  [8e850b90] libblastrampoline_jll v5.8.0+1\nPrecompiling project...\n  1 dependency successfully precompiled in 1 seconds. 2 already precompiled.\n     Testing Running tests...\nTest Summary: | Pass  Total  Time\nlorenz        |    2      2  0.1s\nTest Summary: | Pass  Total  Time\nfluid         |    1      1  0.1s\n     Testing MyFirstPackage tests passed","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Cheers! All tests passed.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"You might want to write some examples to visualize the results of the package with Makie. We create an examples folder in the package path and then write the following example code","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"File: examples/lorenz.jl","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"using CairoMakie, MyFirstPackage\nset_theme!(theme_black())\n\n# create a Lorenz system\nlz = Lorenz(10, 28, 8/3)\n\n# initial condition\ny = MyFirstPackage.Point(1.0, 1.0, 1.0)\n \n# `Observable` defines the signal that can be used to update plots efficiently\npoints = Observable(Point3f[])\ncolors = Observable(Int[])\n\n# create a figure\nfig, ax, l = lines(points, color = colors,\n    colormap = :inferno, transparency = true, \n    axis = (; type = Axis3, protrusions = (0, 0, 0, 0), \n              viewmode = :fit, limits = (-30, 30, -30, 30, 0, 50)))\n\nrecord(fig, \"lorenz.mp4\", 1:120) do frame\n    global y\n    for i in 1:50\n        # update arrays inplace\n        y = integrate_step(lz, RungeKutta{4}(), y, 0.01)\n        push!(points[], Point3f(y...))\n        push!(colors[], frame)\n    end\n    ax.azimuth[] = 1.7pi + 0.3 * sin(2pi * frame / 120) # set the view angle of the axis\n    notify(points); notify(colors) # tell points and colors that their value has been updated\n    l.colorrange = (0, frame) # update plot attribute directly\nend","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"It output a video file lorenz.mp4 that visualizes the Lorenz system: (Image: )","category":"page"},{"location":"chap2/julia-release/#3.-Write-documentation","page":"My First Package","title":"3. Write documentation","text":"","category":"section"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"The documentation is built with Documenter.jl. The build script is docs/make.jl. To build the documentation, you can first enter the package environment and then type the following commands in a terminal:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"$ cd docs\n$ julia --project","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Instantiate or update the documentation environment if necessary. ","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"(docs) pkg> dev ..\n\n(docs) pkg> instantiate # or `up`","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"where dev .. is used to add the MyFirstPackage under development to the documentation environment.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Then build the documentation by typing","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"julia> include(\"make.jl\")","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"where include is a built-in function in Julia that includes the source code of the documentation build file make.jl into the current environment.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"The generated HTML files are located in the docs/build folder of the package path. To preview the documentation, just open the index.html file in a web browser.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"note: Live preview of documentation\nFor seamless previewing of documentation on updates, it is highly recommended using the LiveServer.jl package.","category":"page"},{"location":"chap2/julia-release/#Open-source-the-package","page":"My First Package","title":"Open-source the package","text":"","category":"section"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"To open-source the package, you need to push the package to a public repository on GitHub.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"First create a GitHub repository with the same name as the package. In this example, the repository name should be MyFirstPackage.jl. To check the remote repository of the package, you can use the following command in the package path:\n$ git remote -v\norigin\tgit@github.com:GiggleLiu/MyFirstPackage.jl.git (fetch)\norigin\tgit@github.com:GiggleLiu/MyFirstPackage.jl.git (push)\nThen push the package to the remote repository:\n$ git add -A\n$ git commit -m \"Initial commit\"\n$ git push\nAfter that, you need to check if all your GitHub Actions are passing. You can check the status of the GitHub Actions from the badge in the README.md file of the package repository. The configuration of GitHub Actions is located in the .github/workflows folder of the package path. Its file structure is as follows:\n.github\n├── dependabot.yml\n└── workflows\n    ├── CI.yml\n    ├── CompatHelper.yml\n    └── TagBot.yml\nThe CI.yml file contains the configuration for the CI of the package, which is used to automate the process of\nTesting the package after a pull request is opened, or the main branch is updated. This process can be automated with the julia-runtest action.\nBuilding the documentation after the main branch is updated. Please check the Documenter documentation for more information.\nThe TagBot.yml file contains the configuration for the TagBot, which is used to automate the process of tagging a release after a pull request is merged.\nThe CompatHelper.yml file contains the configuration for the CompatHelper, which is used to automate the process of updating the [compat] section of the Project.toml file after a pull request is merged.\nConfiguring GitHub Actions is a bit complicated. For beginners, it is a good practise to mimic the configuration of another package, e.g. OMEinsum.jl.","category":"page"},{"location":"chap2/julia-release/#Register-the-package","page":"My First Package","title":"Register the package","text":"","category":"section"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Once your package is ready to be published, you need to register the package in the General registry. The General registry is a central GitHub repository of metadata about all registered Julia packages. To register the package, you need to create a pull request to the General registry and wait for the pull request to be reviewed and merged. This process can be automated by the Julia registrator. If the pull request meets all guidelines, your pull request will be merged after a few days. Then, your package is available to the public. ","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"A good practice is to tag a release after the pull request is merged so that your package version update can be reflected in your GitHub repository. This process can be automated by the TagBot.","category":"page"},{"location":"chap2/julia-release/#Case-study:-The-file-structure-of-[OMEinsum.jl](https://github.com/under-Peter/OMEinsum.jl)","page":"My First Package","title":"Case study: The file structure of OMEinsum.jl","text":"","category":"section"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"(Image: )","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"OMEinsum.jl is a package for tensor contraction. The badges in the README.md file of the package repository are the following:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"build/passing: the tests executed by GitHub Actions are passing.\ncodecov/89%: the code coverage is 89%, meaning that 89% of the code is covered by tests.\ndocs/dev: the documentation is built and deployed with GitHub pages.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Now, let's take a look at the file structure of the package by running the following command in the package path (~/.julia/dev/OMEinsum):","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"$ tree . -L 1 -a\n.\n├── .git\n├── .github\n├── .gitignore\n├── LICENSE\n├── Project.toml\n├── README.md\n├── benchmark\n├── docs\n├── examples\n├── ext\n├── ome-logo.png\n├── src\n└── test","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"File: Project.toml","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"name = \"OMEinsum\"\nuuid = \"ebe7aa44-baf0-506c-a96f-8464559b3922\"\nauthors = [\"Andreas Peter <andreas.peter.ch@gmail.com>\"]\nversion = \"0.8.1\"\n\n[deps]\nAbstractTrees = \"1520ce14-60c1-5f80-bbc7-55ef81b5835c\"\nBatchedRoutines = \"a9ab73d0-e05c-5df1-8fde-d6a4645b8d8e\"\nChainRulesCore = \"d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4\"\nCombinatorics = \"861a8166-3701-5b0c-9a16-15d98fcdc6aa\"\nLinearAlgebra = \"37e2e46d-f89d-539d-b4ee-838fcccc9c8e\"\nMacroTools = \"1914dd2f-81c6-5fcd-8719-6d5c9610ff09\"\nOMEinsumContractionOrders = \"6f22d1fd-8eed-4bb7-9776-e7d684900715\"\nTupleTools = \"9d95972d-f1c8-5527-a6e0-b4b365fa01f6\"\n\n[weakdeps]\nCUDA = \"052768ef-5323-5732-b1bb-66c8b64840ba\"\n\n[extensions]\nCUDAExt = \"CUDA\"\n\n[compat]\nAbstractTrees = \"0.3, 0.4\"\nBatchedRoutines = \"0.2\"\nCUDA = \"4, 5\"\nChainRulesCore = \"1\"\nCombinatorics = \"1.0\"\nMacroTools = \"0.5\"\nOMEinsumContractionOrders = \"0.8\"\nTupleTools = \"1.2, 1.3\"\njulia = \"1\"\n\n[extras]\nDocumenter = \"e30172f5-a6a5-5a46-863b-614d45cd2de4\"\nDoubleFloats = \"497a8b3b-efae-58df-a0af-a86822472b78\"\nForwardDiff = \"f6369f11-7733-5829-9624-2563aa707210\"\nLinearAlgebra = \"37e2e46d-f89d-539d-b4ee-838fcccc9c8e\"\nPolynomials = \"f27b6e38-b328-58d1-80ce-0feddd5e7a45\"\nProgressMeter = \"92933f4c-e287-5a05-a399-4b506db050ca\"\nRandom = \"9a3f8284-a2c9-5f02-9a11-845980a1fd5c\"\nSymEngine = \"123dc426-2d89-5057-bbad-38513e3affd8\"\nTest = \"8dfed614-e22c-5e08-85e1-65c5234f0b40\"\nTropicalNumbers = \"b3a74e9c-7526-4576-a4eb-79c0d4c32334\"\nZygote = \"e88e6eb3-aa80-5325-afca-941959d7151f\"\n\n[targets]\ntest = [\"Test\", \"Documenter\", \"LinearAlgebra\", \"ProgressMeter\", \"SymEngine\", \"Random\", \"Zygote\", \"DoubleFloats\", \"TropicalNumbers\", \"ForwardDiff\", \"Polynomials\", \"CUDA\"]","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"It contains the following more sections:","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"[weakdeps] and [extensions]: the sections that specify the extensions of the package, which is related with the files in the ext folder. A package \"extension\" is a module that is automatically loaded when a specified set of other packages (its \"extension dependencies\") are loaded in the current Julia session. As a using case, consider you want to add the CUDA support to your package, but you don't want to force all users to install CUDA package if they don't need it, then adding CUDA as a weak dependency and move this feature ext folder is a good choice. Please check the Julia documentation about package extensions for more information.\n[extras] and [targets]: the section that specifies the extra dependencies of the package that used to test the package. One can also specify the extra dependencies for the test environment in the test folder of the package path.","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Quiz: ","category":"page"},{"location":"chap2/julia-release/","page":"My First Package","title":"My First Package","text":"Is ChainRulesCore at version 1.2 compatible with OMEinsum?\nIf ChainRulesCore at version 2.0 is released, what should be done to make OMEinsum compatible with the new version of ChainRulesCore? Which GitHub Action is used to automate this process?\nIf an author of OMEinsum fixed a bug, what should be done to make the new version of OMEinsum available to the public?\nIf an author of OMEinsum changed an exported function, what should be done to make the new version of OMEinsum available to the public?","category":"page"},{"location":"chap1/opensource_scientist/#Becoming-an-Open-Source-Developer","page":"Becoming an Open-Source Developer","title":"Becoming an Open-Source Developer","text":"","category":"section"},{"location":"chap1/opensource_scientist/","page":"Becoming an Open-Source Developer","title":"Becoming an Open-Source Developer","text":"This section focuses on understanding the open source workflow, which is the foundation of scientific computing. Along the way, we will introduce to you our recommended tools for accomplishing each task. ","category":"page"},{"location":"chap2/julia-setup/#Setup-Julia","page":"Setup Julia","title":"Setup Julia","text":"","category":"section"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"Julia is a high-level, high-performance, dynamic programming language. From the designing stage, Julia is intended to address the needs of high-performance numerical analysis and computational science, without the typical need of separate compilation to be fast, while also being effective for general-purpose programming, web use or as a specification language. Julia is also a free and open-source language, with a large community and a rich ecosystem.","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"We will devlve deeper into Julia later in the chapter. For now, we will just install Julia and setup the environment.","category":"page"},{"location":"chap2/julia-setup/#Step-1:-Installing-Julia","page":"Setup Julia","title":"Step 1: Installing Julia","text":"","category":"section"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"For Linux/Mac users, please open a terminal and type the following command to install Julia with juliaup. Juliaup is a tool to manage Julia versions and installations. It allows you to install multiple versions of Julia and switch between them easily.","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"curl -fsSL https://install.julialang.org | sh # Linux and macOS","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"For Windows users, please open execute the following command in a cmd,","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"winget install julia -s msstore # Windows","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"You can also install Juliaup directly from Windows Store.","category":"page"},{"location":"chap2/julia-setup/#For-users-suffering-from-the-slow-download-speed","page":"Setup Julia","title":"For users suffering from the slow download speed","text":"","category":"section"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"Network connectivity can be an issue for some users, especially for those who are in China. You may need to specify another server for installing Juliaup and Julia packages. To do so, execute the following command in your terminal before running the script above.","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"Linux and macOS","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"export JULIAUP_SERVER=https://mirror.nju.edu.cn/julia-releases/ # Linux & macOS\nexport JULIA_PKG_SERVER=https://mirrors.nju.edu.cn/julia","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"Windows","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"$env:JULIAUP_SERVER=\"https://mirror.nju.edu.cn/julia-releases/\" # Windows\n$env:JULIA_PKG_SERVER=\"https://mirrors.nju.edu.cn/julia\"","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"An alternative approach is downloading the corresponding Julia binary from the Nanjing university mirror website. After installing the binary, please set the Julia binary path properly if you want to start a Julia REPL from a terminal, check this manual page to learn more.","category":"page"},{"location":"chap2/julia-setup/#Installing-Julia","page":"Setup Julia","title":"Installing Julia","text":"","category":"section"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"To verify that Julia is installed, please open a new terminal and run the following command in your terminal.","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"julia","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"It should start a Julia REPL(Read-Eval-Print-Loop) session.\nIf you wish to install a specific version of Julia, please refer to the documentation.","category":"page"},{"location":"chap2/julia-setup/#Step-2:-Package-Management","page":"Setup Julia","title":"Step 2: Package Management","text":"","category":"section"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"Julia has a mature eco-system for scientific computing.\nPkg is the built-in package manager for Julia.\nTo enter the package manager, press ] in the REPL.","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"(Image: PackageMangement)","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"The environment is indicated by the (@v1.9).\nTo add a package, type add <package name>.\nTo exit the package manager press backspace key\nRead More","category":"page"},{"location":"chap2/julia-setup/#Step-3.-Configure-the-startup-file","page":"Setup Julia","title":"Step 3. Configure the startup file","text":"","category":"section"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"First create a new file ~/.julia/config/startup.jl by executing the following commands ","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"mkdir -p ~/.julia/config\necho 'try\n    using Revise\ncatch e\n    @warn \"fail to load Revise.\"\nend' > ~/.julia/config/startup.jl","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"The contents in the startup file is executed immediately after you open a new Julia session.","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"Then you need to install Revise, which is an Julia package that can greatly improve the using experience of Julia. To install Revise, open Julia REPL and type","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"julia> using Pkg; Pkg.add(\"Revise\")","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"If you don't know about startup.jl and where to find it, here is a good place for further information. ","category":"page"},{"location":"chap2/julia-setup/#More-Packages","page":"Setup Julia","title":"More Packages","text":"","category":"section"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"You may find more Julia packages here.","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"As a final step, please verify your Julia configuration by openning a Julia REPL and type","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"julia> versioninfo()\nJulia Version 1.9.2\nCommit e4ee485e909 (2023-07-05 09:39 UTC)\nPlatform Info:\n  OS: macOS (arm64-apple-darwin22.4.0)\n  CPU: 10 × Apple M2 Pro\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-14.0.6 (ORCJIT, apple-m1)\n  Threads: 1 on 6 virtual cores\nEnvironment:\n  JULIA_NUM_THREADS = 1\n  JULIA_PROJECT = @.\n  JULIA_PKG_SERVER = http://cn-southeast.pkg.juliacn.com/ ","category":"page"},{"location":"chap2/julia-setup/#Step-4.-Download-an-editor:-VSCode","page":"Setup Julia","title":"Step 4. Download an editor: VSCode","text":"","category":"section"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"Install VSCode by downloading the correct binary for your platform from here. Open VSCode and open the Extensions tab on the left side-bar of the window, search Julia and install the most popular extension: julia-vscode","category":"page"},{"location":"chap2/julia-setup/#The-four-modes-of-Julia-REPL","page":"Setup Julia","title":"The four modes of Julia REPL","text":"","category":"section"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"A Julia REPL has four modes,","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"Julian mode is the default mode that can interpret your Julia code.\nShell mode is the mode that you can run shell commands. Press ; in the Julian mode and type","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"shell> date\nSun Nov  6 10:50:21 PM CST 2022","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"To return to the Julian mode, type the Backspace key.","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"Package mode is the mode that you can manage packages. Press ] in the Julian mode and type","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"(@v1.8) pkg> st\nStatus `~/.julia/environments/v1.8/Project.toml`\n  [295af30f] Revise v3.4.0","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"To return to the Julian mode, type the Backspace key.","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"Help mode is the mode that you can access the docstrings of functions. Press ? in the Julian mode and type","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"help> sum\n... docstring for sum ...","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"To return to the Julian mode, type the Backspace key.","category":"page"},{"location":"chap2/julia-setup/","page":"Setup Julia","title":"Setup Julia","text":"read more...","category":"page"},{"location":"chap3/fft/#Fast-Fourier-transform","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"","category":"section"},{"location":"chap3/fft/#Definition","page":"Fast Fourier transform","title":"Definition","text":"","category":"section"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"Given a function f(x) defined on x in mathbbC, the Fourier transformation is defined as","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"g(u) = int_-infty^infty e^-2pi iux f(x) dx","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"The space of u is called the momentum space, and the space of x is called the position space. Its inverse process, or the inverse Fourier transformation is defined as","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"f(x) = int_-infty^infty e^2pi iux g(u) dk","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"The two-dimensional Fourier transformation and its inverse transformation are defined as","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"beginalign*\ng(u v) = int_-infty^inftydyint_-infty^infty e^-2pi i(ux+vy) f(x y) dx\nf(x y) = int_-infty^inftyduint_-infty^infty e^2pi i(ux+vy) g(u v) dv\nendalign*","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"Fourier transformation is widely used in many fields, including","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"Image and audio processing: YouTube: Image Compression and the FFT, Steve Brunton\nSolid state physics: Kittel, Charles, and Paul McEuen. Introduction to solid state physics. John Wiley & Sons, 2018.\nQuantum computing: Nielsen, Michael A., and Isaac L. Chuang. Quantum computation and quantum information. Cambridge university press, 2010.\nFourier optics: Goodman, Joseph W. Introduction to Fourier optics. Roberts and Company publishers, 2005.","category":"page"},{"location":"chap3/fft/#Discrete-Fourier-Transformation-(DFT)","page":"Fast Fourier transform","title":"Discrete Fourier Transformation (DFT)","text":"","category":"section"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"Let x be a vector of length n, the DFT of x is defined as","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"y_i=sum_n=0^n-1x_jcdot e^-frac i2pi nij","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"Since this transformation is linear, we can represent it as a matrix multiplication. Let F_n be the matrix of size n times n defined as","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"F_n = left(\nbeginmatrix\n1  1  1  ldots  1\n1  omega  omega^2  ldots  omega^n-1\n1  omega^2  omega^4  ldots  omega^2n-2\nvdots  vdots  vdots  ddots  vdots\n1  omega^n-1  omega^2n-2  ldots  omega^(n-1)^2\nendmatrix\nright)","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"where omega = e^-2pi in.  This matrix is called the DFT matrix, and the DFT of x is represented as F_n x. The inverse transformation is defined as F_n^dagger xn, i.e. F_n F_n^dagger = I.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"using Test, LinearAlgebra\n\nfunction dft_matrix(n::Int)\n    ω = exp(-2π*im/n)\n    return [ω^((i-1)*(j-1)) for i=1:n, j=1:n]\nend","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"n = 3\nFn = dft_matrix(n)\ndft_matrix(n) * dft_matrix(n)' ./ n","category":"page"},{"location":"chap3/fft/#The-Cooley–Tukey's-Fast-Fourier-transformation-(FFT)","page":"Fast Fourier transform","title":"The Cooley–Tukey's Fast Fourier transformation (FFT)","text":"","category":"section"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"We have a recursive algorithm to compute the DFT.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"F_n x = left(beginmatrixI_n2  D_n2I_n2  -D_n2 endmatrixright)left(beginmatrix F_n2  0  0  F_n2endmatrixright)left(beginmatrixx_rm oddx_rm evenendmatrixright)","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"where D_n = rm diag(1 omega omega^2 ldots omega^n-1).","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"note: Quiz\nWhat is the computational complexity of evaluating F_n x? Hint: T(n) = 2 T(n2) + O(n).","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"using SparseArrays\n\n@testset \"fft decomposition\" begin\n    n = 4\n    Fn = dft_matrix(n)\n    F2n = dft_matrix(2n)\n\n    # the permutation matrix to permute elements at 1:2:n (odd) to 1:n÷2 (top half)\n    pm = sparse([iseven(j) ? (j÷2+n) : (j+1)÷2 for j=1:2n], 1:2n, ones(2n), 2n, 2n)\n\n    # construct the D matrix\n    ω = exp(-π*im/n)\n    d1 = Diagonal([ω^(i-1) for i=1:n])\n\n    # construct F_{2n} from F_n\n    F2n_ = [Fn d1 * Fn; Fn -d1 * Fn]\n    @test F2n * pm' ≈ F2n_\nend","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"We implement the O(nlog(n)) time Cooley-Tukey FFT algorithm.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"function fft!(x::AbstractVector{T}) where T\n    N = length(x)\n    @inbounds if N <= 1\n        return x\n    end\n \n    # divide\n    odd  = x[1:2:N]\n    even = x[2:2:N]\n \n    # conquer\n    fft!(odd)\n    fft!(even)\n \n    # combine\n    @inbounds for i=1:N÷2\n       t = exp(T(-2im*π*(i-1)/N)) * even[i]\n       oi = odd[i]\n       x[i]     = oi + t\n       x[i+N÷2] = oi - t\n    end\n    return x\nend","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"@testset \"fft\" begin\n    x = randn(ComplexF64, 8)\n    @test fft!(copy(x)) ≈ dft_matrix(8) * x\nend","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"The Julia package FFTW.jl contains a superfast FFT implementation.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"using FFTW\n\n@testset \"fft\" begin\n    x = randn(ComplexF64, 8)\n    @test FFTW.fft(copy(x)) ≈ dft_matrix(8) * x\nend","category":"page"},{"location":"chap3/fft/#Application-1:-Fast-polynomial-multiplication","page":"Fast Fourier transform","title":"Application 1: Fast polynomial multiplication","text":"","category":"section"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"Given two polynomials p(x) and q(x)","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"beginalign*\np(x) = sum_k=0^n-1 a_k x^k\nq(x) = sum_k=0^n-1 b_k x^k\nendalign*","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"The multiplication of them is defined as","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"p(x)q(x) = sum_k=0^2n-2 c_k x^k","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"Fourier transformation can be used to compute the product of two polynomials in O(n log n) time, which is much faster than the naive algorithm that takes O(n^2) time.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"note: Algorithm: Fast polynomial multiplication\nEvaluate p(x) and q(x) at 2n points ω^0 ldots  ω^2n1 using DFT. This step takes time O(n log n).\nObtain the values of p(x)q(x) at these 2n points through pointwise multiplicationbeginalign*\n(p circ q)(ω^0) = p(ω^0) q(ω^0) \n(p circ q)(ω^1) = p(ω^1) q(ω^1)\nvdots\n(p circ q)(ω^2n1) = p(ω^2n1) q(ω^2n1)\nendalign*This step takes time O(n).Interpolate the polynomial p circ q at the product values using inverse DFT to obtain coefficients c_0 c_1 ldots c_2n2. This last step requires time O(n log n).We can also use FFT to compute the convolution of two vectors a = (a_0ldots  a_n1) and b = (b_0 ldots  b_n1), which is defined as a vector c = (c_0 ldots  c_n1) wherec_j = sum^j_k=0 a_kb_jk  j = 0ldots n  1The running time is again O(n log n).","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"In the following example, we use the Polynomials package to define the polynomial and use the FFT algorithm to compute the product of two polynomials.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"using Polynomials\np = Polynomial([1, 3, 2, 5, 6])\nq = Polynomial([3, 1, 6, 2, 2])","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"Step 1: evaluate p(x) at 2n-1 different points.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"pvals = fft(vcat(p.coeffs, zeros(4)))","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"which is equivalent to computing:","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"n = 5\nω = exp(-2π*im/(2n-1))\nmap(k->p(ω^k), 0:(2n-1))","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"The same for q(x).","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"qvals = fft(vcat(q.coeffs, zeros(4)))","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"Step 2: Compute p(x) q(x) at 2n-1 points.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"pqvals = pvals .* qvals\nifft(pqvals)","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"Summarize:","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"function fast_polymul(p::AbstractVector, q::AbstractVector)\n    pvals = fft(vcat(p, zeros(length(q)-1)))\n    qvals = fft(vcat(q, zeros(length(p)-1)))\n    pqvals = pvals .* qvals\n    return real.(ifft(pqvals))\nend\n\nfunction fast_polymul(p::Polynomial, q::Polynomial)\n    Polynomial(fast_polymul(p.coeffs, q.coeffs))\nend","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"A similar algorithm has already been implemented in package Polynomials. One can easily verify the correctness.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"p * q\nfast_polymul(p, q)","category":"page"},{"location":"chap3/fft/#Application-2:-Image-compression","page":"Fast Fourier transform","title":"Application 2: Image compression","text":"","category":"section"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"If you google the logo of the Hong Kong University of Science and Technology, you will probably find the following png of size 2000 times 3000.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"using Images\nimg = Images.load(\"../assets/images/hkust-gz.png\")","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"It is too large! We can compress it with the Fourier transformation algorithm. To simplify the discussion, let us using the gray scale image.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"gray_image = Gray.(img)","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"The gray scale image uses 8-bit fixed point numbers as the pixel storage type.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"typeof(gray_image)\nimg_data = Float32.(gray_image)\nimg_data_k = fftshift(fft(img_data))","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"it is sparse!","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"Gray.(abs2.(img_data_k) ./ length(img_data_k))","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"We can store it in the sparse matrix format.","category":"page"},{"location":"chap3/fft/","page":"Fast Fourier transform","title":"Fast Fourier transform","text":"# let us discard all variables smaller than 1e-5\nimg_data_k[abs.(img_data_k) .< 1e-5] .= 0\nsparse_img = sparse(img_data_k)\ncompression_ratio = nnz(sparse_img) / (2000 * 3000)\nrecovered_img = ifft(fftshift(Matrix(sparse_img)))\nGray.(abs.(recovered_img))","category":"page"},{"location":"chap3/sparse/#Sparse-Matrices-and-Graphs","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"","category":"section"},{"location":"chap3/sparse/#Sparse-Matrices","page":"Sparse Matrices and Graphs","title":"Sparse Matrices","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Matrices are often sparse. Consider the matrix that we used in the spring chain example, the stiffness matrix is tridiagonal and has only 3n-2 nonzero elements.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"beginalign*\nA = beginpmatrix\n-C  C  0  ldots  0\nC  -2C  C  ldots  0\n0  C  -2C  ldots  0\nvdots  vdots  vdots  ddots  vdots\n0  0  0  C  -C\nendpmatrix\nendalign*","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Storing such a matrix in a dense format requires n^2 elements, which is very memory inefficient since it has only 3n-2 nonzero elements.","category":"page"},{"location":"chap3/sparse/#COOrdinate-(COO)-format","page":"Sparse Matrices and Graphs","title":"COOrdinate (COO) format","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"The coordinate format means storing nonzero matrix elements into triples","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"beginalign*\n(i_1 j_1 v_1)\n(i_2 j_2 v_2)\nvdots\n(i_k j_k v_k)\nendalign*","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"To store the stiffness matrix in COO format, we only need to store 3n-2 triples.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"To implement a COO matrix in Julia, we need to define a new data type and implement the AbstractArray interface.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"size: return the size of the matrix\ngetindex: return the element at the given index","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Let the number of nonzero elements in a COO matrix A be rm nnz(A). The indexing operation requires enumerating over all rm nnz(A) elements.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"using LinearAlgebra\n\nstruct COOMatrix{Tv, Ti} <: AbstractArray{Tv, 2}   # Julia does not have a COO data type\n    m::Ti                   # number of rows\n    n::Ti                 # number of columns\n    colval::Vector{Ti}   # column indices\n    rowval::Vector{Ti}   # row indices\n    nzval::Vector{Tv}        # values\n    function COOMatrix(m::Ti, n::Ti, colval::Vector{Ti}, rowval::Vector{Ti}, nzval::Vector{Tv}) where {Tv, Ti}\n        @assert length(colval) == length(rowval) == length(nzval)\n        new{Tv, Ti}(m, n, colval, rowval, nzval)\n    end\nend\n\nBase.size(coo::COOMatrix) = (coo.m, coo.n)\nBase.size(coo::COOMatrix, i::Int) = getindex((coo.m, coo.n), i)\n# the number of non-zero elements\nnnz(coo::COOMatrix) = length(coo.nzval)\n\n# implement get index for CSC matrix, call with A[i, j]\nfunction Base.getindex(coo::COOMatrix{Tv}, i::Integer, j::Integer) where Tv\n    @boundscheck checkbounds(coo, i, j)\n    v = zero(Tv)\n    for (i2, j2, v2) in zip(coo.rowval, coo.colval, coo.nzval)\n        if i == i2 && j == j2\n            v += v2  # accumulate the value, since repeated indices are allowed.\n        end\n    end\n    return v\nend\n\nfunction Base.:(*)(A::COOMatrix{T1}, B::COOMatrix{T2}) where {T1, T2}\n    @assert size(A, 2) == size(B, 1)\n    rowval = Int[]\n    colval = Int[]\n    nzval = promote_type(T1, T2)[]\n    for (i, j, v) in zip(A.rowval, A.colval, A.nzval)\n        for (i2, j2, v2) in zip(B.rowval, B.colval, B.nzval)\n            if j == i2\n                push!(rowval, i)\n                push!(colval, j2)\n                push!(nzval, v * v2)\n            end\n        end\n    end\n    return COOMatrix(size(A, 1), size(B, 2), colval, rowval, nzval)\nend","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"using Test\n\nstiffmatrix = COOMatrix(3, 3, [1, 1, 2, 2, 2, 3, 3], [1, 2, 1, 2, 3, 2, 3], [-1.0, 1, 1, -2, 1, 1, -1])\nsize(stiffmatrix)\nnnz(stiffmatrix)\n\ndense_matrix = Matrix(stiffmatrix)\n@test stiffmatrix * stiffmatrix ≈ dense_matrix ^ 2","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Most operations on COO matrices are computational expensive. For example, multiplying two COO matrices requires O(rm nnz(A)^2) computing time.","category":"page"},{"location":"chap3/sparse/#Compressed-Sparse-Column-(CSC)-format","page":"Sparse Matrices and Graphs","title":"Compressed Sparse Column (CSC) format","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"A CSC format sparse matrix can be constructed with the SparseArrays.sparse function. However, here we will implement a simple CSC matrix from scratch.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"struct CSCMatrix{Tv,Ti} <: AbstractMatrix{Tv}\n    m::Int\n    n::Int\n    colptr::Vector{Ti}\n    rowval::Vector{Ti}\n    nzval::Vector{Tv}\n    function CSCMatrix(m::Int, n::Int, colptr::Vector{Ti}, rowval::Vector{Ti}, nzval::Vector{Tv}) where {Tv, Ti}\n        @assert length(colptr) == n + 1\n        @assert length(rowval) == length(nzval) == colptr[end] - 1\n        new{Tv, Ti}(m, n, colptr, rowval, nzval)\n    end\nend\nBase.size(A::CSCMatrix) = (A.m, A.n)\nBase.size(A::CSCMatrix, i::Int) = getindex((A.m, A.n), i)\n# the number of non-zero elements\nnnz(csc::CSCMatrix) = length(csc.nzval)\n\n# convert a COO matrix to a CSC matrix\nfunction CSCMatrix(coo::COOMatrix{Tv, Ti}) where {Tv, Ti}\n    m, n = size(coo)\n    # sort the COO matrix by column\n    order = sortperm(1:nnz(coo); by=i->coo.rowval[i] + m * (coo.colval[i]-1))\n    colptr, rowval, nzval = similar(coo.rowval, n+1), similar(coo.rowval), similar(coo.nzval)\n    k = 0\n    ipre, jpre = 0, 0\n    colptr[1] = 1\n    for idx in order\n        i, j, v = coo.rowval[idx], coo.colval[idx], coo.nzval[idx]\n        # values with the same indices are accumulated\n        if i == ipre && j == jpre\n            nzval[k] += v\n        else\n            k += 1\n            if j != jpre\n                # a new column starts\n                colptr[jpre+1:j+1] .= k\n            end\n            rowval[k] = i\n            nzval[k] = v\n            ipre, jpre = i, j\n        end\n    end\n    colptr[jpre+1:end] .= k + 1\n    resize!(rowval, k)\n    resize!(nzval, k)\n    return CSCMatrix(m, n, colptr, rowval, nzval)\nend\n\n# implement get index for CSC matrix, call with A[i, j]\nfunction Base.getindex(A::CSCMatrix{T}, i::Int, j::Int) where T\n    @boundscheck checkbounds(A, i, j)\n    for k in nzrange(A, j)\n        if A.rowval[k] == i\n            return A.nzval[k]\n        end\n    end\n    return zero(T)\nend\n\nfunction Base.:*(A::CSCMatrix{T1}, B::CSCMatrix{T2}) where {T1, T2}\n    T = promote_type(T1, T2)\n    @assert size(A, 2) == size(B, 1)\n    rowval, colval, nzval = Int[], Int[], T[]\n    for j2 in 1:size(B, 2)  # enumerate the columns of B\n        for k2 in nzrange(B, j2)  # enumerate the rows of B\n            v2 = B.nzval[k2]\n            for k1 in nzrange(A, B.rowval[k2])  # enumerate the rows of A\n                push!(rowval, A.rowval[k1])\n                push!(colval, j2)\n                push!(nzval, A.nzval[k1] * v2)\n            end\n        end\n    end\n    return CSCMatrix(COOMatrix(size(A, 1), size(B, 2), colval, rowval, nzval))\nend\n\n# return the range of non-zero elements in the j-th column\nnzrange(A::CSCMatrix, j::Int) = A.colptr[j]:A.colptr[j+1]-1","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"coo_matrix = COOMatrix(5, 4, [1, 1, 2, 2, 4, 4], [2, 3, 1, 4, 3, 4], [1, 2, 3, 4, 5, 6])\ncsc_matrix = CSCMatrix(coo_matrix)","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"The csc_matrix has type CSCMatrix, which contains 5 fields","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"fieldnames(csc_matrix |> typeof)\ncsc_matrix.m, csc_matrix.n\ncsc_matrix.colptr\ncsc_matrix.rowval\ncsc_matrix.nzval","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"(Image: )","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"The m, n, rowval and nzval have the same meaning as those in the COO format. colptr is an integer vector of size n+1, where colptr[j] is the index in rowval and nzval of the first nonzero element in the j-th column, and colptr[j+1] is the index of the first nonzero element in the (j+1)-th column. Hence the j-th column of the matrix is stored in rowval[colptr[j]:colptr[j+1]-1] and nzval[colptr[j]:colptr[j+1]-1].","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"The number of operations required to index an element in the j-th column of a CSC matrix is linear to the nonzero elements in the j-th column. To get an element from the 2nd row and 3rd column of a CSC matrix, we can use the following code","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"csc_matrix[2, 3]","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"The row indices and values of nonzero elements in the 3rd column can be obtained by","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"rows3 = csc_matrix.rowval[csc_matrix.colptr[3]:csc_matrix.colptr[4]-1]\nval3 = csc_matrix.nzval[csc_matrix.colptr[3]:csc_matrix.colptr[4]-1]\ncsc_matrix.rowval[nzrange(csc_matrix, 3)] # or equivalently, we can use `nzrange`","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Multiplying two CSC matrices is much faster than multiplying two COO matrices. The time complexity of multiplying two CSC matrices A and B is O(rm nnz(A)rm nnz(B)n).","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"csc_matrix2 = CSCMatrix(COOMatrix(coo_matrix.n, coo_matrix.m, coo_matrix.rowval, coo_matrix.colval, coo_matrix.nzval))  # transpose\n@test Matrix(csc_matrix) * Matrix(csc_matrix2) ≈ csc_matrix * csc_matrix2","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"note: Question\nCan you explain why the above test does not pass?","category":"page"},{"location":"chap3/sparse/#Dominant-eigenvalue-problem","page":"Sparse Matrices and Graphs","title":"Dominant eigenvalue problem","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Given a matrix A in mathbbR^n times n, the dominant eigenvalue problem is to find the largest eigenvalue lambda_1 and its corresponding eigenvector x_1 such that","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"A x_1 = lambda_1 x_1","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"The power method is a simple iterative algorithm to solve the dominant eigenvalue problem. The algorithm starts with a random vector v_0 and repeatedly multiplies it with the matrix A.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"v_k = A^k v_0","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"By representing the initial vector v_0 as a linear combination of eigenvectors of A, i.e. v_0 = sum_i=1^n c_i x_i, we have","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"v_k = sum_i=1^n lambda_i^k c_i x_i","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"where lambda_1  lambda_2 geq ldots geq lambda_n are the eigenvalues of A and x_i are the corresponding eigenvectors. The power method converges to the eigenvector corresponding to the largest eigenvalue as k rightarrow infty. The rate of convergence is dedicated by lambda_2lambda_1^k. The Julia code for the power method is as follows.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"function power_method(A::AbstractMatrix{T}, n::Int) where T\n    n = size(A, 2)\n    x = normalize!(randn(n))\n    for i=1:n\n        x = A * x\n        normalize!(x)\n    end\n    return x' * A * x', x\nend","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"By inverting the sign, Arightarrow -A, we can use the same method to obtain the smallest eigenvalue.","category":"page"},{"location":"chap3/sparse/#The-Krylov-subspace-method","page":"Sparse Matrices and Graphs","title":"The Krylov subspace method","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Let A in mathbbC^n times n be a large sparse matrix, the Arnoldi and Lanczos algorithms can be used to obtain its largest/smallest eigenvalue, with much faster convergence speed comparing with the power method.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"The key idea of these algorithms is to generate an orthogonal matrix Q in mathbbC^ntimes k, Q^dagger Q = I, such that","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Q^dagger A Q = B","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"We have the following property","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"lambda_1(B) leq lambda_1(A)","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"where lambda_1(A) is the largest eigenvalue of A. By chooing Q carefully, such that rm span(Q) contains the dominant eigenvectors of A, then lambda_1(B) = lambda_1(A). When the equality holds, we have","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"By_1 = lambda_1(B)y_1","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"where y_i is the i-th eigenvector of B. By multiplying y^dagger on the left, we have","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"y_1^dagger Q^dagger A Q y_1 = lambda_1(B)","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Hence, the eigenvectors of B are related to the eigenvectors of A by the orthogonal matrix Q.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Inspired by the power method, we can define the Q as the Krylov subspace that generated from a random initial vector q_1.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"mathcalK(A q_1 k) = rm spanq_1 Aq_1 A^2q_1 ldots A^k-1q_1","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"The Arnoldi and Lanczos algorithm are two special cases of the Krylov subspace method. The Arnoldi algorithm is used to solve the eigenvalue problem, while the Lanczos algorithm is used to solve the symmetric eigenvalue problem.","category":"page"},{"location":"chap3/sparse/#KrylovKit.jl","page":"Sparse Matrices and Graphs","title":"KrylovKit.jl","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"The Julia package KrylovKit.jl contains many Krylov space based algorithms. KrylovKit.jl accepts general functions or callable objects as linear maps, and general Julia objects with vector like behavior (as defined in the docs) as vectors. The high level interface of KrylovKit is provided by the following functions:","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"linsolve: solve linear systems\neigsolve: find a few eigenvalues and corresponding eigenvectors\ngeneigsolve: find a few generalized eigenvalues and corresponding vectors\nsvdsolve: find a few singular values and corresponding left and right singular vectors\nexponentiate: apply the exponential of a linear map to a vector\nexpintegrator: exponential integrator   for a linear non-homogeneous ODE, computes a linear combination of the ϕⱼ functions which generalize ϕ₀(z) = exp(z).","category":"page"},{"location":"chap3/sparse/#The-Lanczos-algorithm","page":"Sparse Matrices and Graphs","title":"The Lanczos algorithm","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"The Lanczos algorithm is used to solve the symmetric eigenvalue problem. Given a symmetric matrix A in mathbbR^n times n, the Lanczos algorithm generates an orthogonal matrix Q in mathbbR^n times k, such that","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Q^T A Q = T","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"where T is a tridiagonal matrix","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"T = left(beginmatrix\nalpha_1  beta_1  0  ldots  0\nbeta_1  alpha_2  beta_2  ldots  0\n0  beta_2  alpha_3  ldots  0\nvdots  vdots  vdots  ddots  vdots\n0  0  0  beta_k-1  alpha_k\nendmatrixright)","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Let Q = q_1  q_2  ldots  q_n and rm span(q_1 q_2 ldots q_k) = mathcalK(A q_1 k). We have Aq_k = beta_k-1q_k-1 + alpha_k q_k + beta_k q_k+1, or equivalently in the recursive style","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"q_k+1 = (Aq_k - beta_k-1q_k-1 - alpha_k q_k)beta_k","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"By multiplying q_k^T on the left, we have","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"alpha_k  = q_k^T A q_k","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Since q_k+1 is normalized, we have","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"beta_k = Aq_k - beta_k-1q_k-1 - alpha_k q_k_2","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"If at any moment, beta_k = 0, the interation stops due to convergence of a subspace. We have the following reducible form","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"T(beta_2 = 0) = left(beginarrayccccc\nalpha_1  beta_1  0  ldots  0\nbeta_1  alpha_2  0  ldots  0\nhline\n0  0  alpha_3  ldots  0\nvdots  vdots  vdots  ddots  vdots\n0  0  0  beta_k-1  alpha_k\nendarrayright)","category":"page"},{"location":"chap3/sparse/#A-Julia-implementation","page":"Sparse Matrices and Graphs","title":"A Julia implementation","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"function lanczos(A, q1::AbstractVector{T}; abstol, maxiter) where T\n    # normalize the input vector\n    q1 = normalize(q1)\n    # the first iteration\n    q = [q1]\n    Aq1 = A * q1\n    α = [q1' * Aq1]\n    rk = Aq1 .- α[1] .* q1\n    β = [norm(rk)]\n    for k = 2:min(length(q1), maxiter)\n        # the k-th orthonormal vector in Q\n        push!(q, rk ./ β[k-1])\n        Aqk = A * q[k]\n        # compute the diagonal element as αₖ = qₖᵀ A qₖ\n        push!(α, q[k]' * Aqk)\n        rk = Aqk .- α[k] .* q[k] .- β[k-1] * q[k-1]\n        # compute the off-diagonal element as βₖ = |rₖ|\n        nrk = norm(rk)\n        # break if βₖ is smaller than abstol or the maximum number of iteration is reached\n        if abs(nrk) < abstol || k == length(q1)\n            break\n        end\n        push!(β, nrk)\n    end\n    # returns T and Q\n    return SymTridiagonal(α, β), hcat(q...)\nend","category":"page"},{"location":"chap3/sparse/#Reorthogonalization","page":"Sparse Matrices and Graphs","title":"Reorthogonalization","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Let r_0 ldots r_k-1 in mathbbC_n be linearly independent vectors and the corresponding Householder matrices H_0 ldots H_k-1 such that (H_0ldots H_k- 1)^T r_0midldotsmid r_k-1 is an upper triangular matrix. Let q_1 mid ldots mid q_k  denote the first k columns of the Householder product (H_0 ldots H_k-1), then q_1 ldots q_k are orthonormal vectors up to machine precision. The Lanczos algorithm with complete reorthogonalization is as follows:","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"function lanczos_reorthogonalize(A, q1::AbstractVector{T}; abstol, maxiter) where T\n    n = length(q1)\n    # normalize the input vector\n    q1 = normalize(q1)\n    # the first iteration\n    q = [q1]\n    Aq1 = A * q1\n    α = [q1' * Aq1]\n    rk = Aq1 .- α[1] .* q1\n    β = [norm(rk)]\n    householders = [householder_matrix(q1)]\n    for k = 2:min(n, maxiter)\n        # reorthogonalize rk: 1. compute the k-th householder matrix\n        for j = 1:k-1\n            left_mul!(view(rk, j:n), householders[j])\n        end\n        push!(householders, householder_matrix(view(rk, k:n)))\n        # reorthogonalize rk: 2. compute the k-th orthonormal vector in Q\n        qk = zeros(T, n); qk[k] = 1  # qₖ = H₁H₂…Hₖeₖ\n        for j = k:-1:1\n            left_mul!(view(qk, j:n), householders[j])\n        end\n        push!(q, qk)\n        Aqk = A * q[k]\n        # compute the diagonal element as αₖ = qₖᵀ A qₖ\n        push!(α, q[k]' * Aqk)\n        rk = Aqk .- α[k] .* q[k] .- β[k-1] * q[k-1]\n        # compute the off-diagonal element as βₖ = |rₖ|\n        nrk = norm(rk)\n        # break if βₖ is smaller than abstol or the maximum number of iteration is reached\n        if abs(nrk) < abstol || k == n\n            break\n        end\n        push!(β, nrk)\n    end\n    return SymTridiagonal(α, β), hcat(q...)\nend\nstruct HouseholderMatrix{T} <: AbstractArray{T, 2}\n    v::Vector{T}\n    β::T\nend\n\n# the `mul!` interfaces can take two extra factors.\nfunction left_mul!(B, A::HouseholderMatrix)\n    B .-= (A.β .* A.v) * (A.v' * B)\n    return B\nend\n\nfunction householder_matrix(v::AbstractVector{T}) where T\n    v = copy(v)\n    v[1] -= norm(v, 2)\n    return HouseholderMatrix(v, 2/norm(v, 2)^2)\nend","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"using Graphs\nn = 1000\ngraph = random_regular_graph(n, 3)\nA = laplacian_matrix(graph)\nq1 = randn(n)\ntr, Q = lanczos_reorthogonalize(A, q1; abstol=1e-5, maxiter=100)\neigen(tr)\n\nusing KrylovKit\neigsolve(A, q1, 2, :SR)","category":"page"},{"location":"chap3/sparse/#Notes-on-Lanczos","page":"Sparse Matrices and Graphs","title":"Notes on Lanczos","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"A sophisticated Lanczos implementation should consider the following aspects:","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"In practice, storing all q vectors is not necessary.\nBlocking technique can be used to improve the solution, especially when the matrix has degenerate eigenvalues.\nRestarting technique can be used to improve the solution without increasing the memory usage.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"These techniques could be found in Ref.[Golub2013].","category":"page"},{"location":"chap3/sparse/#The-Arnoldi-algorithm","page":"Sparse Matrices and Graphs","title":"The Arnoldi algorithm","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"If A is not symmetric, then the orthogonal tridiagonalization Q^T A Q = T does not exist in general. The Arnoldi approach involves the column by column generation of an orthogonal Q such that Q^TAQ = H is a Hessenberg matrix.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"H = left(beginmatrix\nh_11  h_12  h_13  ldots  h_1k\nh_21  h_22  h_23  ldots  h_2k\n0  h_32  h_33  ldots  h_3k\nvdots  vdots  vdots  ddots  vdots\n0  0  0  ldots  h_kk\nendmatrixright)","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"That is, h_ij = 0 for ij+1.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"function arnoldi_iteration(A::AbstractMatrix{T}, x0::AbstractVector{T}; maxiter) where T\n    h = Vector{T}[]\n    q = [normalize(x0)]\n    n = length(x0)\n    @assert size(A) == (n, n)\n    for k = 1:min(maxiter, n)\n        u = A * q[k]    # generate next vector\n        hk = zeros(T, k+1)\n        for j = 1:k # subtract from new vector its components in all preceding vectors\n            hk[j] = q[j]' * u\n            u = u - hk[j] * q[j]\n        end\n        hkk = norm(u)\n        hk[k+1] = hkk\n        push!(h, hk)\n        if abs(hkk) < 1e-8 || k >=n # stop if matrix is reducible\n            break\n        else\n            push!(q, u ./ hkk)\n        end\n    end\n\n    # construct `h`\n    kmax = length(h)\n    H = zeros(T, kmax, kmax)\n    for k = 1:length(h)\n        if k == kmax\n            H[1:k, k] .= h[k][1:k]\n        else\n            H[1:k+1, k] .= h[k]\n        end\n    end\n    return H, hcat(q...)\nend","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"import SparseArrays\nn = 100\nA = SparseArrays.sprand(n, n, 0.1)\nq1 = randn(n)\nh, q = arnoldi_iteration(A, q1; maxiter=20)\neigen(h).values   # naive implementation\neigsolve(A, q1, 2, :LR)  # KrylovKit.eigsolve","category":"page"},{"location":"chap3/sparse/#Graphs","page":"Sparse Matrices and Graphs","title":"Graphs","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"A graph is a pair G = (V E), where V is a set of vertices and E is a set of edges. In Julia, the package Graphs.jl provides a simple graph data structure. The following code creates a simple graph with 10 vertices.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"using Graphs\ng = SimpleGraph(10)  # create an empty graph with 10 vertices\nadd_vertex!(g)  # add a vertex\nadd_edge!(g, 3, 11)  # add an edge between vertex 3 and 11\nhas_edge(g, 3, 11)  # check if there is an edge between vertex 3 and 11\nrem_vertex!(g, 7)  # remove vertex 7\nhas_edge(g, 3, 11)\nhas_edge(g, 3, 7)  # vertex number 11 \"renamed\" to vertex number 7\nneighbors(g, 3)   # get the neighbors of vertex 3","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"A graph can be represented by an adjacency matrix A in mathbbR^n times n, where n is the number of vertices. The element A_ij is 1 if there is an edge between vertex i and vertex j, and 0 otherwise.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"For example, the adjacency matrix of the Petersen graph is","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"using Graphs\ngraph = smallgraph(:petersen)\nadj_matrix = adjacency_matrix(graph)","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"(Image: )","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"The Laplacian matrix L_ntimes n of a graph G is defined as L = D - A, where D is the degree matrix of the graph. The degree matrix is a diagonal matrix, where the diagonal element D_ii is the degree of vertex i. The Laplacian matrix is symmetric and positive semidefinite.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"lap_matrix = laplacian_matrix(graph)","category":"page"},{"location":"chap3/sparse/#The-spectral-graph-theory","page":"Sparse Matrices and Graphs","title":"The spectral graph theory","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Theorem: The number of connected components in the graph is the dimension of the nullspace of the Laplacian and the algebraic multiplicity of the 0 eigenvalue.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"graphsize = 1000\ngraph = random_regular_graph(graphsize, 3)\nlmat = laplacian_matrix(graph)\nq1 = randn(graphsize)\ntri, Q = lanczos(lmat, q1; abstol=1e-8, maxiter=100)\n-eigen(-tri).values  # the eigenvalues of the tridiagonal matrix\nQ' * Q             # the orthogonality of the Krylov vectors\neigsolve(lmat, q1, 2, :SR)  # using function `KrylovKit.eigsolve`","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"NOTE: with larger graph_size, you should see some \"ghost\" eigenvalues ","category":"page"},{"location":"chap3/sparse/#Graph-layout-and-clustering","page":"Sparse Matrices and Graphs","title":"Graph layout and clustering","text":"","category":"section"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Given a graph, we can use the spectral graph theory to detect the number of connected components and the clustering of the graph. What if we are interested in the clustering of the graph? The spectral clustering algorithm is a popular method to partition a graph into clusters[Ng2001]. The algorithm is as follows:","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Given a set of points S = s_1 ldotss_n in mathbbR^l that we want to cluster into k subsets:","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Form the affinity matrix A in mathbbR^ntimes n defined by A_ij = exp(-s_i -s_j_22sigma^2) if i neq j, and A_ii = 0.\nDefine D to be the diagonal matrix whose (i i)-element is the sum of A's i-th row, and construct the matrix L = D^-12AD^-12.\nFind x_1  x_2 ldots x_k, the k largest eigenvectors of L (chosen to be orthogonal to each other in the case of repeated eigenvalues), and form the matrix X = x_1x_2ldots x_k in mathbbR^ntimes n by stacking the eigenvectors in columns.\nForm the matrix Y from X by renormalizing each of X's rows to have unit length (i.e. Y_ij = X_ij(sum_j X_ij^2)^12).\nTreating each row of Y as a point in mathbbR^k, cluster them into k clusters via K-means or any other algorithm (that attempts to minimize distortion).\nFinally, assign the original point S_i to cluster j if and only if row i of the matrix Y was assigned to cluster j.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"Here, the scaling parameter sigma^2 controls how rapidly the affinity A_ij falls off with the distance between s_i and s_j, and we will later describe a method for choosing it automatically.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"For an implementation of the spectral clustering algorithm, please check the demo.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"[Golub2013]: Golub, Gene H., and Charles F. Van Loan. Matrix computations. JHU press, 2013.","category":"page"},{"location":"chap3/sparse/","page":"Sparse Matrices and Graphs","title":"Sparse Matrices and Graphs","text":"[Ng2001]: Ng, Andrew, Michael Jordan, and Yair Weiss. \"On spectral clustering: Analysis and an algorithm.\" Advances in neural information processing systems 14 (2001).","category":"page"},{"location":"chap1/terminal/#Get-a-Terminal!","page":"Get a Terminal!","title":"Get a Terminal!","text":"","category":"section"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"You need to get a working terminal to follow the instructions in this book, because every cool guy needs a terminal.","category":"page"},{"location":"chap1/terminal/#Linux-operating-system","page":"Get a Terminal!","title":"Linux operating system","text":"","category":"section"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"Using Linux or macOS is the most straight-forward way to get a terminal. Just like Windows, IOS, and macOS, Linux is an operating system. In fact, Android, one of the most popular platforms on the planet, is powered by the Linux operating system. It is free to use, open source, widely used on clusters and good at automating your works. Linux kernel and Linux distribution are different concepts.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"The Linux kernel is started by Linus Torvalds in 1991.\nA Linux distribution is an operating system made from a software collection that includes the Linux kernel and, often, a package management system.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"The Linux distribution used in this course is Ubuntu. If you want to stick to Windows, you can use Windows Subsystem for Linux (WSL) to get a Linux terminal.","category":"page"},{"location":"chap1/terminal/#Shell-(or-Terminal)","page":"Get a Terminal!","title":"Shell (or Terminal)","text":"","category":"section"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"Although you can use a graphical user interface (GUI) to interact with your Linux distribution, you will find that the command line interface (CLI) is more efficient and powerful. The CLI is also known as the shell or terminal.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"The shell is a program that takes commands from the keyboard and gives them to the operating system to perform. Zsh and Bash are two popular shell interpreters used in the Linux operating systems.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"Bash is the default shell on most Linux distributions.\nZsh (with oh-my-zsh extension) is an extended version of the shell, with a more powerful command-line editing and completion system. It includes features like spelling correction and tab-completion, and it also supports plugins and themes.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"In Ubuntu, one can use Ctrl + Alt + T to open a shell. In a shell, we use","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"man command_name to get help information related to a command,\nCTRL-C to break a program and\nCTRL-D to exit a shell.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"To learn more about shell, please check:","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"MIT Open course: Missing semester\nGet started with the Linux command line and the Shell","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"The following is a short list of bash commands that will be used frequently in this book.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"man     # an interface to the system reference manuals\n\nls      # list directory contents\ncd      # change directory\nmkdir   # make directories\nrm      # remove files or directories\npwd     # print name of current/working directory\n\necho    # display a line of text\ncat     # concatenate files and print on the standard output\n\nalias   # create an alias for a command\n\nlscpu   # display information about the CPU architecture\nlsmem   # list the ranges of available memory with their online status\n\ntop     # display Linux processes\nssh     # the OpenSSH remote login client\nvim     # Vi IMproved, a programmer's text editor\ngit     # the stupid content tracker\n\ntar     # an archiving utility","category":"page"},{"location":"chap1/terminal/#Editor-in-terminal-Vim","page":"Get a Terminal!","title":"Editor in terminal - Vim","text":"","category":"section"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"To edit files in the terminal, you can use Vim - the default text editor in most Linux distributions. Vim has three primary modes, each tailored for specific tasks. The primary modes include","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"Normal Mode, where users can navigate through the file and perform tasks like deleting lines or copying text; One can enter the normal mode by typing ESC;\nInsert Mode, where users can insert text as in conventional text editors; One can enter the insert mode by typing i in the normal mode;\nCommand Mode, where users input commands for tasks like saving files or searching; One can enter the command mode by typing : in the normal mode.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"A few commands are listed below to get you started with Vim.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"i       # input\n:w      # write\n:q      # quit\n:q!     # force quit without saving\n\nu       # undo\nCTRL-R  # redo","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"All the commands must be executed in the normal mode (press ESC if not). To learn more about Vim, please check this lecture.","category":"page"},{"location":"chap1/terminal/#Connect-to-the-remote-SSH","page":"Get a Terminal!","title":"Connect to the remote - SSH","text":"","category":"section"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"The Secure Shell (SSH) protocol is a method for securely sending commands to a computer over an unsecured network. SSH uses cryptography to authenticate and encrypt connections between devices. It is widely used to:","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"push code to a remote git repository,\nlog into a remote machine and execute commands.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"Consider you want to access a remote machine, e.g. the cluster of your university. You will be given a host name and a username. You can use the following command to log in to the remote machine.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"ssh <username>@<hostname>","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"where <username> is the user's account name and <hostname> is the host name or IP of the target machine. You will get logged in after inputting the password.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"Tips to make your life easier","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"It will be tedious to type the host name and user name everytime you want to login to the remote machine. You can setup the ~/.ssh/config file to make your life easier. The following is an example of the ~/.ssh/config file.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"Host amat5315\n  HostName <hostname>\n  User <username>","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"where amat5315 is the alias of the host. After setting up the ~/.ssh/config, you can login to the remote machine by typing","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"ssh amat5315","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"If you want to avoid typing the password everytime you login, you can use the command ","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"ssh-keygen","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"to generate a pair of public and private keys, which will be stored in the ~/.ssh folder on the local machine. After setting up the keys, you can copy the public key to the remote machine by typing","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"ssh-copy-id amat5315","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"Try connecting to the remote machine again, you will find that you don't need to type the password anymore.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"note: How does an SSH key pair work?\nThe SSH key pair is a pair of asymmetric keys, one is the public key and the other is the private key. In the above example, the public key is uploaded to the remote machine and the private key is stored on the local machine. The public key can be shared with anyone, but the private key must be kept secret.To connect to a server, the server needs to know that you are the one who with the right to access it. To do so, the server will need to check if you have the private key that corresponds to the public key stored on the server. If you have the private key, you will be granted access to the server.The secret of the SSH key pair is that the public key can be used to encrypt a message that can only be decrypted by the private key, i.e. the public key is more like a lock and the private key is the key to unlock the lock. This is the foundation of the SSH protocol. So server can send you a message encrypted by your public key, and only you can decrypt it with your private key. This is how the server knows that you are the one who has the private key without actually sending the private key to the server.","category":"page"},{"location":"chap1/terminal/#Practice","page":"Get a Terminal!","title":"Practice","text":"","category":"section"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"In the following example, we will use the ssh command to connect to the remote machine gpu and do some basic operations. If you don't have a remote machine, you can use your local machine to do the following operations.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"(base) ➜  ~ ssh gpu\nWelcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-91-generic x86_64)\n...\n*** System restart required ***\nLast login: Tue Mar  5 06:20:05 2024 from 10.13.139.204\n(base) ➜  ~","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"Then we switch to the jcode directory and create a directory test and a file README.md in the directory.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"(base) ➜  ~ ls     # list directory contents\nClusterManagement                 jcode       packages\nScientificComputingForPhysicists  miniconda3  software\n(base) ➜  ~ cd jcode   # change directory\n(base) ➜  jcode mkdir test # make directories\n(base) ➜  jcode cd test # change directory\n(base) ➜  test vim README.md # create a file and edit it","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"You will see the following screen after typing vim README.md. (Image: ) Type i to enter the insert mode and type some text, e.g. \"# Read me!\". Then type ESC to enter the normal mode and type :wq to save and quit the file.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"After returning to the terminal, you can type ls -l to check the file you just created.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"total 4\n-rw-rw-r-- 1 jinguoliu jinguoliu 11 Mar  5 06:30 README.md","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"You can also use the cat command to check the content of the file.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"(base) ➜  test cat README.md\n# Read me!","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"Finally, you can press CTRL-D to exit the shell.","category":"page"},{"location":"chap1/terminal/","page":"Get a Terminal!","title":"Get a Terminal!","text":"Enjoy your journey in the terminal!","category":"page"},{"location":"chap3/lu/#Solving-linear-equations-by-LU-factorization:-Bottom-up","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"","category":"section"},{"location":"chap3/lu/#Forward-substitution","page":"Solving linear equations by LU factorization: Bottom-up","title":"Forward-substitution","text":"","category":"section"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Forward substitution is an algorithm used to solve a system of linear equations with a lower triangular matrix","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Lx = b","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"where L in mathbbR^ntimes n is a lower triangular matrix defined as","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"L = left(beginmatrix\nl_11  0  ldots  0\nl_21  l_22  ldots  0\nvdots  vdots  ddots  vdots\nl_n1  l_n2  ldots  l_nn\nendmatrixright)","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"The forward substitution can be summarized to the following algorithm","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"x_1 = b_1l_11 x_i = left(b_i - sum_j=1^i-1l_ijx_jright)l_ii i=2  n","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"note: Example\nConsider the following system of lower triangular linear equations:L = left(beginmatrix\n3  0  0\n2  5  0\n1  4  2\nendmatrixright)\nleft(beginmatrix\nx_1\nx_2\nx_3\nendmatrixright) = \nleft(beginmatrix\n9\n12\n13\nendmatrixright)To solve for x_1, x_2, and x_3 using forward substitution, we start with the first equation:3x_1 + 0x_2 + 0x_3 = 9Solving for x_1, we get x_1 = 3. Substituting x = 3 into the second equation (row), we get:2(3) + 5x_2 + 0x_3 = 12Solving for x_2, we get x_2 = (12 - 6)  5 = 12. Substituting x = 3 and x_2 = 12 into the third equation (row), we get:1(3) + 4(12) + 2x_3 = 13Solving for x_3, we get x_3 = (13 - 3 - 4(12))  2 = 15. Therefore, the solution to the system of equations is:x = left(beginmatrix\n3\n12\n15\nendmatrixright)","category":"page"},{"location":"chap3/lu/#Back-substitution","page":"Solving linear equations by LU factorization: Bottom-up","title":"Back-substitution","text":"","category":"section"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Back substitution is an algorithm used to solve a system of linear equations with an upper triangular matrix","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Ux = b","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"where U in mathbbR^ntimes n is an upper triangular matrix defined as","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"U = left(beginmatrix\nu_11  u_12  ldots  u_1n\n0  u_22  ldots  u_2n\nvdots  vdots  ddots  vdots\n0  0  ldots  u_nn\nendmatrixright)","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"The back substitution can be summarized to the following algorithm","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"x_n = b_nu_nn x_i = left(b_i - sum_j=i+1^nu_ijx_jright)u_ii i=n-1  1","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"We implement the above algorithm in Julia language.","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"function back_substitution!(l::AbstractMatrix, b::AbstractVector)\n    n = length(b)\n    @assert size(l) == (n, n) \"size mismatch\"\n    x = zero(b)\n    # loop over columns\n    for j = 1:n\n        # stop if matrix is singular\n        if iszero(l[j, j])\n            error(\"The lower triangular matrix is singular!\")\n        end\n        # compute solution component\n        x[j] = b[j] / l[j, j]\n        for i = j+1:n\n            # update right hand side\n            b[i] = b[i] - l[i, j] * x[j]\n        end\n    end\n    return x\nend","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"We can write a test for this algorithm.","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"using Test, LinearAlgebra\n\n@testset \"back substitution\" begin\n    # create a random lower triangular matrix\n    l = LinearAlgebra.tril(randn(4, 4))\n    # target vector\n    b = randn(4)\n    # solve the linear equation with our algorithm\n    x = back_substitution!(l, copy(b))\n    @test l * x ≈ b\n\n    # The Julia's standard library `LinearAlgebra` contains a native implementation.\n    x_native = LowerTriangular(l) \\ b\n    @test l * x_native ≈ b\nend","category":"page"},{"location":"chap3/lu/#LU-Factorization-with-Gaussian-Elimination","page":"Solving linear equations by LU factorization: Bottom-up","title":"LU Factorization with Gaussian Elimination","text":"","category":"section"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"LU decomposition is a method for solving linear equations that involves breaking down a matrix into lower and upper triangular matrices. The LU decomposition of a matrix A is represented as A = LU, where L is a lower triangular matrix and U is an upper triangular matrix.","category":"page"},{"location":"chap3/lu/#The-elementary-elimination-matrix","page":"Solving linear equations by LU factorization: Bottom-up","title":"The elementary elimination matrix","text":"","category":"section"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"An elementary elimination matrix is a matrix that is used in the process of Gaussian elimination to transform a system of linear equations into an equivalent system that is easier to solve. It is a square matrix that is obtained by performing a single elementary row operation on the identity matrix.","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"(M_k)_ij = begincases\n    delta_ij  i= j\n    - a_ika_kk  i  j land j = k \n    0  rm otherwise\nendcases","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Let A = (a_ij) be a square matrix of size n times n. The kth elementary elimination matrix for it is defined as","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"M_k = left(beginmatrix\n\n1  ldots  0  0  0  ldots  0\nvdots  ddots  vdots  vdots  vdots  ddots  vdots\n0  ldots  1  0  0  ldots  0\n0  ldots  0  1  0  ldots  0\n0  ldots  0  -m_k+1  1  ldots  0\nvdots  ddots  vdots  vdots  vdots  ddots  vdots\n0  ldots  0  -m_n  0  ldots  1\n\nendmatrixright)","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"where m_i = a_ika_kk.","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"By applying this elementary elimination matrix M_1 on A, we can obtain a new matrix with the a_i1 = 0 for all i1.","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"M_1 A = left(beginmatrix\na_11  a_12  a_13  ldots  a_1n\n0  a_22  a_23  ldots  a_2n\n0  a_32  a_33  ldots  a_3n\nvdots  vdots  vdots  ddots  vdots\n0  a_n2  a_n3  ldots  a_nn\nendmatrixright)","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"For k=12ldotsn, apply M_k on A. We will have an upper triangular matrix.","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"U = M_n-1ldots M_1 A","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Since M_k is reversible, we have","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"beginalign*\nA = LU\nL = M_1^-1 M_2^-1 ldots M_n-1^-1\nendalign*","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Elementary elimination matrices have the following properties that making the above process efficient:","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Its inverse can be computed in O(n) time\nM_k^-1 = 2I - M_k\nThe multiplication of two elementary matrices can be computed in O(n) time\nM_k M_k  k = M_k + M_k - I","category":"page"},{"location":"chap3/lu/#Code:-Elementary-Elimination-Matrix","page":"Solving linear equations by LU factorization: Bottom-up","title":"Code: Elementary Elimination Matrix","text":"","category":"section"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"A3 = [1 2 2; 4 4 2; 4 6 4]\n\nfunction elementary_elimination_matrix(A::AbstractMatrix{T}, k::Int) where T\n    n = size(A, 1)\n    @assert size(A, 2) == n\n    # create Elementary Elimination Matrices\n    M = Matrix{Float64}(I, n, n)\n    for i=k+1:n\n        M[i, k] =  -A[i, k] ./ A[k, k]\n    end\n    return M\nend","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"The elementary elimination matrix for the above matrix A3 eliminating the first column is","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"elementary_elimination_matrix(A3, 1)\nelementary_elimination_matrix(A3, 1) * A3","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Verify the property 1","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"inv(elementary_elimination_matrix(A3, 1))","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Verify the property 2","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"elementary_elimination_matrix(A3, 2)\ninv(elementary_elimination_matrix(A3, 1)) * inv(elementary_elimination_matrix(A3, 2))","category":"page"},{"location":"chap3/lu/#Code:-LU-Factorization-by-Gaussian-Elimination","page":"Solving linear equations by LU factorization: Bottom-up","title":"Code: LU Factorization by Gaussian Elimination","text":"","category":"section"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"A naive implementation of elimentary elimination matrix is as follows","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"function lufact_naive!(A::AbstractMatrix{T}) where T\n    n = size(A, 1)\n    @assert size(A, 2) == n\n    M = Matrix{T}(I, n, n)\n    for k=1:n-1\n        m = elementary_elimination_matrix(A, k)\n        M = M * inv(m)\n        A .= m * A\n    end\n    return M, A\nend\n\nlufact_naive!(copy(A3))\n\n@testset \"naive LU factorization\" begin\n    A = [1 2 2; 4 4 2; 4 6 4]\n    L, U = lufact_naive!(copy(A))\n    @test L * U ≈ A\nend","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"The above implementation has time complexity O(n^4) since we did not use the sparsity of elimentary elimination matrix. A better implementation that gives O(n^3) time complexity is as follows.","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"function lufact!(a::AbstractMatrix)\n    n = size(a, 1)\n    @assert size(a, 2) == n \"size mismatch\"\n    m = zero(a)\n    m[1:n+1:end] .+= 1\n    # loop over columns\n    for k=1:n-1\n        # stop if pivot is zero\n        if iszero(a[k, k])\n            error(\"Gaussian elimination fails!\")\n        end\n        # compute multipliers for current column\n        for i=k+1:n\n            m[i, k] = a[i, k] / a[k, k]\n        end\n        # apply transformation to remaining sub-matrix\n        for j=k+1:n\n            for i=k+1:n\n                a[i,j] -= m[i,k] * a[k, j]\n            end\n        end\n    end\n    return m, triu!(a)\nend\n\nlufact(a::AbstractMatrix) = lufact!(copy(a))\n\n@testset \"LU factorization\" begin\n    a = randn(4, 4)\n    L, U = lufact(a)\n    @test istril(L)\n    @test istriu(U)\n    @test L * U ≈ a\nend","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"We can test the performance of our implementation.","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"A4 = randn(4, 4)\n\nlufact(A4)","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Julia language has a much better implementation in the standard library LinearAlgebra.","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"julia_lures = lu(A4, NoPivot())  # the version we implemented above has no pivot\n\njulia_lures.U\n\ntypeof(julia_lures)\n\nfieldnames(julia_lures |> typeof)","category":"page"},{"location":"chap3/lu/#Pivoting-technique","page":"Solving linear equations by LU factorization: Bottom-up","title":"Pivoting technique","text":"","category":"section"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"note: How to handle small diagonal entries?\nThe above Gaussian elimination process is not stable if any diagonal entry in A has a value that close to zero.small_diagonal_matrix = [1e-8 1; 1 1]\nlures = lufact(small_diagonal_matrix)This issue is can be resolved by permuting the rows of A before factorizing it. For example:lufact(small_diagonal_matrix[end:-1:1, :])This technique is called pivoting.","category":"page"},{"location":"chap3/lu/#Partial-pivoting","page":"Solving linear equations by LU factorization: Bottom-up","title":"Partial pivoting","text":"","category":"section"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"LU factoriazation (or Gaussian elimination) with row pivoting is defined as","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"P A = L U","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"where P is a permutation matrix. Pivoting in Gaussian elimination is the process of selecting a pivot element in a matrix and then using it to eliminate other elements in the same column or row. The pivot element is chosen as the largest absolute value in the column, and its row is swapped with the row containing the current element being eliminated if necessary. This is done to avoid division by zero or numerical instability, and to ensure that the elimination process proceeds smoothly. Pivoting is an important step in Gaussian elimination, as it ensures that the resulting matrix is in reduced row echelon form and that the solution to the system of equations is accurate.","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Let A=(a_ij) be a square matrix of size ntimes n. The Gaussian elimination process with partial pivoting can be represented as","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"M_n-1P_n-1ldots M_2P_2M_1P_1 A = U","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Here we emphsis that P_k and M_jk commute.","category":"page"},{"location":"chap3/lu/#Complete-pivoting","page":"Solving linear equations by LU factorization: Bottom-up","title":"Complete pivoting","text":"","category":"section"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"The complete pivoting also allows permuting columns. The LU factorization with complete pivoting is defined as","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"P A Q = L U","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"Complete pivoting produces better numerical stability but is also harder to implement. In most practical using cases, partial pivoting is good enough.","category":"page"},{"location":"chap3/lu/#Code:-LU-Factoriazation-by-Gaussian-Elimination-with-Partial-Pivoting","page":"Solving linear equations by LU factorization: Bottom-up","title":"Code: LU Factoriazation by Gaussian Elimination with Partial Pivoting","text":"","category":"section"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"A Julia implementation of the Gaussian elimination with partial pivoting is","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"function lufact_pivot!(a::AbstractMatrix)\n    n = size(a, 1)\n    @assert size(a, 2) == n \"size mismatch\"\n    m = zero(a)\n    P = collect(1:n)\n    # loop over columns\n    @inbounds for k=1:n-1\n        # search for pivot in current column\n        val, p = findmax(x->abs(a[x, k]), k:n)\n        p += k-1\n        # find index p such that |a_{pk}| ≥ |a_{ik}| for k ≤ i ≤ n\n        if p != k\n            # swap rows k and p of matrix A\n            for col = 1:n\n                a[k, col], a[p, col] = a[p, col], a[k, col]\n            end\n            # swap rows k and p of matrix M\n            for col = 1:k-1\n                m[k, col], m[p, col] = m[p, col], m[k, col]\n            end\n            P[k], P[p] = P[p], P[k]\n        end\n        if iszero(a[k, k])\n            # skip current column if it's already zero\n            continue\n        end\n        # compute multipliers for current column\n        m[k, k] = 1\n        for i=k+1:n\n            m[i, k] = a[i, k] / a[k, k]\n        end\n        # apply transformation to remaining sub-matrix\n        for j=k+1:n\n            akj = a[k, j]\n            for i=k+1:n\n                a[i,j] -= m[i,k] * akj\n            end\n        end\n    end\n    m[n, n] = 1\n    return m, triu!(a), P\nend\n\n@testset \"lufact with pivot\" begin\n    n = 5\n    A = randn(n, n)\n    L, U, P = lufact_pivot!(copy(A))\n    pmat = zeros(Int, n, n)\n    setindex!.(Ref(pmat), 1, 1:n, P)\n    @test L ≈ lu(A).L\n    @test U ≈ lu(A).U\n    @test pmat * A ≈ L * U\nend","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"The performance of our implementation is as follows.","category":"page"},{"location":"chap3/lu/","page":"Solving linear equations by LU factorization: Bottom-up","title":"Solving linear equations by LU factorization: Bottom-up","text":"julia> using BenchmarkTools\n\njulia> n = 200\n200\n\njulia> A = randn(n, n);\n\njulia> @benchmark lufact_pivot!($A)\nBenchmarkTools.Trial: 7451 samples with 1 evaluation.\n Range (min … max):  621.834 μs …  11.111 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     643.541 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   668.927 μs ± 255.808 μs  ┊ GC (mean ± σ):  0.84% ± 2.57%\n\n     ▂█▂                                                        \n  ▄▄▂███▆▄▄▅▅▅▅▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▂▂ ▃\n  622 μs           Histogram: frequency by time          835 μs <\n\n Memory estimate: 314.31 KiB, allocs estimate: 3.\n\njulia> n = 200\n200\n\njulia> A = randn(n, n);\n\njulia> @benchmark lu($A)\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  247.709 μs …  11.649 ms  ┊ GC (min … max): 0.00% … 96.82%\n Time  (median):     269.583 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   318.077 μs ± 247.482 μs  ┊ GC (mean ± σ):  1.69% ±  2.69%\n\n  ▆██▄▂▃▅▅▄▃▂▂▁ ▁                     ▁▁▁                       ▂\n  ████████████████▇▇▇▆▆▇▆▆▆▆▆▄▆▅▄▄▆▄▇█████▇▆▆▆▆▆▅▆▅▄▄▆▅▄▅▄▅▄▅▅▄ █\n  248 μs        Histogram: log(frequency) by time        835 μs <\n\n Memory estimate: 314.31 KiB, allocs estimate: 3.","category":"page"},{"location":"chap3/tensors/#Tensor-Operations","page":"Tensor Operations","title":"Tensor Operations","text":"","category":"section"},{"location":"chap3/tensors/#Background","page":"Tensor Operations","title":"Background","text":"","category":"section"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Tensor networks serve as a fundamental tool for modeling and analyzing correlated systems. This section reviews the fundamental concepts of tensor networks.","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"A tensor is a mathematical object that generalizes scalars, vectors, and matrices. It can have multiple dimensions and is used to represent data in various mathematical and physical contexts. It is formally defined as follows:","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Definition (Tensor): A tensor T associated to a set of discrete variables V is defined as a function that maps each possible instantiation of the variables in its scope mathcalD_V = prod_vin V mathcalD_v to an element in the set mathcalE, given by","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"T_V prod_v in V mathcalD_v rightarrow mathcalE","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Within the context of probabilistic modeling, the elements in mathcalE are non-negative real numbers, while in other scenarios, they can be of generic types.","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Definition (Tensor Network): A tensor network is a mathematical framework for defining multilinear maps, which can be represented by a triple mathcalN = (Lambda mathcalT V_0), where:","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Lambda is the set of variables present in the network mathcalN.\nmathcalT =  T_V_k _k=1^K is the set of input tensors, where each tensor T_V_k is associated with the labels V_k.\nV_0 specifies the labels of the output tensor.","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Specifically, each tensor T_V_k in mathcalT is labeled by a set of variables V_k subseteq Lambda, where the cardinality V_k equals the rank of T_V_k. The multilinear map, or the \\textbf{contraction}, applied to this triple is defined as","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"T_V_0 = textttcontract(Lambda mathcalT V_0) oversetmathrmdef= sum_m in mathcalD_Lambdasetminus V_0 prod_T_V in mathcalT T_VM=m","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"where M = Lambda setminus V_0. T_VM=m denotes a slicing of the tensor T_V with the variables M fixed to the values m. The summation runs over all possible configurations of the variables in M.","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"For instance, matrix multiplication can be described as the contraction of a tensor network given by","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"(AB)_i k = textttcontractleft(ijk A_i j B_j k i kright)","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"where matrices A and B are input tensors containing the variable sets i j j k, respectively, which are subsets of Lambda = i j k. The output tensor is comprised of variables i k and the summation runs over variables Lambda setminus i k = j. The contraction corresponds to","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"(A B)_i k = sum_j A_ijB_j k","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Diagrammatically, a tensor network can be represented as an open hypergraph, where each tensor is mapped to a vertex and each variable is mapped to a hyperedge. Two vertices are connected by the same hyperedge if and only if they share a common variable. The diagrammatic representation of the matrix multiplication is given as follows: ","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"(Image: )","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Here, we use different colors to denote different hyperedges. Hyperedges for i and k are left open to denote variables of the output tensor. A slightly more complex example of this is the star contraction:","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"textttcontract(ijkl A_i l B_j l C_k l ijk) \n= sum_lA_il B_jl C_kl","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Note that the variable l is shared by all three tensors, making regular edges, which by definition connect two nodes, insufficient for its representation. This motivates the need for hyperedges, which can connect a single variable to any number of nodes. The hypergraph representation is given as:","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"(Image: )","category":"page"},{"location":"chap3/tensors/#Einsum-notation","page":"Tensor Operations","title":"Einsum notation","text":"","category":"section"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"The einsum notation is a compact way to specify tensor contractions with a string. In this notation, an index (subscripts) is represented by a char, and the tensors are represented by the indices. The input tensors and the output tensor are separated by an arrow -> and input tensors are separated by comma ,. For example, the matrix multiplication left(ijk A_i j B_j k i kright) can be concisely written as \"ij,jk->ik\". A general contraction can be defined with pseudocode as follows:","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Let A, B, C, ... be input tensors, O be the output tensor\nfor indices in domain_of_unique_indices(einsum_notation)\n    O[indices in O] += A[indices in A] * B[indices in B] * ...\nend","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"In the following example, we demonstrate the einsum notation for matrix multiplication and other tensor operations.","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"note: Example - Einsum notation\nWe first define the tensors and then demonstrate the einsum notation for various tensor operations.using OMEinsum\ns = fill(1)  # scalar\nw, v = [1, 2], [4, 5];  # vectors\nA, B = [1 2; 3 4], [5 6; 7 8]; # matrices\nT1, T2 = reshape(1:8, 2, 2, 2), reshape(9:16, 2, 2, 2); # 3D tensorUnary examples:ein\"i->\"(w)  # sum of the elements of a vector.\nein\"ij->i\"(A)  # sum of the rows of a matrix.\nein\"ii->\"(A)  # sum of the diagonal elements of a matrix, i.e., the trace.\nein\"ij->\"(A)  # sum of the elements of a matrix.\nein\"i->ii\"(w)  # create a diagonal matrix.\nein\"i->ij\"(w; size_info=Dict('j'=>2))  # repeat a vector to form a matrix.\nein\"ijk->ikj\"(T1)  # permute the dimensions of a tensor.Binary examples:ein\"ij, jk -> ik\"(A, B)  # matrix multiplication.\nein\"ijb,jkb->ikb\"(T1, T2)  # batch matrix multiplication.\nein\"ij,ij->ij\"(A, B)  # element-wise multiplication.\nein\"ij,ij->\"(A, B)  # sum of the element-wise multiplication.\nein\"ij,->ij\"(A, s)  # element-wise multiplication by a scalar.Nary examples:ein\"ai,aj,ak->ijk\"(A, A, B)  # star contraction.\nein\"ia,ajb,bkc,cld,dm->ijklm\"(A, T1, T2, T1, A)  # tensor train contraction.","category":"page"},{"location":"chap3/tensors/#The-spin-glass-problem","page":"Tensor Operations","title":"The spin-glass problem","text":"","category":"section"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"The spin-glass problem is a combinatorial optimization problem that is widely used in physics, computer science, and mathematics. The problem is to find the ground state of a spin-glass Hamiltonian, which is a function of the spin configuration. The Hamiltonian is defined as","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"H(sigma) = -sum_ij J_ij sigma_i sigma_j + sum_i h_i sigma_i","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"where sigma_i in -1 1 is the spin variable, J_ij is the coupling strength between spins i and j, and h_i is the external field acting on spin i. The first term is the interaction energy between spins, and the second term is the energy due to the external field. The ground state is the spin configuration that minimizes the Hamiltonian.","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"<img src=\"../../assets/images/spinglass.png\" width=\"400\" />","category":"page"},{"location":"chap3/tensors/#Partition-function","page":"Tensor Operations","title":"Partition function","text":"","category":"section"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"The thermal equilibrium of the spin-glass system is described by the Boltzmann distribution","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"P(sigma) = frac1Z e^-beta H(sigma)","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"where beta = 1T is the inverse temperature, and Z is the partition function","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Z = sum_sigma e^-beta H(sigma)","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"The partition function is the normalization constant that ensures the probability distribution sums to one. The partition function is a sum over all possible spin configurations, which makes it computationally intractable for large systems.","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"The partition function can be expressed as a tensor contraction using the einsum notation. The partition function is a sum over all possible spin configurations, which can be represented as a tensor contraction over the spins. The partition function can be written as","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Z = sum_sigma e^-beta H(sigma) = sum_sigma e^beta sum_ij J_ij sigma_i sigma_j + sum_i h_i sigma_i = sum_sigma prod_ij e^beta J_ij sigma_i sigma_j prod_i e^h_i sigma_i","category":"page"},{"location":"chap3/tensors/#Best-configuration","page":"Tensor Operations","title":"Best configuration","text":"","category":"section"},{"location":"chap3/tensors/#Landscape","page":"Tensor Operations","title":"Landscape","text":"","category":"section"},{"location":"chap3/tensors/#The-backward-rule-of-tensor-contraction","page":"Tensor Operations","title":"The backward rule of tensor contraction","text":"","category":"section"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"The backward rule for matrix multiplication is","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"C = ein\"ij,jk->ik\"(A, B)\n̄A = ein\"ik,jk->ij\"(̄C, B)\n̄B = ein\"ik,jk->ij\"(A, ̄C)\nv = ein\"ii->i\"(A)\n̄A = ein\"?\"(̄v)","category":"page"},{"location":"chap3/tensors/#Probability-graph","page":"Tensor Operations","title":"Probability graph","text":"","category":"section"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"Random variable Meaning\nA Recent trip to Asia\nT Patient has tuberculosis\nS Patient is a smoker\nL Patient has lung cancer\nB Patient has bronchitis\nE Patient hast T and/or L\nX Chest X-Ray is positive\nD Patient has dyspnoea","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"A probabilistic graphical model (PGM) illustrates the mathematical modeling of reasoning in the presence of uncertainty. Bayesian networks (above) and Markov random fields are popular types of PGMs. Consider the Bayesian network shown in the figure above known as the ASIA network. It is a simplified example from the context of medical diagnosis that describes the probabilistic relationships between different random variables corresponding to possible diseases, symptoms, risk factors and test results. It consists of a graph G = (VmathcalE) and a probability distribution P(V) where G is a directed acyclic graph, V is the set of variables and mathcalE is the set of edges connecting the variables. We assume all variables to be discrete (0 or 1). Each variable v in V is quantified with a conditional probability distribution P(v mid pa(v)) where pa(v) are the parents of v. These conditional probability distributions together with the graph G induce a joint probability distribution over P(V), given by","category":"page"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"P(V) = prod_vin V P(v mid pa(v))","category":"page"},{"location":"chap3/tensors/#The-partition-function","page":"Tensor Operations","title":"The partition function","text":"","category":"section"},{"location":"chap3/tensors/","page":"Tensor Operations","title":"Tensor Operations","text":"https://uaicompetition.github.io/uci-2022/competition-entry/tasks/","category":"page"},{"location":"chap3/sensitivity/#Sensitivity-Analysis","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"","category":"section"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"Sensitivity analysis in linear algebra is the study of how changes in the input data or parameters of a linear system affect the output or solution of the system. It is crucial to the reliability and accuracy of numerical algorithms.","category":"page"},{"location":"chap3/sensitivity/#Relative-Error-and-Absolute-Error","page":"Sensitivity Analysis","title":"Relative Error and Absolute Error","text":"","category":"section"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"The relevant error in floating number system is the relative error.","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"Absolute error: x - hat x\nRelative error: fracx - hat xx","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"where cdot is a measure of size.","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"Floating point numbers have almost \"constant\" relative error, which is called the machine epsilon.","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"eps(Float64)","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"eps(1.0)\neps(1e10) / 1e10\neps(1e-10) / 1e-10","category":"page"},{"location":"chap3/sensitivity/#(Relative)-Condition-Number","page":"Sensitivity Analysis","title":"(Relative) Condition Number","text":"","category":"section"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"Condition number is a measure of the sensitivity of a mathematical problem to changes or errors in the input data. It is a way to quantify how much the output of a function or algorithm can vary due to small changes in the input. A high condition number indicates that the problem is ill-conditioned, meaning that small errors in the input can lead to large errors in the output. A low condition number indicates that the problem is well-conditioned, meaning that small errors in the input have little effect on the output.","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"In short, the (relative) condition number of an operation f with input x measures the relative error application power of f with input x, which is formally defined as","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"lim _varepsilon rightarrow 0^+sup _delta xleq varepsilon frac delta f(x)f(x)delta xx","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"note: Quiz: What is the condition number of the following function?\ny = exp(x)\na + b\na - bWith the obtained result, can you explain why subtracting two big floating point numbers should be avoided?","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"note: Quiz: The algorithm matters?\nConsider the quadratic equation x^2 - 2px - q, the roots can be computed by the following two algorithms.p - sqrtp^2 + q\nfrac-qp+sqrtp^2+qPlease explain the difference between the two algorithms.p, q = 12345678, 1\np - sqrt(p^2 + q)  # numerically unstable\n-q/(p + sqrt(p^2 + q))  # numerically stable","category":"page"},{"location":"chap3/sensitivity/#Condition-Number-of-a-Linear-Operator","page":"Sensitivity Analysis","title":"Condition Number of a Linear Operator","text":"","category":"section"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"The condition number of a linear system Ax = b is defined as","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"rm cond(A) = A A^-1","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"where the matrix p-norm is formally defined as","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"A_p = max_xneq 0 fracAx_px_p","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"and the vector p-norm that defined as","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"v_p = left(sum_iv_i^pright)^1p","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"Using the definition of condition number, we have","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"beginalign*\nrm cond(A x)=lim _varepsilon rightarrow 0^+sup _delta xleq varepsilon frac delta (Ax)A xdelta xx\n=lim _varepsilon rightarrow 0^+sup _delta xleq varepsilon frac Adelta xxdelta xAx\nendalign*","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"Let y = Ax, we have","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"beginalign*\nrm cond(A x)=lim _varepsilon rightarrow 0^+sup _delta xleq varepsilon frac Adelta xA^-1ydelta xy\n=AfracA^-1yy\nendalign*","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"Suppose we want to get an upper bound for any input x, then using the definition of matrix norm, we have","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"rm cond(A) = Asup_y fracA^-1yy = A A^-1","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"Considering the fact that A is the maximum singular value of A and A^-1 is the reciprocal of the minimum singular value of A, we have","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"rm cond(A) = fracsigma_max(A)sigma_min(A)","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"where sigma_max(A) and sigma_min(A) are the maximum and minimum singular values of A.","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"An ill conditioned matrix may produce unreliable result, or the output is very sensitive to the input. The following is an example of a matrix close to singular.","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"A = left(beginmatrix\n0913  0659\n0457  0330\nendmatrix\nright)","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"using LinearAlgebra\nicond_matrix = [0.913 0.659; 0.457 0.330]\ncond(icond_matrix)\nspectrum = svd(icond_matrix).S\nmaximum(spectrum)/minimum(spectrum)  # the same as the condition number","category":"page"},{"location":"chap3/sensitivity/","page":"Sensitivity Analysis","title":"Sensitivity Analysis","text":"note: Numeric experiment on condition number\nWe randomly generate matrices of size 10times 10 and show the condition number approximately upper bounds the numeric error amplification factor of a linear equation solver.n = 10000\np = 2\nerrors = zeros(n)\nconds = map(1:n) do k\n    A = rand(10, 10)\n    b = rand(10)\n    dx = A \\ b\n    sx = Float32.(A) \\ Float32.(b)\n    errors[k] = (norm(sx - dx, p)/norm(dx, p)) / (norm(b-Float32.(b), p)/norm(b, p))\n    cond(A, p)\nend\n\n# visualization\nusing CairoMakie\nfig = Figure()\nax = Axis(fig[1, 1], xlabel=\"condition number\", ylabel=\"error amplification factor\", limits=(1, 10000, 1, 10000), xscale=log10, yscale=log10)\nplot!(ax, conds, conds; label=\"condition number\")\nscatter!(ax, conds, errors; label=\"samples\")\nfig","category":"page"},{"location":"chap2/julia-type/#Types-and-Multiple-dispatch","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"","category":"section"},{"location":"chap2/julia-type/#Julia-Types","page":"Types and Multiple-dispatch","title":"Julia Types","text":"","category":"section"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Julia has rich type system, which is not limited to the primitive types that supported by the hardware. The type system is the key to the multiple dispatch feature of Julia.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"As an example, let us consider the type for complex numbers.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Complex{Float64}","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"where Float64 is the type parameter of Complex. Type parameters are a part of a type, without which the type is not fully specified. A fully specified type is called a concrete type, which has a fixed memory layout and can be instantiated in memory. For example, the Complex{Float64} consists of two fields of type Float64, which are the real and imaginary parts of the complex number.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"fieldnames(Complex{Float64})\nfieldtypes(Complex{Float64})","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Extending the example, we can define the type for a matrix of complex numbers.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Array{Complex{Float64}, 2}","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Array type has two type parameters, the first one is the element type and the second one is the dimension of the array.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"One can get the type of value with typeof function.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"typeof(1+2im)\ntypeof(randn(Complex{Float64}, 2, 2))","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Then, what the type of a type?","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"typeof(Complex{Float64})","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"There is a very special type: Tuple, which is different from regular types in the following ways:","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Tuple types may have any number of parameters.\nTuple types are covariant in their parameters: Tuple{Int} is a subtype of Tuple{Any}. Therefore Tuple{Any} is considered an abstract type, and tuple types are only concrete if their parameters are.\nTuples do not have field names; fields are only accessed by index.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"tp = (1, 2.0, 'c')\ntypeof(tp)\ntp[2]","category":"page"},{"location":"chap2/julia-type/#Multiple-dispatch","page":"Types and Multiple-dispatch","title":"Multiple dispatch","text":"","category":"section"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Multiple dispatch is a feature of some programming languages in which a function or method can be dynamically dispatched based on the run-time type. The dispatch is the process of selecting the method to invoke based on the type of the arguments.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"We first define of an abstract type AbstractAnimal with the keyword abstract type:","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"abstract type AbstractAnimal{L} end","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"where the type parameter L stands for the number of legs. Defining the number of legs as a type parameter or a field of a concrete type is a design choice. Providing more information in the type system can help the compiler to optimize the code, but it can also make the compiler generate more code.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Abstract types can have subtypes. In the following we define a concrete subtype type Dog with 4 legs, which is a subtype of AbstractAnimal{4}.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"struct Dog <: AbstractAnimal{4}\n\tcolor::String\nend","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"where <: is the symbol for sybtyping， A <: B means A is a subtype of B. Concrete types can have fields, which are the data members of the type. However, they can not have subtypes.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Similarly, we define a Cat with 4 legs, a Cock with 2 legs and a Human with 2 legs.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"struct Cat <: AbstractAnimal{4}\n\tcolor::String\nend\n\nstruct Cock <: AbstractAnimal{2}\n\tgender::Bool\nend\n\nstruct Human{FT <: Real} <: AbstractAnimal{2}\n\theight::FT\n\tfunction Human(height::T) where T <: Real\n\t\tif height <= 0 || height > 300\n\t\t\terror(\"The tall of a Human being must be in range 0~300, got $(height)\")\n\t\tend\n\t\treturn new{T}(height)\n\tend\nend","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Here, the Human type has its own constructor. The new function is the default constructor.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"We can define a fall back method fight on the abstract type AbstractAnimal","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"fight(a::AbstractAnimal, b::AbstractAnimal) = \"draw\"","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"where :: is a type assertion. This function will be invoked if two subtypes of AbstractAnimal are fed into the function fight and no more explicit methods are defined.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"We can define many more explicit methods with the same name.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"fight(dog::Dog, cat::Cat) = \"win\"\nfight(hum::Human, a::AbstractAnimal) = \"win\"\nfight(hum::Human, a::Union{Dog, Cat}) = \"loss\"\nfight(hum::AbstractAnimal, a::Human) = \"loss\"","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"where Union{Dog, Cat} is a union type. It is a type that can be either Dog or Cat. Union types are not concrete since they do not have a fixed memory layout, meanwhile, they can not be subtyped! Here, we defined 5 methods for the function fight. However, defining too many methods for the same function can be dangerous. You need to be careful about the ambiguity error!","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"fight(Human(170), Human(180))","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"It makes sense because we claim Human wins any other animals, but we also claim any animal losses to Human. When it comes to two Humans, the two functions are equally valid. To resolve the ambiguity, we can define a new method for the function fight as follows.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"fight(hum::Human{T}, hum2::Human{T}) where T<:Real = hum.height > hum2.height ? \"win\" : \"loss\"","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Now, we can test the function fight with different combinations of animals.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"fight(Cock(true), Cat(\"red\"))\nfight(Dog(\"blue\"), Cat(\"white\"))\nfight(Human(180), Cat(\"white\"))\nfight(Human(170), Human(180))","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Quiz: How many method instances are generated for fight so far?","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"using MethodAnalysis\nmethodinstances(fight)","category":"page"},{"location":"chap2/julia-type/#Example:-Julia-number-system","page":"Types and Multiple-dispatch","title":"Example: Julia number system","text":"","category":"section"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"The Julia type system is a tree, and Any is the root of type tree, i.e. it is a super type of any other type. The Number type is the root type of Julia number system, which is also a subtype of Any.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Number <: Any","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"The type tree rooted on Number looks like:","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Number\n├─ Base.MultiplicativeInverses.MultiplicativeInverse{T}\n│  ├─ Base.MultiplicativeInverses.SignedMultiplicativeInverse{T<:Signed}\n│  └─ Base.MultiplicativeInverses.UnsignedMultiplicativeInverse{T<:Unsigned}\n├─ Complex{T<:Real}\n├─ Real\n│  ├─ AbstractFloat\n│  │  ├─ BigFloat\n│  │  ├─ Float16\n│  │  ├─ Float32\n│  │  └─ Float64\n│  ├─ AbstractIrrational\n...","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"There are utilities to analyze the type tree:","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"using InteractiveUtils # hide\nsubtypes(Number)\nsupertype(Float64)\nAbstractFloat <: Real","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"The leaf nodes of the type tree are called concrete types. They are the types that can be instantiated in memory. Among the concrete types, there are primitive types and composite types. Primitive types are built into the language, such as Int64, Float64, Bool, and Char, while composite types are built on top of primitive types, such as Dict, Complex and the user-defined types.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"The list of primitive types","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"primitive type Float16 <: AbstractFloat 16 end\nprimitive type Float32 <: AbstractFloat 32 end\nprimitive type Float64 <: AbstractFloat 64 end\n\nprimitive type Bool <: Integer 8 end\nprimitive type Char <: AbstractChar 32 end\n\nprimitive type Int8    <: Signed   8 end\nprimitive type UInt8   <: Unsigned 8 end\nprimitive type Int16   <: Signed   16 end\nprimitive type UInt16  <: Unsigned 16 end\nprimitive type Int32   <: Signed   32 end\nprimitive type UInt32  <: Unsigned 32 end\nprimitive type Int64   <: Signed   64 end\nprimitive type UInt64  <: Unsigned 64 end\nprimitive type Int128  <: Signed   128 end\nprimitive type UInt128 <: Unsigned 128 end","category":"page"},{"location":"chap2/julia-type/#Extending-the-number-system-a-comparison-with-object-oriented-programming","page":"Types and Multiple-dispatch","title":"Extending the number system - a comparison with object-oriented programming","text":"","category":"section"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Extending the number system in Julia is much easier than in object-oriented languages like Python. In the following example, we show how to implement addition operation of a user defined class in Python (feel free to skip if you do not know Python).","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"class X:\n  def __init__(self, num):\n    self.num = num\n\n  def __add__(self, other_obj):\n    return X(self.num+other_obj.num)\n\n  def __radd__(self, other_obj):\n    return X(other_obj.num + self.num)\n\n  def __str__(self):\n    return \"X = \" + str(self.num)\n\nclass Y:\n  def __init__(self, num):\n    self.num = num\n\n  def __radd__(self, other_obj):\n    return Y(self.num+other_obj.num)\n\n  def __str__(self):\n    return \"Y = \" + str(self.num)\n\nprint(X(3) + Y(5))\n\nprint(Y(3) + X(5))","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Here, we implemented the addition operation of two classes X and Y. The __add__ method is called when the + operator is used with the object on the left-hand side, while the __radd__ method is called when the object is on the right-hand side. The output is as follows:","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"X = 8\nX = 8","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"It turns out the __radd__ method of Y is not called at all. This is because the __radd__ method is only called when the object on the left-hand side does not have the __add__ method by some artifical rules.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Implement addition in Julian style is much easier. We can define the addition operation of two types X and Y as follows.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"struct X{T} <: Number\n\tnum::T\nend\n\nstruct Y{T} <: Number\n\tnum::T\nend\n\nBase.:(+)(a::X, b::Y) = X(a.num + b.num);\n\nBase.:(+)(a::Y, b::X) = X(a.num + b.num);\n\nBase.:(+)(a::X, b::X) = X(a.num + b.num);\n\nBase.:(+)(a::Y, b::Y) = Y(a.num + b.num);","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Multiple dispatch seems to be more expressive than object-oriented programming.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Now, supposed you want to extend this method to a new type Z. In python, he needs to define a new class Z as follows.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"class Z:\n  def __init__(self, num):\n    self.num = num\n\n  def __add__(self, other_obj):\n    return Z(self.num+other_obj.num)\n\n  def __radd__(self, other_obj):\n    return Z(other_obj.num + self.num)\n\n  def __str__(self):\n    return \"Z = \" + str(self.num)\n\nprint(X(3) + Z(5))\n\nprint(Z(3) + X(5))","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"The output is as follows:","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"X = 8\nZ = 8","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"No matter how hard you try, you can not make the __add__ method of Z to be called when the object is on the left-hand side. In Julia, this is not a problem at all. We can define the addition operation of Z as follows.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"struct Z{T} <: Number\n    num::T\nend\nBase.:(+)(a::X, b::Z) = Z(a.num + b.num);\nBase.:(+)(a::Z, b::X) = Z(a.num + b.num);\nBase.:(+)(a::Y, b::Z) = Z(a.num + b.num);\nBase.:(+)(a::Z, b::Y) = Z(a.num + b.num);\nBase.:(+)(a::Z, b::Z) = Z(a.num + b.num);\nX(3) + Y(5)\nY(3) + X(5)\nX(3) + Z(5)\nZ(3) + Y(5)","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"There is a deeper reason why multiple dispatch is more expressive than object-oriented programming. The Julia function space is exponentially large! If a function f has k parameters, and the module has t types, there can be t^k methods for the function f:","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"f(x::T1, y::T2, z::T3...)","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Exponential function space allows us to specify the behavior of a function in a very fine-grained way. However, in an object-oriented language like Python, the function space is only linear to the number of classes.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"class T1:\n    def f(self, y, z, ...):\n        self.num = num\n","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"The behavior of method f is completely determined by the first argument self, which means object-oriented programming is equivalent to single dispatch.","category":"page"},{"location":"chap2/julia-type/#Example:-Computing-Fibonacci-number-at-compile-time","page":"Types and Multiple-dispatch","title":"Example: Computing Fibonacci number at compile time","text":"","category":"section"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"The Fibonacci number has a recursive definition:","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"using BenchmarkTools\nfib(x::Int) = x <= 2 ? 1 : fib(x-1) + fib(x-2)\naddup(x::Int, y::Int) = x + y","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"julia> @btime fib(40)\n  278.066 ms (0 allocations: 0 bytes)\n102334155","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Oops, it is really slow. There is definitely a better way to calculate the Fibonacci number, but let us stick to this recursive implementation for now.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"If you know the Julia type system, you can implement the Fibonacci number in a zero cost way. The trick is to use the type system to calculate the Fibonacci number at compile time. There is a type Val defined in the Base module, which is just a type with a type parameter. The type parameter can be a number:","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Val(3.0)","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"We can define the addition operation of Val as the addition of the type parameters.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"addup(::Val{x}, ::Val{y}) where {x, y} = Val(x + y)\naddup(Val(5), Val(7))","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Finally, we can define the Fibonacci number in a zero cost way.","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"fib(::Val{x}) where x = x <= 2 ? Val(1) : addup(fib(Val(x-1)), fib(Val(x-2)))","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"julia> @btime fib(Val(40))\n  0.792 ns (0 allocations: 0 bytes)\nVal{102334155}()","category":"page"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Wow, it computes in no time! However, this trick is not recommended in the Julia performance tips. This implementation simply transfers the run-time computation to the compile time. On the other hand, we find the compiling time of the function fib is much shorter than the run-time. The recursive form turns out to be optimized away by the Julia compiler. But still, it is not recommended to abuse the type system.","category":"page"},{"location":"chap2/julia-type/#Summary","page":"Types and Multiple-dispatch","title":"Summary","text":"","category":"section"},{"location":"chap2/julia-type/","page":"Types and Multiple-dispatch","title":"Types and Multiple-dispatch","text":"Multiple dispatch is a feature of some programming languages in which a function or method can be dynamically dispatched based on the run-time type.\nJulia's multiple dispatch provides exponential large function space, which allows extending the number system easily.","category":"page"},{"location":"chap7/hpc/#MPI-and-OpenMP","page":"MPI and OpenMP","title":"MPI and OpenMP","text":"","category":"section"},{"location":"chap4/ad/#Automatic-Differentiation","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Automatic differentiation[Griewank2008] is a technique to compute the derivative of a function automatically. It is a powerful tool for scientific computing, machine learning, and optimization. The automatic differentiation can be classified into two types: forward mode and backward mode. The forward mode AD computes the derivative of a function with respect to many inputs, while the backward mode AD computes the derivative of a function with respect to many outputs. The forward mode AD is efficient when the number of inputs is small, while the backward mode AD is efficient when the number of outputs is small.","category":"page"},{"location":"chap4/ad/#A-brief-history-of-autodiff","page":"Automatic Differentiation","title":"A brief history of autodiff","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"1964 (forward mode AD) ~ Robert Edwin Wengert, A simple automatic derivative evaluation program.\n1970 (backward mode AD) ~ Seppo Linnainmaa, Taylor expansion of the accumulated rounding error.\n1986 (AD for machine learning) ~ Rumelhart, D. E., Hinton, G. E., and Williams, R. J., Learning representations by back-propagating errors.\n1992 (optimal checkpointing) ~ Andreas Griewank, Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation.\n2000s ~ The boom of tensor based AD frameworks for machine learning.\n2018 ~ Re-inventing AD as differential programming (wiki.) (Image: )\n2020 (AD on LLVM) ~ Moses, William and Churavy, Valentin, Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients.","category":"page"},{"location":"chap4/ad/#Differentiating-the-Bessel-function","page":"Automatic Differentiation","title":"Differentiating the Bessel function","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"J_nu(z) = sumlimits_n=0^infty frac(z2)^nuGamma(k+1)Gamma(k+nu+1) (-z^24)^n","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"A poorman's implementation of this Bessel function is as follows","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"function poor_besselj(ν, z::T; atol=eps(T)) where T\n    k = 0\n    s = (z/2)^ν / factorial(ν)\n    out = s\n    while abs(s) > atol\n        k += 1\n        s *= (-1) / k / (k+ν) * (z/2)^2\n        out += s\n    end\n    out\nend","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Let us plot the Bessel function","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"using CairoMakie\n\nx = 0.0:0.01:10\nfig = Figure()\nax = Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"J(ν, x)\")\nfor i=0:5\n    yi = poor_besselj.(i, x)\n    lines!(ax, x, yi; label=\"J(ν=$i)\", linewidth=2)\nend\nfig","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"The derivative of the Bessel function is","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"fracd J_nu(z)dz = fracJ_nu-1(z) - J_nu+1(z) 2","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"In the following code, we compute the gradient of the Bessel function with respect to nu=2 using different methods.","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"using FiniteDifferences: central_fdm\nusing Enzyme\n\nν = 2\nyi = poor_besselj.(ν, x)\n\ng_f = [Enzyme.autodiff(Enzyme.Forward, poor_besselj, ν, Enzyme.Duplicated(xi, 1.0))[1] for xi in x] # forward mode\ng_m = (poor_besselj.(ν-1, x) - poor_besselj.(ν+1, x)) ./ 2 # manual\ng_b = [Enzyme.autodiff(Enzyme.Reverse, poor_besselj, ν, Enzyme.Active(xi))[1][2] for xi in x]\ng_c = central_fdm(5, 1).(z->poor_besselj(ν, z), x) # central finite difference","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Here, the forward and backward mode AD are implemented using the Enzyme package. The manual method is the direct application of the derivative formula. The central finite difference is computed using the central_fdm function from the FiniteDifferences package.","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"fig = Figure()\nax = Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"y\")\nlines!(ax, x, yi; label=\"J(ν=$ν, x)\", linewidth=2)\nlines!(ax, x, g_b; label=\"g(ν=$ν, x)\", linewidth=2, linestyle=:dash)\naxislegend(ax)\nfig","category":"page"},{"location":"chap4/ad/#Finite-difference","page":"Automatic Differentiation","title":"Finite difference","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"The finite difference is a numerical method to approximate the derivative of a function. The finite difference is a simple and efficient method to compute the derivative of a function. The finite difference can be classified into three types: forward, backward, and central.","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"For example, the first order forward difference is","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"fracpartial fpartial x approx fracf(x+Delta) - f(x)Delta","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"The first order backward difference is","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"fracpartial fpartial x approx fracf(x) - f(x-Delta)Delta","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"The first order central difference is","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"fracpartial fpartial x approx fracf(x+Delta) - f(x-Delta)2Delta","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Among these three methods, the central difference is the most accurate. It has an error of O(Delta^2), while the forward and backward differences have an error of O(Delta).","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Higher order finite differences can be found in the wiki page.","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"note: Example: central finite difference to the 4th order\nThe coefficients of the central finite difference to the 4th order are-2 -1 0 1 2\n1/12 −2/3 0 2/3 −1/12The induced formula isfracpartial fpartial x approx fracf(x-2Delta) - 8f(x-Delta) + 8f(x+Delta) - f(x+2Delta)12DeltaIn the following, we will derive this formula using the Taylor expansion.left(beginmatrix\nf(x-2Delta)f(x-Delta)f(x)f(x+Delta)f(x+2Delta)\nendmatrixright) approx left(beginmatrix\n1  (-2)^1  (-2)^2  (-2)^3    (-2)^4\n1  (-1)^1  (-1)^2  (-1)^3    (-1)^4\n1  0  0  0    0\n1  (1)^1  (1)^2  (1)^3    (1)^4\n1  (2)^1  (2)^2  (2)^3    (2)^4\nendmatrixright)left(beginmatrix\nf(x)f(x)Deltaf(x)Delta^22f(x)Delta^36f(x)Delta^424\nendmatrixright)Let us denote the matrix on the right-hand side as A. Then we want to find the coefficients vec alpha = (alpha_-2 alpha_-1 alpha_0 alpha_1 alpha_2)^T such thatbeginalign*\nalpha_-2f(x-2Delta) + alpha_-1f(x-Delta) + alpha_0f(x) + alpha_1f(x+Delta) + alpha_2f(x+2Delta)\n = f(x)Delta + O(Delta^5)\nendalign*which can be computed by solving the linear systemA vec alpha = (0 1 0 0 0)^TThe following code demonstrates the central finite difference to the 4th order.b = [0.0, 1, 0, 0, 0]\nA = [i^j for i=-2:2, j=0:4]\nA' \\ b  # the central_fdm(5, 1) coefficients\ncentral_fdm(5, 1)(x->poor_besselj(2, x), 0.5)julia> using BenchmarkTools\n\njulia> @benchmark central_fdm(5, 1)(y->poor_besselj(2, y), x) setup=(x=0.5)\nBenchmarkTools.Trial: 10000 samples with 9 evaluations.\nRange (min … max):  2.588 μs … 434.102 μs  ┊ GC (min … max): 0.00% … 98.68%\nTime  (median):     2.708 μs               ┊ GC (median):    0.00%\nTime  (mean ± σ):   2.832 μs ±   5.422 μs  ┊ GC (mean ± σ):  3.49% ±  1.96%\n\n▁▂▅▆▇██▆▅▄▂▁                                               ▂\n▇███████████████▆▆▆▅▄▅▅▄▆▄▅▇██▆▆▆▄▆▆▆▆▅▆▄▄▄▄▃▄▄▄▃▁▄▄▄▁▁▄▁▃▄ █\n2.59 μs      Histogram: log(frequency) by time      3.62 μs <\n\nMemory estimate: 2.47 KiB, allocs estimate: 36.","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"The central finite difference can be generalized to the nth order. The nth order central finite difference has an error of O(Delta^n+1).","category":"page"},{"location":"chap4/ad/#Forward-mode-automatic-differentiation","page":"Automatic Differentiation","title":"Forward mode automatic differentiation","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Forward mode AD attaches a infitesimal number epsilon to a variable, when applying a function f, it does the following transformation","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"beginalign\n    f(x+g epsilon) = f(x) + f(x) gepsilon + mathcalO(epsilon^2)\nendalign","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"The higher order infinitesimal is ignored. ","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"In the program, we can define a dual number with two fields, just like a complex number","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"f((x, g)) = (f(x), f'(x)*g)","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"using ForwardDiff\nres = sin(ForwardDiff.Dual(π/4, 2.0))\nres === ForwardDiff.Dual(sin(π/4), cos(π/4)*2.0)","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"We can apply this transformation consecutively, it reflects the chain rule.","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"beginalign\nfracpartial vec y_i+1partial x = boxedfracpartial vec y_i+1partial vec y_ifracpartial vec y_ipartial x\ntextlocal Jacobian\nendalign","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Example: Computing two gradients fracpartial zsin xpartial x and fracpartial sin^2xpartial x at one sweep","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"julia> autodiff(Forward, poor_besselj, 2, Duplicated(0.5, 1.0))[1]\n0.11985236384014333\n\njulia> @benchmark autodiff(Forward, poor_besselj, 2, Duplicated(x, 1.0))[1] setup=(x=0.5)\nBenchmarkTools.Trial: 10000 samples with 996 evaluations.\n Range (min … max):  22.256 ns … 66.349 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     23.050 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   23.290 ns ±  1.986 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n  ▅▅  █▆▂▂▄▂▂▂▁                                               ▁\n  ██▅▇██████████▅▅▄▅▅▅▅▃▄▄▄▅▆▅▅▃▄▃▄▅▄▅▄▆▅▅▄▃▂▄▃▃▂▃▄▃▄▄▃▂▂▃▂▃▃ █\n  22.3 ns      Histogram: log(frequency) by time      31.8 ns <\n\n Memory estimate: 0 bytes, allocs estimate: 0.","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"The computing time grows linearly as the number of variables that we want to differentiate. But does not grow significantly with the number of outputs.","category":"page"},{"location":"chap4/ad/#Reverse-mode-automatic-differentiation","page":"Automatic Differentiation","title":"Reverse mode automatic differentiation","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"On the other side, the back-propagation can differentiate many inputs with respect to a single output efficiently","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"beginalign\n    fracpartial mathcalLpartial vec y_i = fracpartial mathcalLpartial vec y_i+1boxedfracpartial vec y_i+1partial vec y_i\ntextlocal jacobian\nendalign","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"julia> autodiff(Enzyme.Reverse, poor_besselj, 2, Enzyme.Active(0.5))[1]\n(nothing, 0.11985236384014332)\n\njulia> @benchmark autodiff(Enzyme.Reverse, poor_besselj, 2, Enzyme.Active(x))[1] setup=(x=0.5)\nBenchmarkTools.Trial: 10000 samples with 685 evaluations.\n Range (min … max):  182.482 ns … 503.771 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     208.880 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   210.059 ns ±  17.016 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n  ▃                 ▁▁█▃                                        \n  █▂▁▂▃▁▁▁▁▂▂▁▁▁▁▁▁▃████▄▃▄▄▃▂▂▃▂▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▂\n  182 ns           Histogram: frequency by time          260 ns <\n\n Memory estimate: 0 bytes, allocs estimate: 0.","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Computing local Jacobian directly can be expensive. In practice, we can use the back-propagation rules to update the adjoint of the variables directly. It requires the forward pass storing the intermediate variables.","category":"page"},{"location":"chap4/ad/#Rule-based-AD-and-source-code-transformation","page":"Automatic Differentiation","title":"Rule based AD and source code transformation","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Rule based AD is a technique to define the backward rules of the functions. The backward rules are the derivatives of the functions with respect to the inputs. The backward rules are crucial for the reverse mode AD. For example, the backward rule of the Bessel function is","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"beginalign\nJ_nu(z) =  fracJ_nu-1(z) - J_nu+1(z) 2\nJ_0(z) =  - J_1(z)\nendalign","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"julia> 0.5 * (poor_besselj(1, 0.5) - poor_besselj(3, 0.5))\n0.11985236384014333\n\n\njulia> @benchmark 0.5 * (poor_besselj(1, x) - poor_besselj(3, x)) setup=(x=0.5)\nBenchmarkTools.Trial: 10000 samples with 998 evaluations.\n Range (min … max):  17.576 ns … 56.947 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     17.702 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   17.999 ns ±  1.796 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n  ▇█▃ ▁     ▁                                                 ▁\n  ███▆██▆▃▅▆█▆▆▅▅▄▅▆▄▄▃▄▂▄▂▄▃▄▃▄▄▄▄▅▄▃▃▅▃▅▅▆▅▅▂▄▄▂▅▄▅▃▅▄▄▅▅▆▆ █\n  17.6 ns      Histogram: log(frequency) by time      24.6 ns <\n\n Memory estimate: 0 bytes, allocs estimate: 0.","category":"page"},{"location":"chap4/ad/#Rule-based-or-not?","page":"Automatic Differentiation","title":"Rule based or not?","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"<table>\n<tr>\n<th width=200></th>\n<th width=300>Rule based</th>\n<th width=300>Source code transformation</th>\n</tr>\n<tr style=\"vertical-align:top\">\n<td></td>\n<td>defining backward rules manully for functions on tensors</td>\n<td>defining backward rules on a limited set of basic scalar operations, and generate gradient code using source code transformation</td>\n</tr>\n<tr style=\"vertical-align:top\">\n<td>pros and cons</td>\n<td>\n<ol>\n<li style=\"color:green\">Good tensor performance</li>\n<li style=\"color:green\">Mature machine learning ecosystem</li>\n</ol>\n</td>\n<td>\n<ol>\n<li style=\"color:green\">Reasonalbe scalar performance</li>\n<li style=\"color:red\">Automatically generated backward rules</li>\n</ol>\n</td>\n<td>\n</td>\n</tr>\n<tr style=\"vertical-align:top\">\n<td>packages</td>\n<td><a href=\"https://jax.readthedocs.io/en/latest/\">JAX</a><br>\n<a href=\"https://pytorch.org/\">PyTorch</a>\n</td>\n<td><a href=\"http://tapenade.inria.fr:8080/tapenade/\">Tapenade</a><br>\n<a href=\"http://www.met.reading.ac.uk/clouds/adept/\">Adept</a><br>\n<a href=\"https://github.com/EnzymeAD/Enzyme\">Enzyme</a>\n</td>\n</tr>\n</table>","category":"page"},{"location":"chap4/ad/#Deriving-the-backward-rules-for-linear-algebra","page":"Automatic Differentiation","title":"Deriving the backward rules for linear algebra","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Please check blog","category":"page"},{"location":"chap4/ad/#Matrix-multiplication","page":"Automatic Differentiation","title":"Matrix multiplication","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Let mathcalT be a stack, and x rightarrow mathcalT and xleftarrow mathcalT be the operation of pushing and poping an element from this stack. Given A in R^ltimes m and Bin R^mtimes n, the forward pass computation of matrix multiplication is","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"beginalign\nC = A B\nA rightarrow mathcalT\nB rightarrow mathcalT\nldots\nendalign","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Let the adjoint of x be overlinex = fracpartial mathcalLpartial x, where mathcalL is a real loss as the final output. The backward pass computes","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"beginalign\nldots\nB leftarrow mathcalT\noverlineA = overlineCB\nA leftarrow mathcalT\noverlineB = AoverlineC\nendalign","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"The rules to compute overlineA and overlineB are called the backward rules for matrix multiplication. They are crucial for rule based automatic differentiation.","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Let us introduce a small perturbation delta A on A and delta B on B,","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"delta C = delta A B + A delta B","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"delta mathcalL = rm tr(delta C^T overlineC) = \nrm tr(delta A^T overlineA) + rm tr(delta B^T overlineB)","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"It is easy to see","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"delta L = rm tr((delta A B)^T overline C) + rm tr((A delta B)^T overline C) = \nrm tr(delta A^T overlineA) + rm tr(delta B^T overlineB)","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"We have the backward rules for matrix multiplication as","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"beginalign\noverlineA = overlineCB^T\noverlineB = A^ToverlineC\nendalign","category":"page"},{"location":"chap4/ad/#Eigen-decomposition","page":"Automatic Differentiation","title":"Eigen decomposition","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Ref: https://arxiv.org/abs/1710.08717","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Given a symmetric matrix A, the eigen decomposition is","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"A = UEU^dagger","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"We have","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"overlineA = UleftoverlineE + frac12left(overlineU^dagger U circ F + hcright)rightU^dagger","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Where F_ij=(E_j- E_i)^-1.","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"If E is continuous, we define the density rho(E) = sumlimits_k delta(E-E_k)=-frac1piint_k ImG^r(E k)  (check sign) Where G^r(E k) = frac1E-E_k+idelta.","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"We have","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"overlineA = UleftoverlineE + frac12left(overlineU^dagger U circ Re G(E_i E_j) + hcright)rightU^dagger","category":"page"},{"location":"chap4/ad/#Obtaining-Hessian","page":"Automatic Differentiation","title":"Obtaining Hessian","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"The second order gradient, Hessian, is also recognized as the Jacobian of the gradient. In practice, we can compute the Hessian by differentiating the gradient function with forward mode AD, which is also known as the forward-over-reverse mode AD.","category":"page"},{"location":"chap4/ad/#Optimal-checkpointing","page":"Automatic Differentiation","title":"Optimal checkpointing","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"The main drawback of the reverse mode AD is the memory usage. The memory usage of the reverse mode AD is proportional to the number of intermediate variables, which scales linearly with the number of operations. The optimal checkpointing[Griewank2008] is a technique to reduce the memory usage of the reverse mode AD. It is a trade-off between the memory and the computational cost. The optimal checkpointing is a step towards solving the memory wall problem","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"Given the binomial function eta(tau delta) = frac(tau + delta)taudelta, show that the following statement is true.","category":"page"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"eta(taudelta) = sum_k=0^delta eta(tau-1k)","category":"page"},{"location":"chap4/ad/#References","page":"Automatic Differentiation","title":"References","text":"","category":"section"},{"location":"chap4/ad/","page":"Automatic Differentiation","title":"Automatic Differentiation","text":"[Griewank2008]: Griewank A, Walther A. Evaluating derivatives: principles and techniques of algorithmic differentiation. Society for industrial and applied mathematics, 2008.","category":"page"},{"location":"chap3/eigen/#Eigenvalue/Singular-value-decomposition-problem","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"","category":"section"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"Ax = lambda x","category":"page"},{"location":"chap3/eigen/#Power-method","page":"Eigenvalue/Singular value decomposition problem","title":"Power method","text":"","category":"section"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"matsize = 10\n\nA10 = randn(matsize, matsize); A10 += A10'\n\neigen(A10).values\n\nvmax = eigen(A10).vectors[:,end]\n\nlet\n    x = normalize!(randn(matsize))\n    for i=1:20\n        x = A10 * x\n        normalize!(x)\n    end\n    1-abs2(x' * vmax)\nend","category":"page"},{"location":"chap3/eigen/#Rayleigh-Quotient-Iteration","page":"Eigenvalue/Singular value decomposition problem","title":"Rayleigh Quotient Iteration","text":"","category":"section"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"let\n    x = normalize!(randn(matsize))\n    U = eigen(A10).vectors\n    for k=1:5\n        sigma = x' * A10 * x\n        y = (A10 - sigma * I) \\ x\n        x = normalize!(y)\n    end\n    (x' * U)'\nend","category":"page"},{"location":"chap3/eigen/#Symmetric-QR-decomposition","page":"Eigenvalue/Singular value decomposition problem","title":"Symmetric QR decomposition","text":"","category":"section"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"function householder_trid!(Q, a)\n    m, n = size(a)\n    @assert m==n && size(Q, 2) == n\n    if m == 2\n        return Q, a\n    else\n        # apply householder matrix\n        H = householder_e1(view(a, 2:n, 1))\n        left_mul!(view(a, 2:n, :), H)\n        right_mul!(view(a, :, 2:n), H')\n        # update Q matrix\n        right_mul!(view(Q, :, 2:n), H')\n        # recurse\n        householder_trid!(view(Q, :, 2:n), view(a, 2:m, 2:n))\n    end\n    return Q, a\nend\n\n@testset \"householder tridiagonal\" begin\n    n = 5\n    a = randn(n, n)\n    a = a + a'\n    Q = Matrix{Float64}(I, n, n)\n    Q, T = householder_trid!(Q, copy(a))\n    @test Q * T * Q' ≈ a\nend","category":"page"},{"location":"chap3/eigen/#The-SVD-algorithm","page":"Eigenvalue/Singular value decomposition problem","title":"The SVD algorithm","text":"","category":"section"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"A = U S V^T","category":"page"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"Form C = A^T A,\nUse the symmetric QR algorithm to compute V_1^T C V_1 = rm diag(sigma_i^2),\nApply QR with column pivoting to AV_1 obtaining U^T(AV_1)Pi = R.","category":"page"},{"location":"chap3/eigen/#Assignments","page":"Eigenvalue/Singular value decomposition problem","title":"Assignments","text":"","category":"section"},{"location":"chap3/eigen/#1.-Review","page":"Eigenvalue/Singular value decomposition problem","title":"1. Review","text":"","category":"section"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"Suppose that you are computing the QR factorization of the matrix","category":"page"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"A = left(beginmatrix\n1  1  1\n1  2  4\n1  3  9\n1  4  16\nendmatrixright)","category":"page"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"by Householder transformations.","category":"page"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"Problems:\nHow many Householder transformations are required?\nWhat does the first column of A become as a result of applying the first Householder transformation?\nWhat does the first column of A become as a result of applying the second Householder transformation?\nHow many Givens rotations would be required to computing the QR factoriazation of A?","category":"page"},{"location":"chap3/eigen/#2.-Coding","page":"Eigenvalue/Singular value decomposition problem","title":"2. Coding","text":"","category":"section"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"Computing the QR decomposition of a symmetric triangular matrix with Givens rotation. Try to minimize the computing time and estimate the number of FLOPS.","category":"page"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"For example, if the input matrix size is T in mathbbR^5times 5","category":"page"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"T = left(beginmatrix\nt_11  t_12  0  0  0\nt_21  t_22  t_23  0  0\n0  t_32  t_33  t_34  0\n0  0  t_43  t_44  t_45\n0  0  0  t_54  t_55\nendmatrixright)","category":"page"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"where t_ij = t_ji.","category":"page"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"In your algorithm, you should first apply Givens rotation on row 1 and 2.","category":"page"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"G(t_11 t_21) T = left(beginmatrix\nt_11  t_12  t_13  0  0\n0  t_22  t_23  0  0\n0  t_32  t_33  t_34  0\n0  0  t_43  t_44  t_45\n0  0  0  t_54  t_55\nendmatrixright)","category":"page"},{"location":"chap3/eigen/","page":"Eigenvalue/Singular value decomposition problem","title":"Eigenvalue/Singular value decomposition problem","text":"Then apply G(t_22 t_32) et al.","category":"page"},{"location":"chap2/julia-why/#Why-Julia?","page":"Why Julia?","title":"Why Julia?","text":"","category":"section"},{"location":"chap2/julia-why/#What-is-Julia-programming-language?","page":"Why Julia?","title":"What is Julia programming language?","text":"","category":"section"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Julia is a modern, open-source, high performance programming language for technical computing. It was born in 2012 in MIT, now is maintained by JuliaHub Inc. located in Boston, US.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Unlike MatLab, Julia is open-source. Julia source code is maintained on GitHub repo JuliaLang/julia, and it open-source LICENSE is MIT. Julia packages can be found on JuliaHub, most of them are open-source.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Unlike Python, Julia is designed for high performance (arXiv:1209.5145). It is a dynamic programming language, but it is as fast as C/C++. The following figure shows the computing time of multiple programming languages normalized to C/C++.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"<img src=\"../../assets/images/benchmark.png\" alt=\"image\" width=\"600\" height=\"auto\">","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Unlike C/C++ and Fortran, Julia is easy to use and is becoming a trend in scientific computing. Julia uses the just-int-time (JIT) technique to achieve high performance, which does not have the problem of platform dependency. Many famous scientists and engineers have switched to Julia from other programming languages.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Steven G. Johnson, creater of FFTW, switched from C++ to Julia years ago.\nAnders Sandvik, creater of Stochastic Series Expansion (SSE) quantum Monte Carlo method, switched from Fortran to Julia recently.\nCourse link: Computational Physics\nMiles Stoudenmire, creater of ITensor, switched from C++ to Julia years ago.\nJutho Haegeman, Chris Rackauckas and more.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"note: Should I switch to Julia?\nBefore switching to Julia, please make sure:the problem you are trying to solve runs more than 10min.\nyou are not satisfied by any existing tools.","category":"page"},{"location":"chap2/julia-why/#My-first-program:-Factorial","page":"Why Julia?","title":"My first program: Factorial","text":"","category":"section"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Before we start, please make sure you have the needed packages installed. Type ] in the Julia REPL to enter the package manager, and then type","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"pkg> add BenchmarkTools, MethodAnalysis","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Go back to the REPL by pressing Backspace.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"julia> function jlfactorial(n)\n           x = 1\n           for i in 1:n\n               x = x * i\n           end\n           return x\n       end\njlfactorial (generic function with 1 method)","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"To make sure the performance is measured correctly, we use the @btime macro in the BenchmarkTools package to measure the performance of the function.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"julia> using BenchmarkTools\n\njulia> @btime jlfactorial(x) setup=(x=5)\n2.208 ns (0 allocations: 0 bytes)\n120","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"CPU clock cycle is ~0.3ns, so it takes only a few clock cycles to compute the factorial of 5. Julia is really fast!","category":"page"},{"location":"chap2/julia-why/#Compare-with-the-speed-of-C-program","page":"Why Julia?","title":"Compare with the speed of C program","text":"","category":"section"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"To measure the performance of the C program, we can utilize the benchmark utilities in Julia. Benchmarking C program with Julia is accurate because Julia has perfect interoperability with C, which allows zero-cost calling of C functions.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"In the following example, we first write a C program to calculate the factorial of a number. The file is named demo.c, and the content is as follows:","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"$ cat demo.c\n#include <stddef.h>\nint c_factorial(size_t n) {\n\tint s = 1;\n\tfor (size_t i=1; i<=n; i++) {\n\t\ts *= i;\n\t}\n\treturn s;\n}","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"To execute a C program in Julia, one needs to compile it to a shared library.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"$ gcc demo.c -fPIC -O3 -shared -o demo.so","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"To call the function in Julia, one can use the @ccall macro in the Libdl package (learn more). Please open a Julia REPL and execute the following code:","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"julia> using Libdl\n\njulia> c_factorial(x) = Libdl.@ccall \"./demo.so\".c_factorial(x::Csize_t)::Int","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"The benchmark result is as follows:","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"julia> using BenchmarkTools\n\njulia> @benchmark c_factorial(5)\nBenchmarkTools.Trial: 10000 samples with 1000 evaluations.\n Range (min … max):  7.333 ns … 47.375 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     7.458 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   7.764 ns ±  1.620 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n  ██▅  ▃▁ ▂▂                         ▁▁▁                     ▂\n  ███▆▄██▆███▅▅▆▆▆▅▆▅▄▅▆▅▅▇▆▆▄▅▅▇█▇▆▆█████▅▃▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▃ █\n  7.33 ns      Histogram: log(frequency) by time     12.6 ns <\n\n Memory estimate: 0 bytes, allocs estimate: 0.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Although the C program requires the type of variables to be manually declared, its performance is very good. The computing time is only 7.33 ns.","category":"page"},{"location":"chap2/julia-why/#Compare-with-the-speed-of-Python-program","page":"Why Julia?","title":"Compare with the speed of Python program","text":"","category":"section"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"We use the timeit module in ipython to measure the performance of the Python program.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"In [5]: def factorial(n):\n...:        x = 1\n...:        for i in range(1, n+1):\n...:            x = x * i\n...:        return x\n...:\n\nIn [6]: factorial(5)\nOut[6]: 120\n\nIn [7]: timeit factorial(5)\n144 ns ± 0.379 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n\nIn [8]: factorial(100)\nOut[8]: 93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"One can also use the PyCall package to call the Python function in Julia.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"The computing time of the Python program is 144 ns, which is 20 times slower than the C program and 70 times slower than the Julia program. On the other hand, the python program is more flexible since its integer type is not limited by the machine word size.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"julia> typemax(Int)\n9223372036854775807\n\njulia> jlfactorial(100)\n0","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"The reason why python is slow and flexible are the same. In python the type of a variable is not declared when it is defined, and it can be changed at any time. This is why the integer type becomes an arbitrary precision integer type when the number is too large. If a variable does not have a fixed type, the program can not preallocate memory for it due to the lack of size information. Then a dynamic typed language has to use a tuple (type, *data) to represent an object, where type is the type of the object and *data is the pointer to the data. Pointing to a random memory location is slow, because it violates the principle of data locality. Lacking of data locality causes the frequent cache miss - failure to find the data in the L1, L2, or L3 cache. Loading data from the main memory is slow, because of the long latency of reading the main memory.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"<img src=\"../../assets/images/data.png\" alt=\"image\" width=\"300\" height=\"auto\">","category":"page"},{"location":"chap2/julia-why/#Combining-Python-and-C/C?","page":"Why Julia?","title":"Combining Python and C/C++?","text":"","category":"section"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"From the maintainer's perspective, it is hard to maintain a program written in both Python and C/C++:","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"It makes the build configuration files complicated.\nLearning two programming languages is hard for new contributors.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Using python as glue is not as powerful as it looks, the following problem can not be solved by this approach:","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Monte Carlo simulation.\nBranching and bound algorithms.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"<img src=\"../../assets/images/pythonc.png\" alt=\"image\" width=\"400\" height=\"auto\"/>","category":"page"},{"location":"chap2/julia-why/#Key-ingredients-of-Julia's-performance:-Just-in-time-(JIT)-compilation","page":"Why Julia?","title":"Key ingredients of Julia's performance: Just in time (JIT) compilation","text":"","category":"section"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"JIT compilation: compile the method to a method instance when a method is called for the first time;\nMultiple dispatch: invoke the correct method instance according to the type of multiple arguments;","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"(Image: )","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Given a user defined Julia function, the Julia compiler will generate a binary for it at the first called. This binary is called a method instance, and it is generated based on the input types of the function. The method instance is then stored in the method table, and it will be called when the function is called with the same input types. The method instance is generated by the LLVM compiler, and it is optimized for the input types. The method instance is a binary, and it is as fast as a C/C++ program.","category":"page"},{"location":"chap2/julia-why/#Step-1:-Infer-the-types","page":"Why Julia?","title":"Step 1: Infer the types","text":"","category":"section"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Knowing the types of the variables is key to generate a fast binary. Given the input types, the Julia compiler can infer the types of the variables in the function.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"If all the types are inferred, the function is called type stable. One can use the @code_warntype macro to check if the function is type stable. For example, the jlfactorial function with integer input is type stable:","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"julia> @code_warntype jlfactorial(10)\nMethodInstance for jlfactorial(::Int64)\n  from jlfactorial(n) @ Main REPL[4]:1\nArguments\n  #self#::Core.Const(jlfactorial)\n  n::Int64\nLocals\n  @_3::Union{Nothing, Tuple{Int64, Int64}}\n  x::Int64\n  i::Int64\nBody::Int64\n1 ─       (x = 1)\n│   %2  = (1:n)::Core.PartialStruct(UnitRange{Int64}, Any[Core.Const(1), Int64])\n│         (@_3 = Base.iterate(%2))\n│   %4  = (@_3 === nothing)::Bool\n│   %5  = Base.not_int(%4)::Bool\n└──       goto #4 if not %5\n2 ┄ %7  = @_3::Tuple{Int64, Int64}\n│         (i = Core.getfield(%7, 1))\n│   %9  = Core.getfield(%7, 2)::Int64\n│         (x = x * i)\n│         (@_3 = Base.iterate(%2, %9))\n│   %12 = (@_3 === nothing)::Bool\n│   %13 = Base.not_int(%12)::Bool\n└──       goto #4 if not %13\n3 ─       goto #2\n4 ┄       return x","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"If the types are not inferred, the function is called type unstable. For example, the badcode function is type unstable:","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"julia> badcode(x) = x > 3 ? 1.0 : 3\n\njulia> @code_warntype badcode(4)\nMethodInstance for badcode(::Int64)\n  from badcode(x) @ Main REPL[9]:1\nArguments\n  #self#::Core.Const(badcode)\n  x::Int64\nBody::Union{Float64, Int64}\n1 ─ %1 = (x > 3)::Bool\n└──      goto #3 if not %1\n2 ─      return 1.0\n3 ─      return 3","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"In this example, the output type Union{Float64, Int64} means the return type is either Float64 or Int64. The function is type unstable because the return type is not fixed. Type unstable code is slow. In the following example, the badcode function is ~10 times slower than its type stable version stable:","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"julia> x = rand(1:10, 1000);\n\njulia> @benchmark badcode.($x)\nBenchmarkTools.Trial: 10000 samples with 8 evaluations.\n Range (min … max):  2.927 μs … 195.198 μs  ┊ GC (min … max):  0.00% … 96.52%\n Time  (median):     3.698 μs               ┊ GC (median):     0.00%\n Time  (mean ± σ):   4.257 μs ±   7.894 μs  ┊ GC (mean ± σ):  12.43% ±  6.54%\n\n                 ▁▅█▅▃▂                                        \n  ▁▃▅▇▇▇▅▃▂▂▂▃▄▆▇███████▇▇▅▄▄▃▃▃▃▃▃▂▂▃▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▃\n  2.93 μs         Histogram: frequency by time        5.44 μs <\n\n Memory estimate: 26.72 KiB, allocs estimate: 696.\n\njulia> stable(x) = x > 3 ? 1.0 : 3.0\nstable (generic function with 1 method)\n\njulia> @benchmark stable.($x)\nBenchmarkTools.Trial: 10000 samples with 334 evaluations.\n Range (min … max):  213.820 ns … 25.350 μs  ┊ GC (min … max):  0.00% … 98.02%\n Time  (median):     662.551 ns              ┊ GC (median):     0.00%\n Time  (mean ± σ):   947.978 ns ±  1.187 μs  ┊ GC (mean ± σ):  29.30% ± 21.05%\n\n  ▂▃▅██▇▅▄▃▂▁                                                  ▂\n  ████████████▇▅▅▄▄▁▁▁▁▁▁▁▁▁▁▁▁▁▃▅▆▇██████▇▇▇▆█▇▇▇▇▇▇▇▇▆▇▆▆▆▇▇ █\n  214 ns        Histogram: log(frequency) by time      6.32 μs <\n\n Memory estimate: 7.94 KiB, allocs estimate: 1.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"In the above example:","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"\".\" is the broadcasting operator, it applies the function to each element of the array.\n\"$\" is the interpolation operator, it is used to interpolate a variable into an expression. In a benchmark, it can be used to avoid the overhead of variable initialization.","category":"page"},{"location":"chap2/julia-why/#Step-2:-Generates-the-LLVM-intermediate-representation","page":"Why Julia?","title":"Step 2: Generates the LLVM intermediate representation","text":"","category":"section"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"LLVM is a set of compiler and toolchain technologies that can be used to develop a front end for any programming language and a back end for any instruction set architecture. LLVM is the backend of multiple languages, including Julia, Rust, Swift and Kotlin.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"In Julia, one can use the @code_llvm macro to show the LLVM intermediate representation of a function.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"julia> @code_llvm jlfactorial(10)\n\nor any instruction set architecture. LLVM is the backend of multiple languages, including Julia, Rust, Swift and Kotlin.\n\n\n\n;  @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:1 within `jlfactorial`\ndefine i64 @julia_jlfactorial_3677(i64 signext %0) #0 {\ntop:\n;  @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:3 within `jlfactorial`\n; ┌ @ range.jl:5 within `Colon`\n; │┌ @ range.jl:403 within `UnitRange`\n; ││┌ @ range.jl:414 within `unitrange_last`\n     %1 = call i64 @llvm.smax.i64(i64 %0, i64 0)\n; └└└\n; ┌ @ range.jl:897 within `iterate`\n; │┌ @ range.jl:672 within `isempty`\n; ││┌ @ operators.jl:378 within `>`\n; │││┌ @ int.jl:83 within `<`\n      %2 = icmp slt i64 %0, 1\n; └└└└\n  br i1 %2, label %L32, label %L17.preheader\n\nL17.preheader:                                    ; preds = %top\n;  @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:5 within `jlfactorial`\n  %min.iters.check = icmp ult i64 %1, 2\n  br i1 %min.iters.check, label %scalar.ph, label %vector.ph\n\nvector.ph:                                        ; preds = %L17.preheader\n  %n.vec = and i64 %1, 9223372036854775806\n  %ind.end = or i64 %1, 1\n  br label %vector.body\n\nvector.body:                                      ; preds = %vector.body, %vector.ph\n  %index = phi i64 [ 0, %vector.ph ], [ %induction12, %vector.body ]\n  %vec.phi = phi i64 [ 1, %vector.ph ], [ %3, %vector.body ]\n  %vec.phi11 = phi i64 [ 1, %vector.ph ], [ %4, %vector.body ]\n  %offset.idx = or i64 %index, 1\n  %induction12 = add i64 %index, 2\n;  @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:4 within `jlfactorial`\n; ┌ @ int.jl:88 within `*`\n   %3 = mul i64 %vec.phi, %offset.idx\n   %4 = mul i64 %vec.phi11, %induction12\n   %5 = icmp eq i64 %induction12, %n.vec\n   br i1 %5, label %middle.block, label %vector.body\n\nmiddle.block:                                     ; preds = %vector.body\n; └\n;  @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:5 within `jlfactorial`\n  %bin.rdx = mul i64 %4, %3\n  %cmp.n = icmp eq i64 %1, %n.vec\n  br i1 %cmp.n, label %L32, label %scalar.ph\n\nscalar.ph:                                        ; preds = %middle.block, %L17.preheader\n  %bc.resume.val = phi i64 [ %ind.end, %middle.block ], [ 1, %L17.preheader ]\n  %bc.merge.rdx = phi i64 [ %bin.rdx, %middle.block ], [ 1, %L17.preheader ]\n  br label %L17\n\nL17:                                              ; preds = %L17, %scalar.ph\n  %value_phi4 = phi i64 [ %7, %L17 ], [ %bc.resume.val, %scalar.ph ]\n  %value_phi6 = phi i64 [ %6, %L17 ], [ %bc.merge.rdx, %scalar.ph ]\n;  @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:4 within `jlfactorial`\n; ┌ @ int.jl:88 within `*`\n   %6 = mul i64 %value_phi6, %value_phi4\n; └\n;  @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:5 within `jlfactorial`\n; ┌ @ range.jl:901 within `iterate`\n; │┌ @ promotion.jl:521 within `==`\n    %.not = icmp eq i64 %value_phi4, %1\n; │└\n   %7 = add nuw i64 %value_phi4, 1\n; └\n  br i1 %.not, label %L32, label %L17\n\nL32:                                              ; preds = %L17, %middle.block, %top\n  %value_phi10 = phi i64 [ 1, %top ], [ %bin.rdx, %middle.block ], [ %6, %L17 ]\n;  @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:6 within `jlfactorial`\n  ret i64 %value_phi10\n}","category":"page"},{"location":"chap2/julia-why/#Step-3:-Compiles-to-binary-code","page":"Why Julia?","title":"Step 3: Compiles to binary code","text":"","category":"section"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"The LLVM intermediate representation is then compiled to binary code by the LLVM compiler. The binary code can be printed by the @code_native macro.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"julia> @code_native jlfactorial(10)\n\t.section\t__TEXT,__text,regular,pure_instructions\n\t.build_version macos, 14, 0\n\t.globl\t_julia_jlfactorial_3726         ; -- Begin function julia_jlfactorial_3726\n\t.p2align\t2\n_julia_jlfactorial_3726:                ; @julia_jlfactorial_3726\n; ┌ @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:1 within `jlfactorial`\n; %bb.0:                                ; %top\n; │ @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:3 within `jlfactorial`\n; │┌ @ range.jl:5 within `Colon`\n; ││┌ @ range.jl:403 within `UnitRange`\n; │││┌ @ range.jl:414 within `unitrange_last`\n\tcmp\tx0, #0\n\tcsel\tx9, x0, xzr, gt\n; │└└└\n\tcmp\tx0, #1\n\tb.lt\tLBB0_3\n; %bb.1:                                ; %L17.preheader\n; │ @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:5 within `jlfactorial`\n\tcmp\tx9, #2\n\tb.hs\tLBB0_4\n; %bb.2:\n\tmov\tw8, #1\n\tmov\tw0, #1\n\tb\tLBB0_7\nLBB0_3:\n\tmov\tw0, #1\n; │ @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:6 within `jlfactorial`\n\tret\nLBB0_4:                                 ; %vector.ph\n\tmov\tx12, #0\n; │ @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:5 within `jlfactorial`\n\tand\tx10, x9, #0x7ffffffffffffffe\n\torr\tx8, x9, #0x1\n\tmov\tw11, #1\n\tmov\tw13, #1\nLBB0_5:                                 ; %vector.body\n                                        ; =>This Inner Loop Header: Depth=1\n; │ @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:4 within `jlfactorial`\n; │┌ @ int.jl:88 within `*`\n\tmadd\tx11, x11, x12, x11\n; │└\n; │ @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:5 within `jlfactorial`\n\tadd\tx14, x12, #2\n; │ @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:4 within `jlfactorial`\n; │┌ @ int.jl:88 within `*`\n\tmul\tx13, x13, x14\n\tmov\tx12, x14\n\tcmp\tx10, x14\n\tb.ne\tLBB0_5\n; %bb.6:                                ; %middle.block\n; │└\n; │ @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:5 within `jlfactorial`\n\tmul\tx0, x13, x11\n\tcmp\tx9, x10\n\tb.eq\tLBB0_9\nLBB0_7:                                 ; %L17.preheader15\n\tadd\tx9, x9, #1\nLBB0_8:                                 ; %L17\n                                        ; =>This Inner Loop Header: Depth=1\n; │ @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:4 within `jlfactorial`\n; │┌ @ int.jl:88 within `*`\n\tmul\tx0, x0, x8\n; │└\n; │ @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:5 within `jlfactorial`\n; │┌ @ range.jl:901 within `iterate`\n\tadd\tx8, x8, #1\n; │└\n\tcmp\tx9, x8\n\tb.ne\tLBB0_8\nLBB0_9:                                 ; %L32\n; │ @ /Users/liujinguo/jcode/ModernScientificComputing2024/Lecture2/3.julia.jl#==#d2429055-58e9-4d84-894f-2e639723e078:6 within `jlfactorial`\n\tret\n; └\n                                        ; -- End function\n.subsections_via_symbols","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Single function definition may have multiple method instances.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"julia> methods(jlfactorial)\n# 1 method for generic function \"jlfactorial\" from Main:\n [1] jlfactorial(n)\n     @ REPL[4]:1","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"Whenever the function is called with a new input type, the Julia compiler will generate a new method instance for the function. The method instance is then stored in the method table, and can be analyzed by the MethodAnalysis package.","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"julia> using MethodAnalysis\n\njulia> methodinstances(jlfactorial)\n1-element Vector{Core.MethodInstance}:\n MethodInstance for jlfactorial(::Int64)\n\njulia> jlfactorial(UInt32(5))\n120\n\njulia> methodinstances(jlfactorial)\n2-element Vector{Core.MethodInstance}:\n MethodInstance for jlfactorial(::Int64)\n MethodInstance for jlfactorial(::UInt32)","category":"page"},{"location":"chap2/julia-why/","page":"Why Julia?","title":"Why Julia?","text":"When a function is called with multiple arguments, the Julia compiler will invoke the correct method instance according to the type of the arguments. This is called multiple dispatch.","category":"page"},{"location":"chap7/cuda/#CUDA-programming","page":"CUDA programming","title":"CUDA programming","text":"","category":"section"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"using Test using CUDA; CUDA.allowscalar(false) using BenchmarkTools","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"TableOfContents()","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"function highlight(str) \tHTML(<span style=\"background-color:yellow\">str</span>) end;","category":"page"},{"location":"chap7/cuda/#Simulting-lattice-gas-cellular-automata\"","page":"CUDA programming","title":"Simulting lattice gas cellular automata\"","text":"","category":"section"},{"location":"chap7/cuda/#Cellular-automata","page":"CUDA programming","title":"Cellular automata","text":"","category":"section"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"A descretized space and time,\nA state defined on the space,\nA simple set of rules (local & finite) to describe the evolution of the state.","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Reference:","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Hardy J, Pomeau Y, De Pazzis O. Time evolution of a two‐dimensional model system. I. Invariant states and time correlation functions[J]. Journal of Mathematical Physics, 1973, 14(12): 1746-1759.","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"(Image: )","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Particles exist only on the grid points, never on the edges or surface of the lattice.\nEach particle has an associated direction (from one grid point to another immediately adjacent grid point).\nEach lattice grid cell can only contain a maximum of one particle for each direction, i.e., contain a total of between zero and four particles.","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"The following rules also govern the model:","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"A single particle moves in a fixed direction until it experiences a collision.\nTwo particles experiencing a head-on collision are deflected perpendicularly.\nTwo particles experience a collision which isn't head-on simply pass through each other and continue in the same direction.\nOptionally, when a particles collides with the edges of a lattice it can rebound.","category":"page"},{"location":"chap7/cuda/#CUDA-programming-with-Julia","page":"CUDA programming","title":"CUDA programming with Julia","text":"","category":"section"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"CUDA programming is a highlight(\"parallel computing platform and programming model\") developed by NVIDIA for performing general-purpose computations on its GPUs (Graphics Processing Units). CUDA stands for Compute Unified Device Architecture.","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"References:","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"JuliaComputing/Training\narXiv: 1712.03112","category":"page"},{"location":"chap7/cuda/#Goal","page":"CUDA programming","title":"Goal","text":"","category":"section"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Run a CUDA program\nWrite your own CUDA kernel\nCreate a CUDA project","category":"page"},{"location":"chap7/cuda/#Run-a-CUDA-program\"","page":"CUDA programming","title":"Run a CUDA program\"","text":"","category":"section"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Make sure you have a NVIDIA GPU device and its driver is properly installed.","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"run(nvidia-smi)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Install the CUDA.jl package, and disable scalar indexing of CUDA arrays.","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"CUDA.jl provides wrappers for several CUDA libraries that are part of the CUDA toolkit:","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Driver library: manage the device, highlight(\"launch kernels\"), etc.\nCUBLAS: linear algebra\nCURAND: random number generation\nCUFFT: fast fourier transform\nCUSPARSE: sparse arrays\nCUSOLVER: decompositions & linear systems","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"There's also support for a couple of libraries that aren't part of the CUDA toolkit, but are commonly used:","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"CUDNN: deep neural networks\nCUTENSOR: linear algebra with tensors","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"CUDA.versioninfo()","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Choose a device (if multiple devices are available).","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"devices()","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"dev = CuDevice(0)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"grid > block > thread\"","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"attribute(dev, CUDA.DEVICEATTRIBUTEMAXTHREADSPER_BLOCK)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"attribute(dev, CUDA.CUDEVICEATTRIBUTEMAXBLOCKDIMX)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"attribute(dev, CUDA.CUDEVICEATTRIBUTEMAXGRIDDIMX)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Create a CUDA Array\"","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"CUDA.zeros(10)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"cuarray1 = CUDA.randn(10)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"@test_throws ErrorException cuarray1[3]","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"CUDA.@allowscalar cuarray1[3] += 10","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Upload a CPU Array to GPU\"","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"CuArray(randn(10))","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Compute\"","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Computing a function on GPU Arrays","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Launch a CUDA job - a few micro seconds\nLaunch more CUDA jobs...\nSynchronize threads - a few micro seconds","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Computing matrix multiplication.\"","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"@elapsed rand(2000,2000) * rand(2000,2000)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"@elapsed CUDA.@sync CUDA.rand(2000,2000) * CUDA.rand(2000,2000)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Broadcasting a native Julia function Julia -> LLVM (optimized for CUDA) -> CUDA \"","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"factorial(n) = n == 1 ? 1 : factorial(n-1)*n","category":"page"},{"location":"chap7/cuda/#this-function-is-copied-from-lecture-9","page":"CUDA programming","title":"this function is copied from lecture 9","text":"","category":"section"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"function poor_besselj(ν::Int, z::T; atol=eps(T)) where T     k = 0     s = (z/2)^ν / factorial(ν)     out = s::T     while abs(s) > atol         k += 1         s *= -(k+ν) * (z/2)^2 / k         out += s     end     out end","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"x = CUDA.CuArray(0.0:0.01:10)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"poor_besselj.(1, x)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"manage your GPU devices\"","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"nvmldev = NVML.Device(parentuuid(device()))","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"NVML.powerusage(nvmldev)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"NVML.utilizationrates(nvmldev)","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"NVML.computeprocesses(nvmldev)","category":"page"},{"location":"chap7/cuda/#CUDA-libraries-and-Kernel-Programming","page":"CUDA programming","title":"CUDA libraries and Kernel Programming","text":"","category":"section"},{"location":"chap7/cuda/#Appendix:-The-Navier-Stokes-equation","page":"CUDA programming","title":"Appendix: The Navier-Stokes equation","text":"","category":"section"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"Reference: https://youtu.be/Ra7aQlenTb8","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"html <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ra7aQlenTb8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"The navier stokes equation describes the fluid dynamics, which contains the following two parts.","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"The first one describes the conservation of volume","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"nabla underbraceu_textvelocity  u in mathbbR^d = 0","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"The second one describes the dynamics","category":"page"},{"location":"chap7/cuda/","page":"CUDA programming","title":"CUDA programming","text":"underbracerho_textdensity fracdudt = underbrace-nabla p_textpressure + underbracemu nabla^2 u_textviscosity (or friction) + underbracef_textexternal force","category":"page"},{"location":"chap2/julia-fluid/#Project:-Fluid-dynamics","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"","category":"section"},{"location":"chap2/julia-fluid/#Fluid-Dynamics-Simulation","page":"Project: Fluid dynamics","title":"Fluid Dynamics Simulation","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Fluid dynamics is the study of the movement of fluids, including air and water. In this project, we will use the Lattice Boltzmann Method (LBM) to simulate fluid dynamics, which is a mesoscopic method based on the kinetic theory of gases.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":" micro meso macro\nScale 10^-9m 10^-9 -10^-6m 10^6m\nPhysics molecular probabilistic continuous\nGov. Eq. Newton Boltzmann Navier-Stokes equations\nMethod Molecular Dynamics Lattice Boltzmann Computational Fluid Dynamics","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"This book does not aim to provide a comprehensive understanding of fluid dynamics. If you are interested in learning more about fluid dynamics, you can refer to the following resources:","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Fluid Dynamics Simulation (in Python, Java and Javascript)\nYouTube - Introduction to Lattice Boltzmann Method","category":"page"},{"location":"chap2/julia-fluid/#Lattice-Boltzmann-Method-(LBM)","page":"Project: Fluid dynamics","title":"Lattice Boltzmann Method (LBM)","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The general idea of LBM is to simulate the fluid dynamics by modeling the movement of particles in a lattice, a grid of cells, without keeping track of the individual particles. The state of a cell in the lattice is defined by the density of particles moving in different directions, i.e. ","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"rm state(i j) equiv rho_ij(mathbfv)","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"where (i j) is the position of the cell in the lattice and mathbfv is the velocity of the particles.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"<img src=\"../../assets/images/lattice.png\" alt=\"image\" width=300 height=\"auto\">","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The particles move with different velocities mathbf v and collide with each other, driving the fluid to reach an equilibrium state, where the energy of the particles is governed by the Boltzmann distribution","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"rho_rm eq(E) sim e^-fracEk_BT (rm or  e^- rm const times mathbfv^2)","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"where k_B is the Boltzmann constant, T is the temperature, and E = frac12mmathbfv^2 is the energy of the particles.","category":"page"},{"location":"chap2/julia-fluid/#D2Q9-model","page":"Project: Fluid dynamics","title":"D2Q9 model","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The lattice Boltzmann method uses a discrete set of velocities, which is a simplification of the continuous velocity space. One of the simplest models is the D2Q9 model, which contains","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"a 2D lattice, and\n9 discrete velocities: (00), (10) (01) (-10) (0-1), (11) (-11) (-1-1) (1-1).","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"<img src=\"../../assets/images/D2Q9.png\" alt=\"image\" width=500 height=\"auto\">","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Lattice Boltzmann Method (LBM) contains two steps:","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Streaming - particles move to neighboring cells\nCollision - particles collide and exchange momentum","category":"page"},{"location":"chap2/julia-fluid/#Streaming","page":"Project: Fluid dynamics","title":"Streaming","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"<img src=\"../../assets/images/stream.png\" alt=\"image\" width=500 height=\"auto\">","category":"page"},{"location":"chap2/julia-fluid/#Collision-Bhatnagar-Gross-Krook-(BGK)-model.","page":"Project: Fluid dynamics","title":"Collision - Bhatnagar-Gross-Krook (BGK) model.","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The collision step is based on the Bhatnagar-Gross-Krook (BGK) model, which is a simplified version of the Boltzmann equation. The collision step is defined as","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"rholeftarrow(1-omega)rho_0+omegarho_mathrmrm eq","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"where rho is the updated density rho_0 is the density before collision, and rho_texteq is the equilibrium density omega = Delta ttau, where tau is the (relative) relaxation time","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"<img src=\"../../assets/images/Equilibrium density.png\" alt=\"image\" width=500 height=\"auto\">","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The BGK model has a nice property that it conserves:","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"total density rho\nmomentum rhomathbfu","category":"page"},{"location":"chap2/julia-fluid/#Julia-implementation","page":"Project: Fluid dynamics","title":"Julia implementation","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The following code is a part of the package MyFirstPackage that we created in the previous section: My First Package.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"File: src/fluid.jl (Step 1-6)","category":"page"},{"location":"chap2/julia-fluid/#Step-1.-Define-the-lattice-Boltzmann-configuration","page":"Project: Fluid dynamics","title":"Step 1. Define the lattice Boltzmann configuration","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Let us start by defining an abstract type for lattice Boltzmann configurations and a concrete type that implements the D2Q9 lattice.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"\"\"\"\n    AbstractLBConfig{D, N}\n\nAn abstract type for lattice Boltzmann configurations.\n\"\"\"\nabstract type AbstractLBConfig{D, N} end","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The D2Q9 lattice Boltzman configuration is defined as follows:","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"\"\"\"\n    D2Q9 <: AbstractLBConfig{2, 9}\n\nA lattice Boltzmann configuration for 2D, 9-velocity model.\n\"\"\"\nstruct D2Q9 <: AbstractLBConfig{2, 9} end\ndirections(::D2Q9) = (\n        Point(1, 1), Point(-1, 1),\n        Point(1, 0), Point(0, -1),\n        Point(0, 0), Point(0, 1),\n        Point(-1, 0), Point(1, -1),\n        Point(-1, -1),\n    )","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The directions function returns the 9 discrete velocities in the D2Q9 model. The velocities are ordered in a specific way, which enables us to define a function to flip the velocity vector. This is useful for handling the boundaries and barriers in the lattice.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"# directions[k] is the opposite of directions[flip_direction_index(k)\nfunction flip_direction_index(::D2Q9, i::Int)\n    return 10 - i\nend","category":"page"},{"location":"chap2/julia-fluid/#Step-2:-Define-the-Cell-type-for-storing-the-state","page":"Project: Fluid dynamics","title":"Step 2: Define the Cell type for storing the state","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The cell state is defined by the density of the fluid in different directions, rho_ij(mathbfv).","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"# the density of the fluid, each component is the density of a velocity\nstruct Cell{N, T <: Real}\n    density::NTuple{N, T}\nend\n# the total density of the fluid\ndensity(cell::Cell) = sum(cell.density)\n# the density of the fluid in a specific direction,\n# where the direction is an integer\ndensity(cell::Cell, direction::Int) = cell.density[direction]","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Expect the total density, the momentum mathbfu is also conserved, which is defined as the momentum of the fluid.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"\"\"\"\n    momentum(lb::AbstractLBConfig, rho::Cell)\n\nCompute the momentum of the fluid from the density of the fluid.\n\"\"\"\nfunction momentum(lb::AbstractLBConfig, rho::Cell)\n    return mapreduce((r, d) -> r * d, +, rho.density, directions(lb)) / density(rho)\nend","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Let us also define the addition and multiplication operations for the Cell type.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Base.:+(x::Cell, y::Cell) = Cell(x.density .+ y.density)\nBase.:*(x::Real, y::Cell) = Cell(x .* y.density)","category":"page"},{"location":"chap2/julia-fluid/#Step-3.-Implement-the-streaming-step","page":"Project: Fluid dynamics","title":"Step 3. Implement the streaming step","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"# streaming step\nfunction stream!(\n        lb::AbstractLBConfig{2, N},  # lattice configuration\n        newgrid::AbstractMatrix{D}, # the updated grid\n        grid::AbstractMatrix{D}, # the original grid\n        barrier::AbstractMatrix{Bool} # the barrier configuration\n    ) where {N, T, D<:Cell{N, T}}\n    ds = directions(lb)\n    @inbounds for ci in CartesianIndices(newgrid)\n        i, j = ci.I\n        newgrid[ci] = Cell(ntuple(N) do k # collect the densities\n            ei = ds[k]\n            m, n = size(grid)\n            i2, j2 = mod1(i - ei[1], m), mod1(j - ei[2], n)\n            if barrier[i2, j2]\n                # if the cell is a barrier, the fluid flows back\n                density(grid[i, j], flip_direction_index(lb, k))\n            else\n                # otherwise, the fluid flows to the neighboring cell\n                density(grid[i2, j2], k)\n            end\n        end)\n    end\nend","category":"page"},{"location":"chap2/julia-fluid/#Step-4.-Implement-the-collision-step","page":"Project: Fluid dynamics","title":"Step 4. Implement the collision step","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"By the Bhatnagar-Gross-Krook (BGK) model, the collision step drives the fluid towards an equilibrium state. The equilibrium density is completely determined by the total density and the momentum of the fluid:","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"rho_rm eq(mathbfv_i) approx rho_rm tot w_i left(1 + 3mathbfv_icdotmathbfu + frac92(mathbfv_icdotmathbfu)^2 - frac32mathbfucdotmathbfuright)","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"where rho_rm tot is the total density, mathbfu is the momentum, w_i sim e^-mathbfv_i^2 is the distribution of velocities with mean-velocity zero, and v_i is the velocity vector.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The detailed derivation of the equilibrium density could be found in the first reference at the top of this page.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"\"\"\"\n    equilibrium_density(lb::AbstractLBConfig, ρ, u)\n\nCompute the equilibrium density of the fluid from the total density and the momentum.\n\"\"\"\nfunction equilibrium_density(lb::AbstractLBConfig{D, N}, ρ, u) where {D, N}\n    ws, ds = weights(lb), directions(lb)\n    return Cell(\n        ntuple(i-> ρ * ws[i] * _equilibrium_density(u, ds[i]), N)\n    )\nend\n\n# the distribution of the 9 velocities at the equilibrium state\nweights(::D2Q9) = (1/36, 1/36, 1/9, 1/9, 4/9, 1/9, 1/9, 1/36, 1/36)\nfunction _equilibrium_density(u, ei)\n    # the equilibrium density of the fluid with a specific mean momentum\n    return (1 + 3 * dot(ei, u) + 9/2 * dot(ei, u)^2 - 3/2 * dot(u, u))\nend","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The collision step that implements the BGK model is defined as follows:","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"# collision step, applied on a single cell\nfunction collide(lb::AbstractLBConfig{D, N}, rho; viscosity = 0.02) where {D, N}\n    omega = 1 / (3 * viscosity + 0.5)   # \"relaxation\" parameter\n    # Recompute macroscopic quantities:\n    v = momentum(lb, rho)\n    return (1 - omega) * rho + omega * equilibrium_density(lb, density(rho), v)\nend","category":"page"},{"location":"chap2/julia-fluid/#Step-5.-Implement-the-lattice-Boltzmann-simulation","page":"Project: Fluid dynamics","title":"Step 5. Implement the lattice Boltzmann simulation","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"\"\"\"\n    LatticeBoltzmann{D, N, T, CFG, MT, BT}\n\nA lattice Boltzmann simulation with D dimensions, N velocities, and lattice configuration CFG.\n\"\"\"\n\nstruct LatticeBoltzmann{D, N, T, CFG<:AbstractLBConfig{D, N}, MT<:AbstractMatrix{Cell{N, T}}, BT<:AbstractMatrix{Bool}}\n    config::CFG # lattice configuration\n    grid::MT    # density of the fluid\n    gridcache::MT # cache for the density of the fluid\n    barrier::BT # barrier configuration\nend\n\nfunction LatticeBoltzmann(config::AbstractLBConfig{D, N}, grid::AbstractMatrix{<:Cell}, barrier::AbstractMatrix{Bool}) where {D, N}\n    @assert size(grid) == size(barrier)\n    return LatticeBoltzmann(config, grid, similar(grid), barrier)\nend","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"A single step of the lattice Boltzmann simulation is defined as follows, which directly modifies the grid field of the LatticeBoltzmann type.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"\"\"\"\n    step!(lb::LatticeBoltzmann)\n\nPerform a single step of the lattice Boltzmann simulation.\n\"\"\"\nfunction step!(lb::LatticeBoltzmann)\n    copyto!(lb.gridcache, lb.grid)\n    stream!(lb.config, lb.grid, lb.gridcache, lb.barrier)\n    lb.grid .= collide.(Ref(lb.config), lb.grid)\n    return lb\nend","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"For better visualization, we define a function to compute the curl of the momentum field in 2D, which is defined as:","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"\"\"\"\n    curl(u::AbstractMatrix{Point2D{T}})\n\nCompute the curl of the momentum field in 2D, which is defined as:\n```math\n∂u_y/∂x−∂u_x/∂y\n```\n\"\"\"\nfunction curl(u::Matrix{Point2D{T}}) where T \n    return map(CartesianIndices(u)) do ci\n        i, j = ci.I\n        m, n = size(u)\n        uy = u[mod1(i + 1, m), j][2] - u[mod1(i - 1, m), j][2]\n        ux = u[i, mod1(j + 1, n)][1] - u[i, mod1(j - 1, n)][1]\n        return uy - ux # a factor of 1/2 is missing here?\n    end\nend","category":"page"},{"location":"chap2/julia-fluid/#Step-6.-Find-the-example-simulation","page":"Project: Fluid dynamics","title":"Step 6. Find the example simulation","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"A D2Q9 lattice Boltzmann simulation example. A simple linear barrier is added to the lattice.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"function example_d2q9(;\n        height = 80, width = 200,\n        u0 = Point(0.0, 0.1)) # initial and in-flow speed\n    # Initialize all the arrays to steady rightward flow:\n    rho = equilibrium_density(D2Q9(), 1.0, u0)\n    rgrid = fill(rho, height, width)\n\n    # Initialize barriers:\n    barrier = falses(height, width)  # True wherever there's a barrier\n    mid = div(height, 2)\n    barrier[mid-8:mid+8, div(height,2)] .= true              # simple linear barrier\n\n    return LatticeBoltzmann(D2Q9(), rgrid, barrier)\nend","category":"page"},{"location":"chap2/julia-fluid/#Step-7.-Include-the-above-file-into-the-package-module","page":"Project: Fluid dynamics","title":"Step 7. Include the above file into the package module","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"File: src/MyFirstPackage.jl","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"module MyFirstPackage\n# import packages\nusing LinearAlgebra\n\n# export interfaces\nexport Lorenz, integrate_step\nexport Point, Point2D, Point3D\nexport RungeKutta, Euclidean\nexport D2Q9, LatticeBoltzmann, step!, equilibrium_density, momentum, curl, example_d2q9, density\n\ninclude(\"lorenz.jl\")\ninclude(\"fluid.jl\")\n\nend","category":"page"},{"location":"chap2/julia-fluid/#Using-the-package","page":"Project: Fluid dynamics","title":"Using the package","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"File: examples/barrier.jl","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"using CairoMakie: RGBA # for visualization\nusing CairoMakie\nusing MyFirstPackage # our package\n\n# Set up the visualization with Makie:\nlb = example_d2q9()\nvorticity = Observable(curl(momentum.(Ref(lb.config), lb.grid))')\nfig, ax, plot = image(vorticity, colormap = :jet, colorrange = (-0.1, 0.1))\n\n# Show barrier\nbarrier_img = map(x -> x ? RGBA(0, 0, 0, 1) : RGBA(0, 0, 0, 0), lb.barrier)\nimage!(ax, barrier_img')\n\n# Recording the simulation\nrecord(fig, joinpath(@__DIR__, \"barrier.mp4\"), 1:100; framerate = 10) do i\n    for i=1:20\n        step!(lb)\n    end\n    vorticity[] = curl(momentum.(Ref(lb.config), lb.grid))'\nend","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"To ensure the reproducibility of the code, we need to create a local environment and install the required packages.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"pkg> activate examples\n\npkg> dev .\n\npkg> add CairoMakie BenchmarkTools","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Type Backspace to exit the package mode. To execute the code, type","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"julia> include(\"examples/barrier.jl\")","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"You should see a new file barrier.mp4 in the examples directory, which is the recording of the simulation.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"<video width=\"320\" height=\"240\" controls>\n  <source src=\"../../assets/images/barrier.mp4\" type=\"video/mp4\">\n</video>","category":"page"},{"location":"chap2/julia-fluid/#Benchmarking","page":"Project: Fluid dynamics","title":"Benchmarking","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"julia> using BenchmarkTools\n\njulia> @benchmark step!($(deepcopy(lb)))\nBenchmarkTools.Trial: 3637 samples with 1 evaluation.\n Range (min … max):  1.323 ms …  1.899 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     1.363 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   1.374 ms ± 30.730 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n             ▁▇█▆▄▃▃▂▂▂▁▂▂▂▂▁▁▁▁▁▁                           ▁\n  ▃▁▁▃▃▃▃▁▁▁▃█████████████████████████▆▆▆▆▅▆▆▆▆▅▅▆▆▃▅▅▄▆▆▅▃▆ █\n  1.32 ms      Histogram: log(frequency) by time     1.49 ms <\n\n Memory estimate: 0 bytes, allocs estimate: 0.","category":"page"},{"location":"chap2/julia-fluid/#Profiling-identify-the-performance-bottleneck","page":"Project: Fluid dynamics","title":"Profiling - identify the performance bottleneck","text":"","category":"section"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Premature optimization is the root of all evil – Donald Knuth","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Performance optimization comes after the correctness of the code, and it should be based on the profiling result. Profiling is mainly used to identify the performance bottleneck of your code. The Profile module in Julia provides a set of tools to profile your code.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"julia> using Profile\n\njulia> Profile.init(n = 10^7, delay = 0.001) # set the number of samples and the delay between samples","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Then you can profile your code by wrapping the code with @profile macro.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"julia> @profile for i in 1:100\n           step!(lb)\n       end","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"In order to collect enough samples, we run the step! function for 100 times. To view the profile result, you can use the Profile.print function.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"julia> Profile.print(; mincount=10) # only show functions with ≥10 counts\nOverhead ╎ [+additional indent] Count File:Line; Function\n=========================================================\n            ... truncated message ...\n  ╎    ╎    ╎    ╎ 88  …rc/fluid.jl:144; step!(lb::LatticeBoltzm…\n  ╎    ╎    ╎    ╎  88  …rc/fluid.jl:76; stream!(lb::D2Q9, newgr…\n33╎    ╎    ╎    ╎   88  …e/ntuple.jl:19; ntuple\n  ╎    ╎    ╎    ╎    18  …rc/fluid.jl:79; (::MyFirstPackage.var…\n  ╎    ╎    ╎    ╎     10  …perators.jl:830; mod1\n 1╎    ╎    ╎    ╎    16  …rc/fluid.jl:80; (::MyFirstPackage.var…\n  ╎    ╎    ╎    ╎     15  …actarray.jl:1291; getindex\n  ╎    ╎    ╎    ╎    ╎ 13  …actarray.jl:1323; _getindex\n 1╎    ╎    ╎    ╎    ╎  13  …actarray.jl:702; checkbounds\n  ╎    ╎    ╎    ╎    ╎   12  …actarray.jl:681; checkbounds\n  ╎    ╎    ╎    ╎ 13  …rc/fluid.jl:145; step!(lb::LatticeBoltzm…\n  ╎    ╎    ╎    ╎  13  …roadcast.jl:911; materialize!\n  ╎    ╎    ╎    ╎   13  …roadcast.jl:914; materialize!\n  ╎    ╎    ╎    ╎    13  …roadcast.jl:956; copyto!\n  ╎    ╎    ╎    ╎     13  …roadcast.jl:1003; copyto!\n  ╎    ╎    ╎    ╎    ╎ 12  …simdloop.jl:77; macro expansion\n  ╎    ╎    ╎    ╎    ╎  12  …roadcast.jl:1004; macro expansion\n  ╎    ╎    ╎    ╎    ╎   12  …roadcast.jl:636; getindex\nTotal snapshots: 112. Utilization: 100% across all threads and tasks. Use the `groupby` kwarg to break down by thread and/or task.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Alternatively, you can use the format :flat to show the profile result in a flat view.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"julia> Profile.print(; mincount=10, format=:flat)\n Count  Overhead File                                                        Line Function\n =====  ======== ====                                                        ==== ========\n            ... truncated message ...\n    10         0 @MyFirstPackage/src/fluid.jl                                  63 #6\n    11        11 @MyFirstPackage/src/fluid.jl                                   ? (::MyFirstPackage.var\"#8#9\"{D2Q9, Matrix{MyFirstPackage.Cell{9, Float64}}, BitMatrix, N…\n    20         0 @MyFirstPackage/src/fluid.jl                                  79 (::MyFirstPackage.var\"#8#9\"{D2Q9, Matrix{MyFirstPackage.Cell{9, Float64}}, BitMatrix, N…\n    17         0 @MyFirstPackage/src/fluid.jl                                  80 (::MyFirstPackage.var\"#8#9\"{D2Q9, Matrix{MyFirstPackage.Cell{9, Float64}}, BitMatrix, N…\n    15         3 @MyFirstPackage/src/fluid.jl                                  94 collide(lb::D2Q9, rho::MyFirstPackage.Cell{9, Float64}; viscosity::Float64)\n    11         1 @MyFirstPackage/src/fluid.jl                                  62 equilibrium_density(lb::D2Q9, ρ::Float64, u::Point2D{Float64})\n    97         0 @MyFirstPackage/src/fluid.jl                                 144 step!(lb::LatticeBoltzmann{2, 9, Float64, D2Q9, Matrix{MyFirstPackage.Cell{9, Float64}}…\n    10         0 @MyFirstPackage/src/fluid.jl                                 145 step!(lb::LatticeBoltzmann{2, 9, Float64, D2Q9, Matrix{MyFirstPackage.Cell{9, Float64}}…\n    97         0 @MyFirstPackage/src/fluid.jl                                  76 stream!(lb::D2Q9, newgrid::Matrix{MyFirstPackage.Cell{9, Float64}}, grid::Matrix{MyFirs…\nTotal snapshots: 120. Utilization: 100% across all threads and tasks. Use the `groupby` kwarg to break down by thread and/or task.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The printed result shows that the stream! step is more costly than the collide step, which should be the focus of optimization.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"note: How profiling works\nProfiling is a statistical method to measure the performance of your code. It works by sampling the function call stack of the running code at a certain frequency.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Let use look at the stream! function to see which part of the code is more costly.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"julia> Profile.clear()   # clear the previous profile result\n\njulia> @profile for i in 1:300\n    MyFirstPackage.stream!(lb.config, lb.grid, lb.gridcache, lb.barrier)\nend\n\nProfile.print(format=:flat, mincount=30)  # show the profile result, only show the functions that are called more than 5 times\n            ... truncated message ...\n  ╎    ╎    ╎    ╎ 263 …rc/fluid.jl:76; stream!(lb::D2Q9, newgri…\n77╎    ╎    ╎    ╎  259 …e/ntuple.jl:19; ntuple\n  ╎    ╎    ╎    ╎   57  …rc/fluid.jl:79; (::MyFirstPackage.var\"…\n28╎    ╎    ╎    ╎    28  @Base/int.jl:86; -\n  ╎    ╎    ╎    ╎    29  …perators.jl:830; mod1\n  ╎    ╎    ╎    ╎     27  @Base/int.jl:287; mod\n  ╎    ╎    ╎    ╎    ╎ 27  @Base/div.jl:319; fld\n  ╎    ╎    ╎    ╎    ╎  27  @Base/div.jl:354; div\n  ╎    ╎    ╎    ╎    ╎   25  @Base/int.jl:1068; -\n25╎    ╎    ╎    ╎    ╎    25  @Base/int.jl:86; -\n 3╎    ╎    ╎    ╎   51  …rc/fluid.jl:80; (::MyFirstPackage.var\"…\n  ╎    ╎    ╎    ╎    48  …actarray.jl:1291; getindex\n  ╎    ╎    ╎    ╎     46  …actarray.jl:1323; _getindex\n 3╎    ╎    ╎    ╎    ╎ 46  …actarray.jl:702; checkbounds\n  ╎    ╎    ╎    ╎    ╎  43  …actarray.jl:681; checkbounds\n  ╎    ╎    ╎    ╎    ╎   39  …actarray.jl:728; checkbounds_indi…\n  ╎    ╎    ╎    ╎    ╎    39  …actarray.jl:728; checkbounds_ind…\n  ╎    ╎    ╎    ╎    ╎     39  …actarray.jl:763; checkindex\n38╎    ╎    ╎    ╎    ╎    ╎ 39  @Base/int.jl:86; -\n  ╎    ╎    ╎    ╎   29  …rc/fluid.jl:83; (::MyFirstPackage.var\"…\n29╎    ╎    ╎    ╎    29  …sentials.jl:14; getindex\nTotal snapshots: 264. Utilization: 100% across all threads and tasks. Use the `groupby` kwarg to break down by thread and/or task.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The line 79 in file src/fluid.jl costs 57/264 samples, which is the following line of code:","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"i2, j2 = mod1(i - ei[1], m), mod1(j - ei[2], n)","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"The line 80 in file src/fluid.jl costs 51/264 samples, which is the following line of code:","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"if barrier[i2, j2]","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"and most of the time is spent on the checkbounds function.","category":"page"},{"location":"chap2/julia-fluid/","page":"Project: Fluid dynamics","title":"Project: Fluid dynamics","text":"Could we remove the boundary check? Please refer to the Julia Performance Tips for more information.","category":"page"},{"location":"chap4/combinatorial/#Combinatorial-Optimization","page":"Combinatorial Optimization","title":"Combinatorial Optimization","text":"","category":"section"},{"location":"chap4/optimization/#Optimization","page":"Optimization","title":"Optimization","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"A general continuous optimization problem has the following form","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"min_mathbf xf(mathbf x)text subject to certian constraints","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The constraints may be either equality or inequality constraints.","category":"page"},{"location":"chap4/optimization/#Gradient-free-optimization","page":"Optimization","title":"Gradient free optimization","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"Gradient-free optimizers are optimization algorithms that do not rely on the gradient of the objective function to find the optimal solution. Instead, they use other methods such as Bayesian optimization, Nelder-Mead algorithm, genetic algorithms, or simulated annealing to explore the search space and find the optimal solution. These methods are particularly useful when the objective function is non-differentiable or when the gradient is difficult to compute. However, gradient-free optimizers can be slower and less efficient than gradient-based methods, especially when the search space is high-dimensional.","category":"page"},{"location":"chap4/optimization/#Golden-section-search","page":"Optimization","title":"Golden section search","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The golden section search is a simple optimization algorithm that can be used to find the minimum of a unimodal function. A unimodal function is a function that has a single minimum within a given interval. The golden section search algorithm works by iteratively narrowing down the search interval until the minimum is found with a specified tolerance.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The Julia implementation of the golden section search algorithm is as follows:","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"function golden_section_search(f, a, b; tol=1e-5)\n    τ = (√5 - 1) / 2  # golden ratio\n    x1 = a + (1 - τ) * (b - a)\n    x2 = a + τ * (b - a)\n    f1, f2 = f(x1), f(x2)\n    k = 0\n    while b - a > tol\n        k += 1\n        if f1 > f2\n            a = x1\n            x1 = x2\n            f1 = f2\n            x2 = a + τ * (b - a)  # update x2\n            f2 = f(x2)\n        else\n            b = x2\n            x2 = x1\n            f2 = f1\n            x1 = a + (1 - τ) * (b - a)  # update x1\n            f1 = f(x1)\n        end\n    end\n    return f1 < f2 ? (a, f1) : (b, f2)\nend;\n\ngolden_section_search(x->(x-4)^2, -5, 5; tol=1e-5)","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"It converges to the minimum in O(log(fracb-aepsilon)) iterations, where epsilon is the tolerance.","category":"page"},{"location":"chap4/optimization/#The-downhill-simplex-method-the-one-dimensional-case","page":"Optimization","title":"The downhill simplex method - the one-dimensional case","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The downhill simplex method, also known as the Nelder-Mead method, is a popular optimization algorithm that does not require the gradient of the objective function. It is a heuristic algorithm that iteratively constructs a simplex (a geometric shape with n+1 vertices in n dimensions) and updates the vertices of the simplex to minimize the objective function. The algorithm is based on the concept of \"downhill\" movement, where the simplex moves towards the minimum of the objective function by iteratively evaluating the function at different points in the search space.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The following is the basic idea of the one-dimensional downhill simplex method:","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"Initialize a one dimensional simplex, evaluate the function at the end points x_1 and x_2 and assume f(x_2)  f(x_1).\nEvaluate the function at x_c = 2x_1 - x_2.\nSelect one of the folloing operations\nIf f(x_c) is smaller than f(x_1), flip the simplex by doing x_1 leftarrow x_c and x_2 leftarrow x_1.\nIf f(x_c) is larger than f(x_1), but smaller than f(x_2), then x_2leftarrow x_c, goto case 3.\nIf f(x_c) is larger than f(x_2), then shrink the simplex: evaluate f on x_dleftarrow (x_1 + x_2)2, if it is larger than f(x_1), then x_2 leftarrow x_d, otherwise x_1leftarrow x_d x_2leftarrow x_1.\nRepeat step 2-3 until convergence.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"function simplex1d(f, x1, x2; tol=1e-6)\n    # initial 1D simplex with two points\n    history = [[x1, x2]]\n    f1, f2 = f(x1), f(x2)\n    while abs(x2 - x1) > tol\n        xc = 2x1 - x2\n        fc = f(xc)\n        if fc < f1   # flip\n            x1, f1, x2, f2 = xc, fc, x1, f1\n        else         # shrink\n            if fc < f2   # let the smaller one be x2.\n                x2, f2 = xc, fc\n            end\n            xd = (x1 + x2) / 2\n            fd = f(xd)\n            if fd < f1   # update x1 and x2\n                x1, f1, x2, f2 = xd, fd, x1, f1\n            else\n                x2, f2 = xd, fd\n            end\n        end\n        push!(history, [x1, x2])\n    end\n    return x1, f1, history\nend\n\nsimplex1d(x -> (x-1)^2, -1.0, 6.0) # optimize a simple quadratic function","category":"page"},{"location":"chap4/optimization/#The-Nelder-Mead-method","page":"Optimization","title":"The Nelder-Mead method","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The Nelder-Mead method for multidimensional optimization is a generalization of the one-dimensional downhill simplex method to higher dimensions. The algorithm constructs an n-dimensional simplex in the search space and iteratively updates the vertices of the simplex to minimize the objective function. The algorithm is based on the concept of \"reflection,\" \"expansion,\" \"contraction,\" and \"shrink\" operations on the simplex to explore the search space efficiently.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"Here is a Julia implementation:","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"function simplex(f, x0; tol=1e-6, maxiter=1000)\n    n = length(x0)\n    x = zeros(n+1, n)\n    fvals = zeros(n+1)\n    x[1,:] = x0\n    fvals[1] = f(x0)\n    alpha = 1.0\n    beta = 0.5\n    gamma = 2.0\n    for i in 1:n\n        x[i+1,:] = x[i,:]\n        x[i+1,i] += 1.0\n        fvals[i+1] = f(x[i+1,:])\n    end\n    history = [x]\n    for iter in 1:maxiter\n        # Sort the vertices by function value\n        order = sortperm(fvals)\n        x = x[order,:]\n        fvals = fvals[order]\n        # Calculate the centroid of the n best vertices\n        xbar = dropdims(sum(x[1:n,:], dims=1) ./ n, dims=1)\n        # Reflection\n        xr = xbar + alpha*(xbar - x[n+1,:])\n        fr = f(xr)\n        if fr < fvals[1]\n            # Expansion\n            xe = xbar + gamma*(xr - xbar)\n            fe = f(xe)\n            if fe < fr\n                x[n+1,:] = xe\n                fvals[n+1] = fe\n            else\n                x[n+1,:] = xr\n                fvals[n+1] = fr\n            end\n        elseif fr < fvals[n]\n            x[n+1,:] = xr\n            fvals[n+1] = fr\n        else\n            # Contraction\n            if fr < fvals[n+1]\n                xc = xbar + beta*(x[n+1,:] - xbar)\n                fc = f(xc)\n                if fc < fr\n                    x[n+1,:] = xc\n                    fvals[n+1] = fc\n                else\n                    # Shrink\n                    for i in 2:n+1\n                        x[i,:] = x[1,:] + beta*(x[i,:] - x[1,:])\n                        fvals[i] = f(x[i,:])\n                    end\n                end\n            else\n                # Shrink\n                for i in 2:n+1\n                    x[i,:] = x[1,:] + beta*(x[i,:] - x[1,:])\n                    fvals[i] = f(x[i,:])\n                end\n            end\n        end\n        push!(history, x)\n        # Check for convergence\n        if maximum(abs.(x[2:end,:] .- x[1,:])) < tol && maximum(abs.(fvals[2:end] .- fvals[1])) < tol\n            break\n        end\n    end\n    # Return the best vertex and function value\n    bestx = x[1,:]\n    bestf = fvals[1]\n    return (bestx, bestf, history)\nend","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The simplex function takes three arguments: the objective function f, the initial guess x0, and optional arguments for the tolerance tol and maximum number of iterations maxiter.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The algorithm initializes a simplex (a high dimensional triangle) with n+1 vertices, where n is the number of dimensions of the problem. The vertices are initially set to x0 and x0 + h_i, where h_i is a small step size in the ith dimension. The function values at the vertices are also calculated.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The algorithm then iteratively performs reflection, expansion, contraction, and shrink operations on the simplex until convergence is achieved. The best vertex and function value are returned.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"We use the Rosenbrock function as the test function.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"function rosenbrock(x)\n    (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\nend","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"using CairoMakie\nx = -2:0.01:2\ny = -2:0.01:2\nf = [rosenbrock((a, b)) for b in y, a in x]\nfig = Figure()\nax = Axis(fig[1, 1]; xlabel=\"x₁\", ylabel=\"x₂\")\nheatmap!(ax, x, y, log.(f))\ncontour!(ax, x, y, f; levels=exp.(-2:2:7), labels=true, color=\"white\", lw=0.5)\nscatter!(ax, [1.0], [1.0]; color=\"red\", marker=:star, markersize=5)\nCairoMakie.text!(ax, 1.02, 0.8; text=\"Minimum at (1, 1)\", color=\"black\")\nfig","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"bestx, bestf, history = simplex(rosenbrock, [-1.2, -1.0]; tol=1e-3)\nbestx, bestf","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The optimization process can be visualized by plotting the simplex at each iteration.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"<video width=\"560\" height=\"480\" controls>\n  <source src=\"../../assets/images/simplex.mp4\" type=\"video/mp4\">\n</video>","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"Optim.jl is a Julia package for optimization algorithms. It provides a common interface for various optimization algorithms, including the Nelder-Mead method.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"using Optim\n# Set the initial guess\nx0 = [-1, -1.0]\n# Set the optimization options\noptions = Optim.Options(iterations = 1000)\n# Optimize the Rosenbrock function using the simplex method\nresult = optimize(rosenbrock, x0, NelderMead(), options)\n# Print the optimization result\nresult.minimizer, result.minimum","category":"page"},{"location":"chap4/optimization/#Gradient-based-optimization","page":"Optimization","title":"Gradient based optimization","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"Consider a differentiable function f R^n rightarrow R, the gradient of f is defined as","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"nabla f(mathbfx) = left(beginmatrix\nfracpartial f(mathbfx)partial x_1\nfracpartial f(mathbfx)partial x_2\nvdots\nfracpartial f(mathbfx)partial x_n\nendmatrixright)","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"Gradient descent is a first-order optimization algorithm that is used to find the minimum of a function. It works by iteratively moving in the direction of the negative gradient of the function at each point until convergence is achieved. The learning rate is a hyperparameter that determines the step size of the update at each iteration. At the first-order approximation, the gradient descent method can be understood as follows:","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"f(mathbfx - epsilon nabla f(mathbf x)) approx f(mathbf x) - epsilon nabla f(mathbf x)^T nabla f(mathbf x) = f(mathbf x) - epsilon nabla f(mathbf x)_2  f(mathbfx)","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The loss function is reduced at each iteration.","category":"page"},{"location":"chap4/optimization/#Simple-gradient-descent","page":"Optimization","title":"Simple gradient descent","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The update rule of gradient descent is","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"beginalign*\ntheta_t+1 = theta_t - alpha g_t\nendalign*","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"where ","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"theta_t is the values of variables at time step t.\ng_t is the gradient at time t along theta_t, i.e. nabla_theta_t f(theta_t).\nalpha is the learning rate.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"In the following example, we optimize the Rosenbrock function using the gradient descent method.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"using ForwardDiff  # forward mode automatic differentiation\nForwardDiff.gradient(rosenbrock, [1.0, 3.0])\n\nfunction gradient_descent(f, x; niters::Int, learning_rate::Real)\n    history = [x]\n    for i=1:niters\n        g = ForwardDiff.gradient(f, x)\n        x -= learning_rate * g\n        push!(history, x)\n    end\n    return history\nend\n\nx0 = [-1, -1.0]\nhistory = gradient_descent(rosenbrock, x0; niters=10000, learning_rate=0.002)\nhistory[end], rosenbrock(history[end])","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"In the above example, we use the ForwardDiff package to compute the gradient of the Rosenbrock function. The gradient_descent function implements the gradient descent algorithm with a specified number of iterations and learning rate.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The training history can be visualized by plotting the loss function and the optimization path.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"function show_history(history)\n    x = -2:0.01:2\n    y = -2:0.01:2\n    f = [rosenbrock((a, b)) for b in y, a in x]\n    fig = Figure()\n    ax = Axis(fig[1, 1]; xlabel=\"x₁\", ylabel=\"x₂\", limits=(-2, 2, -2, 2))\n    hm = heatmap!(ax, x, y, log.(f); label=\"log(f)\")\n    lines!(ax, getindex.(history, 1), getindex.(history, 2); color=\"white\")\n    scatter!(ax, getindex.(history, 1), getindex.(history, 2); color=\"red\", markersize=3)\n    CairoMakie.text!(ax, -1.8, 1.5; text=\"Minimum loss = $(rosenbrock(history[end]))\", color=\"black\")\n    CairoMakie.text!(ax, -1.8, 1.3; text=\"Steps = $(length(history))\", color=\"black\")\n    Colorbar(fig[1, 2], hm)\n    fig\nend\n\n# plot\nshow_history(history)","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The main drawback of the simple gradient descent method is that it can be slow to converge, especially for functions with complex or high-dimensional surfaces. It can also get stuck in local minima and saddle points.","category":"page"},{"location":"chap4/optimization/#Gradient-descent-with-momentum","page":"Optimization","title":"Gradient descent with momentum","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"We can add a \"momentum\" term to the weight update, which helps the optimization algorithm to move more quickly in the right direction and avoid getting stuck in local minima.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The intuition behind the momentum method can be understood by considering a ball rolling down a hill. Without momentum, the ball would roll down the hill and eventually come to a stop at the bottom. However, with momentum, the ball would continue to roll past the bottom of the hill and up the other side, before eventually coming to a stop at a higher point. This is because the momentum of the ball helps it to overcome small bumps and obstacles in its path and continue moving in the right direction.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The update rule of gradient descent with momentum is","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"beginalign*\nv_t+1 = beta v_t - alpha g_t\ntheta_t+1 = theta_t + v_t+1\nendalign*","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"where ","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"g_t is the gradient at time t along theta_t, i.e. nabla_theta_t f(theta_t).\nalpha is the initial learning rate.\nbeta is the parameter for the gradient accumulation.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"function gradient_descent_momentum(f, x; niters::Int, β::Real, learning_rate::Real)\n    history = [x]\n    v = zero(x)\n    for i=1:niters\n        g = ForwardDiff.gradient(f, x)\n        v = β .* v .- learning_rate .* g\n        x += v\n        push!(history, x)\n    end\n    return history\nend\n\nx0 = [-1, -1.0]\nhistory = gradient_descent_momentum(rosenbrock, x0; niters=10000, learning_rate=0.002, β=0.5)\n\n# plot\nshow_history(history)","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"We can see the optimization path is more direct and faster than the simple gradient descent method. However, the momentum method may overshoot the minimum and oscillate around it.","category":"page"},{"location":"chap4/optimization/#Adaptive-Gradient-Algorithm-(Adagrad)","page":"Optimization","title":"Adaptive Gradient Algorithm (Adagrad)","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"AdaGrad is an optimization algorithm used in machine learning for solving convex optimization problems. It is a gradient-based algorithm that adapts the learning rate for each parameter based on the historical gradient information. The main idea behind AdaGrad is to give more weight to the parameters that have a smaller gradient magnitude, which allows for a larger learning rate for those parameters.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The update rule of AdaGrad is","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"beginalign*\n    r_t = r_t + g_t^2\n    mathbfeta = fracalphasqrtr_t + epsilon\n    theta_t+1 = theta_t - eta odot g_t\nendalign*","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"where ","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"theta_t is the values of variables at time t.\nalpha is the initial learning rate.\ng_t is the gradient at time t along theta_t\nr_t is the historical squared gradient sum, which is initialized to 0.\nepsilon is a small positive number.\nodot is the element-wise multiplication.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"function adagrad_optimize(f, x; niters, learning_rate, ϵ=1e-8)\n    rt = zero(x)\n    η = zero(x)\n    history = [x]\n    for step in 1:niters\n        Δ = ForwardDiff.gradient(f, x)\n        @. rt = rt + Δ .^ 2\n        @. η = learning_rate ./ sqrt.(rt + ϵ)\n        x = x .- Δ .* η\n        push!(history, x)\n    end\n    return history\nend\n\nx0 = [-1, -1.0]\nhistory = adagrad_optimize(rosenbrock, x0; niters=10000, learning_rate=1.0)\n\n# plot\nshow_history(history)","category":"page"},{"location":"chap4/optimization/#Adaptive-Moment-Estimation-(Adam)","page":"Optimization","title":"Adaptive Moment Estimation (Adam)","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The Adam optimizer is a popular optimization algorithm used in deep learning for training neural networks. It stands for Adaptive Moment Estimation and is a variant of stochastic gradient descent (SGD) that is designed to be more efficient and effective in finding the optimal weights for the neural network.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The Adam optimizer maintains a running estimate of the first and second moments of the gradients of the weights with respect to the loss function. These estimates are used to adaptively adjust the learning rate for each weight parameter during training. The first moment estimate is the mean of the gradients, while the second moment estimate is the uncentered variance of the gradients.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The Adam optimizer combines the benefits of two other optimization algorithms: AdaGrad, which adapts the learning rate based on the historical gradient information, and RMSProp, which uses a moving average of the squared gradients to scale the learning rate.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The Adam optimizer has become a popular choice for training deep neural networks due to its fast convergence and good generalization performance. It is widely used in many deep learning frameworks, such as TensorFlow, PyTorch, and Keras.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The update rule of Adam is","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"beginalign*\nv_t = beta_1 v_t-1 - (1-beta_1) g_t\ns_t = beta_2 s_t-1 - (1-beta_2) g^2\nhat v_t = v_t  (1-beta_1^t)\nhat s_t = s_t  (1-beta_2^t)\ntheta_t+1 = theta_t -eta frachat v_tsqrthat s_t + epsilon\nendalign*","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"where","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"theta_t is the values of variables at time t.\neta is the initial learning rate.\ng_t is the gradient at time t along theta.\nv_t is the exponential average of gradients along theta.\ns_t is the exponential average of squares of gradients along theta.\nbeta_1 beta_2 are hyperparameters.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"function adam_optimize(f, x; niters, learning_rate, β1=0.9, β2=0.999, ϵ=1e-8)\n    mt = zero(x)\n    vt = zero(x)\n    βp1 = β1\n    βp2 = β2\n    history = [x]\n    for step in 1:niters\n        Δ = ForwardDiff.gradient(f, x)\n        @. mt = β1 * mt + (1 - β1) * Δ\n        @. vt = β2 * vt + (1 - β2) * Δ^2\n        @. Δ =  mt / (1 - βp1) / (√(vt / (1 - βp2)) + ϵ) * learning_rate\n        βp1, βp2 = βp1 * β1, βp2 * β2\n        x = x .- Δ\n        push!(history, x)\n    end\n    return history\nend\n\nx0 = [-1, -1.0]\nhistory = adam_optimize(rosenbrock, x0; niters=10000, learning_rate=0.01)\n\n# plot\nshow_history(history)","category":"page"},{"location":"chap4/optimization/#More-gradient-based-optimizers","page":"Optimization","title":"More gradient based optimizers","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The Julia package Optimisers.jl contains various optimization algorithms for differentiable functions.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"import Optimisers\n\nx0 = [-1, -1.0]\nmethod = Optimisers.RMSProp(0.01)\nstate = Optimisers.setup(method, x0)\nhistory = [x0]\nfor i=1:10000\n    global x0, state\n    grad = ForwardDiff.gradient(rosenbrock, x0)\n    state, x0 = Optimisers.update(state, x0, grad)\n    push!(history, x0)\nend\n\n# plot\nshow_history(history)","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"Optimisers.jl documentation contains stochastic gradient based optimizers.","category":"page"},{"location":"chap4/optimization/#Hessian-based-optimizers","page":"Optimization","title":"Hessian based optimizers","text":"","category":"section"},{"location":"chap4/optimization/#Newton's-Method","page":"Optimization","title":"Newton's Method","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"Newton's method is an optimization algorithm used to find the roots of a function, which can also be used to find the minimum or maximum of a function. The method involves using the first and second derivatives of the function to approximate the function as a quadratic function and then finding the minimum or maximum of this quadratic function. The minimum or maximum of the quadratic function is then used as the next estimate for the minimum or maximum of the original function, and the process is repeated until convergence is achieved.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"beginalign*\n H_kp_k=-g_k\n x_k+1=x_k+p_k\nendalign*","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"where","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"g_k is the gradient at time k along x_k.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"function newton_optimizer(f, x; tol=1e-5)\n    k = 0\n    history = [x]\n    while k < 1000\n        k += 1\n        gk = ForwardDiff.gradient(f, x)\n        hk = ForwardDiff.hessian(f, x)\n        dx = -hk \\ gk\n        x += dx\n        push!(history, x)\n        sum(abs2, dx) < tol && break\n    end\n    return history\nend\n\nx0 = [-1, -1.0]\nhistory = newton_optimizer(rosenbrock, x0; tol=1e-5)\n\n# plot\nshow_history(history)","category":"page"},{"location":"chap4/optimization/#The-Broyden–Fletcher–Goldfarb–Shanno-(BFGS)-algorithm","page":"Optimization","title":"The Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The Hessians are expensive to compute, and the inversion of the Hessian is also expensive. In practice, the Hessian matrix is often approximated using the BFGS algorithm, which is a quasi-Newton method that updates an approximation of the Hessian matrix at each iteration.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The BFGS method is a popular numerical optimization algorithm used to solve unconstrained optimization problems. It is an iterative method that seeks to find the minimum of a function by iteratively updating an estimate of the inverse Hessian matrix of the function.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"beginalign*\n B_kp_k=-g_ktext Newton method like update rule\n alpha_k = rm argmin f(x + alpha p_k)text using line search\n s_k=alpha_kp_k\n x_k+1=x_k+s_k\ny_k=g_k+1-g_k\nB_k+1=B_k+frac y_ky_k^mathrm T y_k^mathrm T s_k-frac B_ks_ks_k^mathrm T B_k^mathrm T s_k^mathrm T B_ks_k\nendalign*","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"where","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"B_k is an approximation of the Hessian matrix, which is intialized to identity.\ng_k is the gradient at time k along x_k.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"We can show B_k+1s_k = y_k (secant equation) is satisfied.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"# Set the initial guess\nx0 = [-1.0, -1.0]\n# Set the optimization options\noptions = Optim.Options(iterations = 1000, store_trace=true, extended_trace=true)\n# Optimize the Rosenbrock function using the BFGS method\nresult = optimize(rosenbrock, x->ForwardDiff.gradient(rosenbrock, x), x0, BFGS(), options, inplace=false)\n# Print the optimization result\nshow_history([t.metadata[\"x\"] for t in result.trace])","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"The L-BFGS algorithm is a limited-memory variant of the BFGS algorithm that uses a limited amount of memory to store the approximation of the Hessian matrix. It is often used for large-scale optimization problems where the full Hessian matrix is too expensive to compute and store. In the following example, we use the L-BFGS algorithm to optimize the Rosenbrock function.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"using Enzyme\nfunction g!(G, x)\n    G[1:length(x)]=gradient(Enzyme.Reverse, rosenbrock, x)\nend\nx0 = [-1, -1.0]\na = optimize(rosenbrock, g!, x0, LBFGS())\na.minimizer, a.minimum","category":"page"},{"location":"chap4/optimization/#Linear-programming","page":"Optimization","title":"Linear programming","text":"","category":"section"},{"location":"chap4/optimization/#Convex-optimization","page":"Optimization","title":"Convex optimization","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"A set Ssubseteq mathbbR^n is convex if it contains the line segment between any two of its points, i.e.,","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"alpha mathbfx + (1-alpha)mathbfy 0leq alpha leq 1 subseteq S","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"for all mathbfx mathbfy in S.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"A function f S in R^n rightarrow R is convex on a convex set S if its graph along any line segment in S lies on or blow the chord connecting the function values at the endpoints of the segment, i.e., if","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"f(alpha mathbfx + (1-alpha) mathbfy) leq alpha f(mathbfx) + (1+alpha)f(mathbfy)","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"for all alpha in 0 1 and all mathbfx mathbfyin S.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"Any local minimum of a convex function f on a convex set Ssubseteq mathbbR^n is a global minimum of f on S.","category":"page"},{"location":"chap4/optimization/#Linear-programming-and-integer-programming","page":"Optimization","title":"Linear programming and integer programming","text":"","category":"section"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"Linear programming is a method to achieve the best outcome in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (mathematical optimization). It can be formulated as","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"beginalignedtextFind a vectormathbf x textthat maximizesmathbf c ^Tmathbf x textsubject toAmathbf x leq mathbf b textandmathbf x geq mathbf 0 endaligned","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"where","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"mathbfx is the vector of variables to be determined.\nmathbfc is the vector of coefficients of the objective function.\nA is the matrix of coefficients of the constraints, which is required to be non-negative.\nmathbfb is the vector.","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"Integer programming is a generalization of the linear programming where some or all of the variables are required to be integers. Unlike linear programming, which can be solved efficiently in polynomial time, integer programming is NP-hard and computationally intractable in general. However, there are efficient algorithms for solving special cases of integer programming, such as binary integer programming, where the variables are restricted to be binary (0 or 1).","category":"page"},{"location":"chap4/optimization/","page":"Optimization","title":"Optimization","text":"For examples, please refer to the JuMP.jl documentation.","category":"page"},{"location":"chap5/montecarlo/#Markov-Chain-Monte-Carlo","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"","category":"section"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"In this chapter, we will introduce the concept of Markov Chain Monte Carlo (MCMC) methods. MCMC methods are a class of algorithms that are used to sample from a probability distribution. They are particularly useful when the distribution is high-dimensional and it is difficult to sample from it directly. MCMC methods are widely used in Bayesian statistics, machine learning, and other fields.","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"In physics, MCMC methods are often used to sample from the Boltzmann distribution of a system. This is useful for studying the equilibrium properties of the system, such as the energy, magnetization, and other thermodynamic quantities. In this chapter, we will focus on the Ferromagnetic Ising Model, which is a simple model of a magnetic system. We will use MCMC methods to sample from the Boltzmann distribution of the Ising model and study its properties.","category":"page"},{"location":"chap5/montecarlo/#Ferromagnetic-Ising-Model","page":"Markov Chain Monte Carlo","title":"Ferromagnetic Ising Model","text":"","category":"section"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"The Ferromagnetic Ising Model is a simple model of a magnetic system. It consists of a lattice of spins, which can be in one of two states: up or down. The energy of the system is given by the Hamiltonian:","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"H = -J sum_langle i j rangle s_i s_j - h sum_i s_i","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"where the first sum is over pairs of neighboring spins, s_i and s_j, J is the coupling constant, and h is the external magnetic field. The spins interact with each other through the first term, which favors alignment of neighboring spins. The second term represents the interaction of the spins with the external magnetic field.","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"The probability distribution of the Ising model is given by the Boltzmann distribution:","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"P(s) = frac1Z e^-beta H(s)","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"where s is the configuration of spins, Z is the partition function, beta = 1(k_B T) is the inverse temperature, and H(s) is the Hamiltonian of the system. The partition function is given by:","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"Z = sum_s e^-beta H(s)","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"where the sum is over all possible configurations of spins.","category":"page"},{"location":"chap5/montecarlo/#Metropolis-Hastings-Algorithm","page":"Markov Chain Monte Carlo","title":"Metropolis-Hastings Algorithm","text":"","category":"section"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"note: Why importance sampling?\nConsider a target function f(x) that to be integrated over a domain D. The integral is given by:I = int_D f(x) dxIf we can sample from a probability distribution p(x), we can estimate the integral by averaging over the samples:I = int_D fracf(x)p(x) p(x)dx approx frac1N sum_i=1^N fracf(x_i)p(x_i)where x_i are the samples drawn from the distribution p(x). It can be shown that the standard deviation of the estimation scales as 1sqrtN, and the result converges the faster the better the importance sampling distribution p(x) matches the target function f(x).","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"The Metropolis-Hastings algorithm is a popular importance sampling method that is used to sample from a probability distribution. It works by constructing a Markov chain that has the desired distribution as its stationary distribution. The algorithm proceeds as follows:","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"Start with an initial configuration of spins s.\nPropose a new configuration s by flipping the spin of a randomly chosen site.\nCalculate the change in energy Delta E = H(s) - H(s).\nIf Delta E  0, accept the new configuration s with probability 1.\nIf Delta E  0, accept the new configuration s with probability e^-beta Delta E.\nRepeat steps 2-5 for a large number of iterations to sample from the distribution.","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"To study the equilibrium properties from the generated samples such as the energy, magnetization, and other thermodynamic quantities, we can use the generated samples to calculate the expectation value of the observable of interest. For example, the expectation value of an operator A is given by:","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"langle A rangle = sum_s A(s) P(s) = frac1Z sum_s A(s) e^-beta H(s)","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"where P(s) is the probability distribution of the system. In practice, we can estimate the expectation value by averaging over the generated samples:","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"frac1M sum_i=1^M A(s_i)","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"where M is the number of samples and s_i is the i-th sample. In this model, quantities of interest include","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"Energy per site: E = frac1N sum_i=1^N H_i, where N is the number of spins.\nEnergy squared per site: E^2 = frac1N sum_i=1^N H_i^2.\nMagnetization per site: M = frac1N sum_i=1^N s_i.\nMagnetization squared per site: M^2 = frac1N sum_i=1^N s_i^2.\nMagnetization quartic per site: M^4 = frac1N sum_i=1^N s_i^4, which is used to study the Binder ratio.","category":"page"},{"location":"chap5/montecarlo/#Ergodicity-and-Detailed-Balance","page":"Markov Chain Monte Carlo","title":"Ergodicity and Detailed Balance","text":"","category":"section"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"The Metropolis-Hastings algorithm satisfies two important properties: ergodicity and detailed balance. Ergodicity means that the Markov chain can reach any state in the state space with a non-zero probability. Detailed balance means that the transition probabilities satisfy the condition:","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"P(s to s) P(s) = P(s to s) P(s)","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"where P(s to s) is the probability of transitioning from state s to state s, and P(s) is the probability of being in state s. The Metropolis-Hastings algorithm satisfies detailed balance by construction, which ensures that the stationary distribution of the Markov chain is the desired distribution.","category":"page"},{"location":"chap5/montecarlo/#Implementation","page":"Markov Chain Monte Carlo","title":"Implementation","text":"","category":"section"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"Our implementation of the Metropolis-Hastings algorithm for solving the Ising model is based on Anders Sandvik's lecture note[Sandvik]. We use a 2D square lattice with periodic boundary conditions and initialize the lattice with random spins. We then run the Metropolis-Hastings algorithm for a large number of iterations to sample from the Boltzmann distribution. The source code is also available in the demo repository. Both simple update and Swendsen-Wang's cluster update[Swendsen1987] are implemented.","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"# required interfaces: num_spin, energy\nabstract type AbstractSpinModel end\n\n# IsingSpinModel: a struct that represents the Ising model\nstruct IsingSpinModel{RT} <: AbstractSpinModel\n    l::Int  # lattice size\n    h::RT   # magnetic field\n    beta::RT  # inverse temperature 1/T\n    pflp::NTuple{10, RT}  # precomputed flip probabilities\n    neigh::Matrix{Int}  # neighbors\nend\nfunction IsingSpinModel(l::Int, h::RT, beta::RT) where RT\n    pflp = ([exp(-2*s*(i + h) * beta) for s=-1:2:1, i in -4:2:4]...,)\n    neigh = lattice(l)\n    IsingSpinModel(l, h, beta, pflp, neigh)\nend\n\n# Constructs a list neigh[1:4,1:nn] of neighbors of each site\nfunction lattice(ll)\n    lis = LinearIndices((ll, ll))\n    return reshape([lis[mod1(ci.I[1]+di, ll), mod1(ci.I[2]+dj, ll)] for (di, dj) in ((1, 0), (0, 1), (-1, 0), (0, -1)), ci in CartesianIndices((ll, ll))], 4, ll*ll)\nend\n\n# Returns the number of spins in the model\nnum_spin(model::IsingSpinModel) = model.l^2\n\n# Computes the energy of the system\nenergy(model::IsingSpinModel, spin) = ferromagnetic_energy(model.neigh, model.h, spin)\nfunction ferromagnetic_energy(neigh::AbstractMatrix, h::Real, spin::AbstractMatrix)\n    @boundscheck size(neigh) == (4, length(spin))\n    sum(1:length(spin)) do i\n        s = spin[i]\n        - s * (spin[neigh[1, i]] + spin[neigh[2, i]] + h)\n    end\nend\n\n# Computes the probability of flipping a spin\n@inline function pflip(model::IsingSpinModel, s::Integer, field::Integer)\n    return @inbounds model.pflp[(field + 5) + (1 + s) >> 1]\nend\n\n# Updates the spin configuration using the Metropolis-Hastings algorithm\nfunction mcstep!(model::IsingSpinModel, spin)\n    nn = num_spin(model)\n    @inbounds for _ = 1:nn\n        s = rand(1:nn)\n        field = spin[model.neigh[1, s]] + spin[model.neigh[2, s]] + spin[model.neigh[3, s]] + spin[model.neigh[4, s]]\n        if rand() < pflip(model, spin[s], field)\n           spin[s] = -spin[s]\n        end\n    end    \nend\n\n# Simulation result\nstruct SimulationResult{RT}\n    nbins::Int  # number of bins\n    nsteps_eachbin::Int  # number of steps in each bin\n    current_bin::Base.RefValue{Int}  # current bin\n    energy::Vector{RT}  # energy/spin\n    energy2::Vector{RT}  # (energy/spin)^2\n    m::Vector{RT}  # |m|\n    m2::Vector{RT}  # m^2\n    m4::Vector{RT}  # m^4\nend\nSimulationResult(nbins, nsteps_eachbin) = SimulationResult(nbins, nsteps_eachbin, Ref(0), zeros(nbins), zeros(nbins), zeros(nbins), zeros(nbins), zeros(nbins))\n\n# Measures the energy and magnetization of the system\nfunction measure!(result::SimulationResult, model::AbstractSpinModel, spin)\n    @boundscheck checkbounds(result.energy, result.current_bin[])\n    m = sum(spin)\n    e = energy(model, spin)\n    n = num_spin(model)\n    k = result.current_bin[]\n    @inbounds result.energy[k] += e/n\n    @inbounds result.energy2[k] += (e/n)^2\n    @inbounds result.m[k] += abs(m/n)\n    @inbounds result.m2[k] += (m/n)^2\n    @inbounds result.m4[k] += (m/n)^4\nend\n\n# Simulates the Ising model and measures the energy and magnetization of the system\nfunction simulate!(model::IsingSpinModel, spin; nsteps_heatbath, nsteps_eachbin, nbins)\n    # heat bath\n    for _ = 1:nsteps_heatbath\n        mcstep!(model, spin)    \n    end\n    result = SimulationResult(nbins, nsteps_eachbin)\n    for j=1:nbins\n        result.current_bin[] = j\n        for _ = 1:nsteps_eachbin\n            mcstep!(model, spin)\n            measure!(result, model, spin)\n        end\n    end\n    return result\nend","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"The following code snippet demonstrates how to use the Ising model implementation to simulate the Ising model and measure the energy and magnetization of the system.","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"using CairoMakie\n# an example for testing\nlattice_size = 100\ntemperature = 2.0\nmagnetic_field = 0.0\nmodel = IsingSpinModel(lattice_size, magnetic_field, 1/temperature)\n\n# Constructs the initial random spin configuration\nspin = rand([-1,1], model.l, model.l);\n\nnsteps_heatbath = 1000\nnsteps_eachbin = 100\nnbins = 100\nresult = simulate!(model, spin; nsteps_heatbath, nsteps_eachbin, nbins)","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"The result contains the energy and magnetization of the system for each bin. We can use this data to calculate the mean energy and magnetization of the system and study its properties. The following code visualizes the energy and magnetization of the system over time.","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"fig = Figure()\nax = Axis(fig[1, 1], xlabel=\"time\")\n\nfor (op, legend) in zip([:energy, :energy2, :m, :m2, :m4], [L\"energy/spin\", L\"(energy/spin)^2\", L\"|m|\", L\"m^2\", L\"m^4\"])\n    lines!(ax, getfield(result, op), label=legend)\nend\naxislegend(ax)\nfig","category":"page"},{"location":"chap5/montecarlo/#Phase-transition","page":"Markov Chain Monte Carlo","title":"Phase transition","text":"","category":"section"},{"location":"chap5/montecarlo/#Simple-update,-temperature-1.0","page":"Markov Chain Monte Carlo","title":"Simple update, temperature = 1.0","text":"","category":"section"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"We first set the temperature to 1.0, which is below the phase transition point T_c = 2269. The video below shows the evolution of the spins over update steps. The spins tend to align with each other due to the ferromagnetic interaction, resulting in large clusters of aligned spins.","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"temperature = 1.0\nmodel = IsingSpinModel(lattice_size, magnetic_field, 1/temperature)\n# animation\nfig = Figure()\nspin = rand([-1,1], model.l, model.l)\nspinobs = Observable(spin)\nax1 = Axis(fig[1, 1]; aspect = DataAspect()); hidedecorations!(ax1); hidespines!(ax1)  # hides ticks, grid and lables, and frame\nMakie.heatmap!(ax1, spinobs, camera=campixel!)\ntxt = Observable(\"t = 0\")\nMakie.text!(ax1, -30, lattice_size-10; text=txt, color=:black, fontsize=30, strokecolor=:white)\nfilename = joinpath(@__DIR__, \"ising-spins-$temperature.mp4\")\nrecord(fig, filename, 2:1000; framerate = 24) do i\n    mcstep!(model, spin)\n    spinobs[] = spin\n    txt[] = \"t = $(i-1)\"\nend","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"<video width=\"320\" height=\"240\" controls style=\"margin-bottom:30px\">\n  <source src=\"../../assets/images/ising-spins-1.0.mp4\" type=\"video/mp4\">\n</video>","category":"page"},{"location":"chap5/montecarlo/#Simple-update,-temperature-3.0","page":"Markov Chain Monte Carlo","title":"Simple update, temperature = 3.0","text":"","category":"section"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"Next, we set the temperature to 3.0, which is above the phase transition point. The video below shows the evolution of the spins over update steps. The spins are disordered and do not align with each other, resulting in small clusters of aligned spins.","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"<video width=\"320\" height=\"240\" controls style=\"margin-bottom:30px\">\n  <source src=\"../../assets/images/ising-spins-3.0.mp4\" type=\"video/mp4\">\n</video>\n<br>","category":"page"},{"location":"chap5/montecarlo/#Cluster-update,-temperature-1.0","page":"Markov Chain Monte Carlo","title":"Cluster update, temperature = 1.0","text":"","category":"section"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"The cluster update, or the Swendsen-Wang algorithm, is a more efficient way to update the spins in the Ising model. It works by grouping the spins into clusters of aligned spins and flipping the clusters with a certain probability. The implementation of the cluster update could be found in the demo repository. The video below shows the evolution of the spins using the cluster update. The spins align with each other more quickly compared to the simple update.","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"<video width=\"320\" height=\"240\" controls style=\"margin-bottom:30px\">\n  <source src=\"../../assets/images/swising-spins-1.0.mp4\" type=\"video/mp4\">\n</video>\n<br>","category":"page"},{"location":"chap5/montecarlo/#Cluster-update,-temperature-3.0","page":"Markov Chain Monte Carlo","title":"Cluster update, temperature = 3.0","text":"","category":"section"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"Similarly, we set the temperature to 3.0 and use the cluster update. The video below shows the evolution of the spins over update steps. The spins are disordered and do not align with each other, resulting in small clusters of aligned spins.","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"<video width=\"320\" height=\"240\" controls>\n  <source src=\"../../assets/images/swising-spins-3.0.mp4\" type=\"video/mp4\">\n</video>","category":"page"},{"location":"chap5/montecarlo/#References","page":"Markov Chain Monte Carlo","title":"References","text":"","category":"section"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"[Sandvik]: Lecture note: Monte Carlo simulations in classical statistical physics, Anders Sandvik (PDF)","category":"page"},{"location":"chap5/montecarlo/","page":"Markov Chain Monte Carlo","title":"Markov Chain Monte Carlo","text":"[Swendsen1987]: Swendsen, Robert H., and Jian-Sheng Wang. \"Nonuniversal critical dynamics in Monte Carlo simulations.\" Physical review letters 58.2 (1987): 86.","category":"page"},{"location":"append/plotting/#Plotting-recipes-with-CairoMakie","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"In this appendix, we have prepared a set of plotting scripts and simple tutorials to show how to generate different type of pictures, such as line plots, scatter plots, subplots, heatmaps, contour plots, colorbars, arrows, brackets, error bars, stream plots, and text. We will use the CairoMakie library, which is a high-performance, interactive plotting library for Julia. They could be installed by running the following command in the Julia REPL:","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"julia> ]add CairoMakie","category":"page"},{"location":"append/plotting/#Importing","page":"Plotting recipes with CairoMakie","title":"Importing","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"First, we should import CairoMakie libraries to start plotting.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"julia> using CairoMakie","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"Before we start, let's introduce some basic concepts in Makie.jl:","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"Figure: This is the top-level container for all the elements of your visualization. It can contain multiple plots, as well as other elements like legends, colorbars, etc.\nAxis: This is the actual plot, where your data is visualized. An axis can contain multiple graphical elements, like lines, scatter points, surfaces, etc. It also contains the x-axis and y-axis, which have scales (linear, logarithmic, etc.) and ticks. \nPlots: These are the graphical representations of your data. In Makie.jl, create a plot by adding graphical elements (like lines, scatter points, etc.) to an axis. Each type of plot is suited to represent a certain kind of data.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"In the next steps, we will take a look at how we can create these objects.","category":"page"},{"location":"append/plotting/#Line-Plot","page":"Plotting recipes with CairoMakie","title":"Line Plot","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"The following code create line plot with the CairoMakie library, including setting titles, labels, and legends.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\nx = range(0, 10, length=100)\nfig = Figure()\n# Create an axis with title and labels\nax = Axis(fig[1, 1], title = \"Line Plots\", xlabel = \"X\", ylabel = \"Y\") \n# Create a line plot, set color and label\nlines!(ax, x, sin.(x), color = :red, label = \"sin\") \n# Add another line plot to the same axis\nlines!(ax, x, cos.(x), color = :blue, label = \"cos\") \n# Add a lengend at the bottom right with label size 15\naxislegend(ax; position = :rb, labelsize = 15)\nsave(\"plot_lines6.png\", fig)\nfig","category":"page"},{"location":"append/plotting/#Error-Bars","page":"Plotting recipes with CairoMakie","title":"Error Bars","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"Error bars are graphical representations used in statistics and data visualization to indicate the standard deviation of data.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\n\n\nfig = Figure()\nax = Axis(fig[1, 1])\n\nxs = 0:0.5:10\nys = 0.5 .* sin.(xs)\n# Define the lower and upper errors for each point. \nlowerrors = fill(0.1, length(xs))\nhigherrors = LinRange(0.1, 0.4, length(xs))\n# Add error bars to the plot, with the color ranging from 0 to 1, and the width of the whiskers set to 10.\nlines!(ax, xs, ys, color = :blue)\nerrorbars!(ax, xs, ys, lowerrors, higherrors,\n    color = range(0, 1, length = length(xs)),\n    whiskerwidth = 10)\n\n# plot position scatters so low and high errors can be discriminated\nscatter!(xs, ys, markersize = 3, color = :black)\n\nfig","category":"page"},{"location":"append/plotting/#Texts","page":"Plotting recipes with CairoMakie","title":"Texts","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"In CairoMakie, text can be positioned at specific coordinates on the plot, aligned to different sides, and styled with different fonts, sizes, colors, and rotations.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\n\nfig = Figure()\nax = Axis(fig[1, 1])\n# Add the first line to the axis, with x ranging from 0 to 10 and y, and add a label\nlines!(0..10, x -> sin(3x) / (cos(x) + 2),\n    label = L\"\\frac{\\sin(3x)}{\\cos(x) + 2}\")\n# Add the second line to the axis, with x ranging from 0 to 10 and y, and add a label.\nlines!(0..10, x -> sin(x^2) / (cos(sqrt(x)) + 2),\n    label = L\"\\frac{\\sin(x^2)}{\\cos(\\sqrt{x}) + 2}\")\n# Add a legend to the figure\nLegend(fig[1, 2], ax)\n\nfig","category":"page"},{"location":"append/plotting/#Bracket","page":"Plotting recipes with CairoMakie","title":"Bracket","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"In the context of plotting in Julia with the CairoMakie library, a bracket can be added to a plot to highlight or annotate a specific range of values.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\n# Create a line plot of the sine function from 0 to 9, with the x and y grid lines turned off\nfig, ax, l = lines(0..9, sin; axis = (; xgridvisible = false, ygridvisible = false))\nylims!(ax, -1.5, 1.5)\n# Add a bracket to highlight the period length of the sine function, from (pi/2, 1) to (5pi/2, 1), with an offset of 5, and the text \"Period length\". The bracket style is square.\nbracket!(pi/2, 1, 5pi/2, 1, offset = 5, text = \"Period length\", style = :square)\n# Add a bracket to highlight the amplitude of the sine function, with the text \"Amplitude\". The bracket is oriented downwards, and the text is aligned to the right and centered vertically.\nbracket!(pi/2, 1, pi/2, -1, text = \"Amplitude\", orientation = :down,\n    linestyle = :dash, rotation = 0, align = (:right, :center), textoffset = 4, linewidth = 2, color = :red, textcolor = :red)\n# Add a bracket to highlight a falling portion of the sine function, from (2.3, sin(2.3)) to (4.0, sin(4.0)), with the text \"Falling\". The bracket is oriented upwards.\nbracket!(2.3, sin(2.3), 4.0, sin(4.0),\n    text = \"Falling\", offset = 10, orientation = :up, color = :purple, textcolor = :purple)\n# Add a bracket to highlight a rising portion of the sine function, from (5.5, sin(5.5)) to (7.0, sin(7.0)), with the text \"Rising\". The bracket is oriented downwards.\nbracket!(Point(5.5, sin(5.5)), Point(7.0, sin(7.0)),\n    text = \"Rising\", offset = 10, orientation = :down, color = :orange, textcolor = :orange, \n    fontsize = 30, textoffset = 30, width = 50)\nfig","category":"page"},{"location":"append/plotting/#Subplots","page":"Plotting recipes with CairoMakie","title":"Subplots","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"Subplots are a way to display multiple plots in different sub-regions of the same window. The following code demonstrates how to create multiple subplots using the CairoMakie library. It will generate a figure with three line plots, each representing the sin function, but with different colors (red, blue, and green).","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\nx = LinRange(0, 10, 100)\ny = sin.(x)\nfig = Figure()\n# Create an axis with title and labels\nax1 = Axis(fig[1, 1], title = \"Red Sin Plot\", xlabel = \"X\", ylabel = \"Y\") \nlines!(ax1, x, y, color = :red, label = \"sin\")\nax2 = Axis(fig[1, 2], title = \"Blue Sin Plot\", xlabel = \"X\", ylabel = \"Y\")\nlines!(ax2, x, y, color = :blue, label = \"sin\")\n# Create a third axis spanning the first two positions of the second row of the figure, set the title, x-axis label, and y-axis label\nax3 = Axis(fig[2, 1:2], title = \"Green Sin Plot\", xlabel = \"X\", ylabel = \"Y\") \nlines!(ax3, x, y, color = :green, label = \"sin\")\n\nfig","category":"page"},{"location":"append/plotting/#Scatter-Plot","page":"Plotting recipes with CairoMakie","title":"Scatter Plot","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"A scatter plot is a type of plot used to display the relationship between two variables, where each point represents an observation. The following code will generate a figure with two scatter plots, one representing the sin function and the other representing the cos function.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\n\nx = range(0, 10, length=100)\nfig = Figure()\n\n# Create an axis at the first position of the figure, set the title, x-axis label, and y-axis label\nax = Axis(fig[1, 1], title = \"Scatter Plots\", xlabel = \"X\", ylabel = \"Y\")\n\n# Create a scatter plot on the axis, set the color to red, marker size to 5, and label to \"sin\"\nscatter!(ax, x, sin.(x), color = :red, markersize = 5, label = \"sin\") \n\n# Add another scatter plot to the same axis, set the color to blue, marker size to 10, and label to \"cos\"\nscatter!(ax, x, cos.(x), color = :blue, markersize = 10, label = \"cos\") \n\n# Set the legend for the axis, position it at the bottom right, and set the label size to 15\naxislegend(ax; position = :rb, labelsize = 15)\n\nfig","category":"page"},{"location":"append/plotting/#Bar-plot","page":"Plotting recipes with CairoMakie","title":"Bar plot","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"A bar plot is a type of plot used to visualize categorical data. It consists of rectangular bars with lengths proportional to the values they represent. Bar plots are commonly used to compare the values of different categories or groups.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"The following code demonstrates how to create a bar plot using the CairoMakie library. It will generate a figure with a bar plot.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\n# Generate a color palette\ncolors = Makie.wong_colors()\n# Define the data for the bar plot\ntbl = (cat = [1, 1, 1, 2, 2, 2, 3, 3, 3],\n       height = 0.1:0.1:0.9,\n       grp = [1, 2, 3, 1, 2, 3, 1, 2, 3],\n       grp1 = [1, 2, 2, 1, 1, 2, 1, 1, 2],\n       grp2 = [1, 1, 2, 1, 2, 1, 1, 2, 1]\n       )\n    \n# Figure and Axis\nfig = Figure()\nax = Axis(fig[1,1], xticks = (1:3, [\"left\", \"middle\", \"right\"]),\n        title = \"Dodged bars with legend\")\n\n# Plot\nbarplot!(ax, tbl.cat, tbl.height,\n        dodge = tbl.grp,\n        color = colors[tbl.grp])\n\n# Define the labels for the legend\nlabels = [\"group 1\", \"group 2\", \"group 3\"]\n# # Create the elements for the legend with custom colors\nelements = [PolyElement(polycolor = colors[i]) for i in 1:length(labels)]\ntitle = \"Groups\"\n\nLegend(fig[1,2], elements, labels, title)\n\nfig","category":"page"},{"location":"append/plotting/#Heatmap","page":"Plotting recipes with CairoMakie","title":"Heatmap","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"A heatmap is a graphical representation of data where individual values contained in a matrix are represented as colors. It is a way of visualizing data density or intensity, making it easier to perceive patterns, trends, and outliers within large data sets.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"This following code is using CairoMakie to create a heatmap of the Mandelbrot set. The Mandelbrot set is a set of complex numbers for which the function f(c) = z^2 + c does not diverge when iterated from z = 0.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\nfig = Figure()\n# Create an axis with title and labels\nax = Axis(fig[1, 1], title = \"Heatmap\", xlabel = \"X\", ylabel = \"Y\") \n# The Mandelbrot function\nfunction mandelbrot(x, y)\n    z = c = x + y*im\n    for i in 1:30.0; abs(z) > 2 && return i; z = z^2 + c; end; 0\nend\n\nhm = heatmap!(ax, -2:0.001:1, -1.1:0.001:1.1, mandelbrot,\n    colormap = Reverse(:deep))\n\n# Add a colorbar to the right of the heatmap with the label \"Color scale\"\nColorbar(fig[1, 2], hm, label = \"Color scale\") \nfig","category":"page"},{"location":"append/plotting/#Contour-Plot","page":"Plotting recipes with CairoMakie","title":"Contour Plot","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"A contour plot is a graphical technique used to represent a 3-dimensional surface in two dimensions. It is like a topographical map in which x and y show the location, and the contour lines represent the third dimension (z) by their level.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"Each contour line in a contour plot represents a set of points at the same height or value. The contour plot provides a way to visualize the relationship between three continuous variables. The color or the line style often indicates the value of the third variable. The following code demonstrates how to create a contour plot using the CairoMakie library.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\n# Define the Himmelblau function\nhimmelblau(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2\nx = y = range(-6, 6; length=100)\n# Calculate z-values as the Himmelblau function for each combination of x and y\nz = himmelblau.(x, y')\n# Define the contour levels as powers of 10 from 0.3 to 3.5\nlevels = 10.0.^range(0.3, 3.5; length=10)\ncolorscale = ReversibleScale(x -> x^(1 / 10), x -> x^10)\n# Create a contour plot of the z-values, with labels, levels, a hsv colormap, and the defined color scale.\n\nfig = Figure()\nax = Axis(fig[1, 1])\n\nct = contour!(ax, x, y, z; labels=true, levels, colormap=:hsv, colorscale)\nfig","category":"page"},{"location":"append/plotting/#3D-Contour-Plot","page":"Plotting recipes with CairoMakie","title":"3D Contour Plot","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\n\nfig = Figure()\n# Create a 3D axis at the first position of the figure, set the aspect ratio and perspective\nAxis3(fig[1, 1], aspect=(0.5,0.5,1), perspectiveness=0.75)\n# Create a linear range of numbers from -0.5 to 0.5, with 100 steps for x and y axes\nxs = ys = LinRange(-0.5, 0.5, 100)\nzs = [sqrt(x^2+y^2) for x in xs, y in ys]\n# Create a 3D contour plot of the negative z-values\ncontour3d!(-zs, levels=-(.025:0.05:.475), linewidth=2, color=:blue2)\ncontour3d!(+zs, levels=  .025:0.05:.475,  linewidth=2, color=:red2)\n\nfig","category":"page"},{"location":"append/plotting/#Surface-plot","page":"Plotting recipes with CairoMakie","title":"Surface plot","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"In Makie.jl, a surface plot is a three-dimensional plot that displays a surface defined by a grid of x, y, and z values. It's used to visualize data that changes over two independent variables, much like a topographical map or a view of a landscape.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\nxs = LinRange(0, 10, 100)\nys = LinRange(0, 15, 100)\nzs = [cos(x) * sin(y) for x in xs, y in ys]\n\nsurface(xs, ys, zs, axis=(type=Axis3,))","category":"page"},{"location":"append/plotting/#Colorbar-of-heatmap/contour","page":"Plotting recipes with CairoMakie","title":"Colorbar of heatmap/contour","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"This Julia code demonstrates how to create heatmaps and contour plots with colorbars using CairoMakie. It first defines a range of x and y values and calculates a corresponding z value for each (x, y) pair. It then creates four subplots: two heatmaps and two contour plots, each with different color maps and level settings. A colorbar is added to each subplot for reference. The heatmap, contourf, and Colorbar functions are used to create the plots and colorbars.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\n\nxs = LinRange(0, 20, 50)\nys = LinRange(0, 15, 50)\n# Calculate z-values \nzs = [cos(x) * sin(y) for x in xs, y in ys]\n\nfig = Figure()\n# Create a heatmap at the first position of the figure and add a colorbar\nax, hm = heatmap(fig[1, 1][1, 1], xs, ys, zs)\nColorbar(fig[1, 1][1, 2], hm)\n# Create a second heatmap at the second position of the figure with a grayscale colormap\nax, hm = heatmap(fig[1, 2][1, 1], xs, ys, zs, colormap = :grays,\n    colorrange = (-0.75, 0.75), highclip = :red, lowclip = :blue)\nColorbar(fig[1, 2][1, 2], hm)\n# Create a filled contour plot at the third position of the figure\nax, hm = contourf(fig[2, 1][1, 1], xs, ys, zs,\n    levels = -1:0.25:1, colormap = :heat)\nColorbar(fig[2, 1][1, 2], hm, ticks = -1:0.25:1)\n# Create a second filled contour plot\nax, hm = contourf(fig[2, 2][1, 1], xs, ys, zs,\n    colormap = :Spectral, levels = [-1, -0.5, -0.25, 0, 0.25, 0.5, 1])\nColorbar(fig[2, 2][1, 2], hm, ticks = -1:0.25:1)\n\nfig","category":"page"},{"location":"append/plotting/#Quiver-Plot","page":"Plotting recipes with CairoMakie","title":"Quiver Plot","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"An arrow plot, also known as a quiver plot, is a type of plot that displays vector fields. This means it shows the direction and magnitude (strength) of data at different points in space. In these plots, each arrow represents a vector and points in the direction the vector is heading. The length (or color) of the arrow can also represent the magnitude of the vector.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"The following examples demonstrate how to create 2D and 3D arrows using the CairoMakie library. ","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\n\nfig = Figure(size = (800, 800))\nAxis(fig[1, 1], backgroundcolor = \"black\")\n\nxs = LinRange(0, 2pi, 20)\nys = LinRange(0, 3pi, 20)\n# Calculate the u-component of the vectors as the sine of x times the cosine of y for each combination of x and y\nus = [sin(x) * cos(y) for x in xs, y in ys]\n# Calculate the v-component of the vectors as the negative cosine of x times the sine of y for each combination of x and y\nvs = [-cos(x) * sin(y) for x in xs, y in ys]\n# Calculate the strength (magnitude) of each vector as the square root of the sum of squares of its u and v components\nstrength = vec(sqrt.(us .^ 2 .+ vs .^ 2))\n# Create a quiver plot with the calculated vectors, specified arrow size and length scale, and color the arrows based on their strength\narrows!(xs, ys, us, vs, arrowsize = 10, lengthscale = 0.3,\n    arrowcolor = strength, linecolor = strength)\n\nfig","category":"page"},{"location":"append/plotting/#3D-Quiver-Plot","page":"Plotting recipes with CairoMakie","title":"3D Quiver Plot","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\nusing LinearAlgebra\n\n# Create a list of 3D points from -5 to 5 in steps of 2 for x, y, and z coordinates\nps = [Point3f(x, y, z) for x in -5:2:5 for y in -5:2:5 for z in -5:2:5]\n# Calculate the direction vectors for each point by swapping the coordinates and scaling by 0.1\nns = map(p -> 0.1 * Vec3f(p[2], p[3], p[1]), ps)\n# Calculate the length (norm) of each direction vector\nlengths = norm.(ns)\n# Create a quiver plot with the calculated points and vectors, turn on anti-aliasing,color the arrows based on their length\n# Specify the line width and arrow size, align the arrows at the center, and create a 3D axis \narrows(\n    ps, ns, fxaa=true, # turn on anti-aliasing\n    color=lengths,\n    linewidth = 0.1, arrowsize = Vec3f(0.3, 0.3, 0.4),\n    align = :center, axis=(type=Axis3,)\n)","category":"page"},{"location":"append/plotting/#Streamplot","page":"Plotting recipes with CairoMakie","title":"Streamplot","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"A streamplot is a type of plot used in fluid dynamics to visualize the flow of a fluid. It shows the direction and magnitude of the flow at different points in space. In a streamplot, the flow is represented by a series of lines that follow the direction of the flow. The density of the lines indicates the speed of the flow, with denser lines indicating faster flow.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\n\n# Define a struct to represent the Fitzhugh-Nagumo model, with parameters ϵ, s, γ, and β\nstruct FitzhughNagumo{T}\n    ϵ::T\n    s::T\n    γ::T\n    β::T\nend\n# Create an instance of the FitzhughNagumo struct with specific parameter values\nP = FitzhughNagumo(0.1, 0.0, 1.5, 0.8)\n# Define a function to represent the Fitzhugh-Nagumo model\nf(x, P::FitzhughNagumo) = Point2f(\n    (x[1]-x[2]-x[1]^3+P.s)/P.ϵ,\n    P.γ*x[1]-x[2] + P.β\n)\n# Define a function to represent the Fitzhugh-Nagumo model with the specific parameter values\nf(x) = f(x, P)\n# Create a streamplot of the Fitzhugh-Nagumo model in both x and y directions, with the magma colormap\nfig, ax, pl = streamplot(f, -1.5..1.5, -1.5..1.5, colormap = :magma)\n# Add another streamplot to the figure, with the color set to a function that returns an RGBA color with the alpha channel set to 1\nstreamplot(fig[1,2], f, -1.5 .. 1.5, -1.5 .. 1.5, color=(p)-> RGBAf(p..., 0.0, 1))\nfig","category":"page"},{"location":"append/plotting/#Animation","page":"Plotting recipes with CairoMakie","title":"Animation","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"In Makie.jl, animation is a feature that allows you to create a sequence of frames, each of which is a different plot, and then combine them into a single animated file. This is useful for visualizing changes in data over time or the progression of an algorithm.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\nusing CairoMakie.Colors\n\nfig, ax, lineplot = lines(0..10, sin; linewidth=10)\n\n# animation settings\nnframes = 30\nframerate = 30\n# Create an iterator for the hue values that will be used to change the color of the line\nhue_iterator = range(0, 360, length=nframes)\n# Start recording the animation\nrecord(fig, \"color_animation.mp4\", hue_iterator;\n        framerate = framerate) do hue\n    lineplot.color = HSV(hue, 1, 0.75)\nend","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"(Image: )","category":"page"},{"location":"append/plotting/#Animation-using-Observables","page":"Plotting recipes with CairoMakie","title":"Animation using Observables","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"Often, you want to animate a complex plot over time, and all the data that is displayed should be determined by the current time stamp. Such a dependency is really easy to express with Observables.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"We can save a lot of work if we create our data depending on a single time Observable, so we don't have to change every plot's data manually as the animation progresses.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"Here is an example that plots two different functions. The y-values of each depend on time and therefore we only have to change the time for both plots to change. We use the convenient @lift macro which denotes that the lifted expression depends on each Observable marked with a $sign.","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"using CairoMakie\ntime = Observable(0.0)\n\nxs = range(0, 7, length=40)\n\nys_1 = @lift(sin.(xs .- $time))\nys_2 = @lift(cos.(xs .- $time) .+ 3)\n# Create a line plot for the sine wave with a dynamic title\nfig = lines(xs, ys_1, color = :blue, linewidth = 4,\n    axis = (title = @lift(\"t = $(round($time, digits = 1))\"),))\n# Add a scatter plot for the cosine wave to the same figure\nscatter!(xs, ys_2, color = :red, markersize = 15)\n# Set the framerate and the timestamps for the animation\nframerate = 10\ntimestamps = range(0, 2, step=1/framerate)\n# Start recording the animation\nrecord(fig, \"time_animation.gif\", timestamps;\n        framerate = framerate) do t\n    time[] = t\nend","category":"page"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"(Image: )","category":"page"},{"location":"append/plotting/#More","page":"Plotting recipes with CairoMakie","title":"More","text":"","category":"section"},{"location":"append/plotting/","page":"Plotting recipes with CairoMakie","title":"Plotting recipes with CairoMakie","text":"For more information on plotting with Makie, please refer to the official documentation.","category":"page"},{"location":"chap2/julia-array/#Array-and-Broadcasting","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"","category":"section"},{"location":"chap2/julia-array/#Array-indexing-and-broadcasting","page":"Array and Broadcasting","title":"Array indexing and broadcasting","text":"","category":"section"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"Julia array can be initialized with multiple ways.","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"A = [1, 2, 3]; # a vector\nB = [1 2 3; 4 5 6; 7 8 9];  # a matrix\nzero_vector = zeros(3); # zero vector\nrand_vector = randn(Float32, 3, 3); # random normal distribution\nstep_vector = collect(1:3);  # collect from a range\nuninitialized_vector = Vector{Int}(undef, 3); # uninitialized vector of size 3","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"Julia array indexing starts from 1, which is different from C, Python, and R. 😞","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"A = [1, 2, 3];\nA[1]     # the first element\nA[end]   # the last element\nA[1:2]   # the first two elements\nA[2:-1:1] # the first two elements in the reversed order","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"B = [1 2 3; 4 5 6; 7 8 9];\nB[1:2]   # the first two elements, returns B[1,1] and B[2,1] since B is column-major\nB[1:2, 1:2] # returns a submatrix","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"Julia has a powerful broadcasting mechanism. It is a way to apply a function to each element of an array. The broadcasting is done by adding a dot . before the function name.","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"x = 0:0.1π:2π\ny = sin.(x) .+ cos.(3 .* x);","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"The broadcasting also does the loop fusion, which means only one loop is used to iterate over the elements of the array and no intermediate array is created. This is often more efficient than the step-by-step loop.","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"We can use Ref to protect an object from being broadcasted.","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"Ref([3,2,1,0]) .* (1:3)","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"We can see the vector is treated as a whole.","category":"page"},{"location":"chap2/julia-array/#Julia-array-is-column-major","page":"Array and Broadcasting","title":"Julia array is column-major","text":"","category":"section"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"In Julia, arrays are stored in column-major order. This may affect the performance of the code.","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"<img src=\"../../assets/images/colmajor.png\" alt=\"column-major\" width=\"200\"/>","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"For example, we can implement the Frobenius norm of a matrix as follows.","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"function frobenius_norm(A::AbstractMatrix)\n    s = zero(eltype(A))\n    # the `@inbounds` macro tells the compiler that the loop is safe and it can skip the boundary check.\n    @inbounds for i in 1:size(A, 1)\n        for j in 1:size(A, 2)\n            s += A[i, j]^2\n        end\n    end\n    return sqrt(s)\nend","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"A = randn(1000, 1000);\nfrobenius_norm(A)","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"julia> using BenchmarkTools\n\njulia> @benchmark frobenius_norm($A)\nBenchmarkTools.Trial: 25 samples with 1 evaluation.\n Range (min … max):  203.310 ms … 214.439 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     204.769 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   205.331 ms ±   2.247 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n  ▃   ▃▃███▃ ▃                                                   \n  █▁▇▁██████▇█▁▁▇▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇ ▁\n  203 ms           Histogram: frequency by time          214 ms <\n\n Memory estimate: 0 bytes, allocs estimate: 0.","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"Alternatively, we can loop over the second index first.","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"function frobenius_norm_colmajor(A::AbstractMatrix)\n    s = zero(eltype(A))\n    @inbounds for j in 1:size(A, 2)\n        for i in 1:size(A, 1)\n            s += A[i, j]^2\n        end\n    end\n    return sqrt(s)\nend","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"julia> @benchmark frobenius_norm_colmajor($A)\nBenchmarkTools.Trial: 53 samples with 1 evaluation.\n Range (min … max):  90.380 ms … 133.823 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     92.729 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   94.415 ms ±   6.425 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n      ▂ ▂█ ▄   ▂                                                \n  ▆██▄█▆████▆▆▄█▄▁▁▁▁▄▄▄▁▁▁▁▁▁▁▁▁▄▁▁▁▄▁▁▁▁▄▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄ ▁\n  90.4 ms         Histogram: frequency by time          108 ms <\n\n Memory estimate: 0 bytes, allocs estimate: 0.","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"We can see by simply changing the order of the loop, the performance is improved by more than 2 times. This is because the memory access pattern is more cache-friendly.","category":"page"},{"location":"chap2/julia-array/#Example:-create-a-meshgrid-for-triangular-lattice","page":"Array and Broadcasting","title":"Example: create a meshgrid for triangular lattice","text":"","category":"section"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"b1 = [1, 0]\nb2 = [0.5, sqrt(3)/2]\nn = 5\nmesh1 = [i * b1 + j * b2 for i in 1:n, j in 1:n]  # list comprehension\nmesh2= (1:n) .* Ref(b1) .+ (1:n)' .* Ref(b2)  # broadcasting","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"using CairoMakie\n\nscatter(vec(getindex.(mesh2, 1)), vec(getindex.(mesh2, 2)), label=\"mesh2\", ratio=1, markersize=10)","category":"page"},{"location":"chap2/julia-array/#Benchmark:-matrix-multiplication","page":"Array and Broadcasting","title":"Benchmark: matrix multiplication","text":"","category":"section"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"Matrix multiplication is a fundamental operation in scientific computing. Julia's built-in * operator is backed by highly optimized BLAS libraries. Let's benchmark the performance of matrix multiplication.","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"julia> @benchmark $A * $B\nBenchmarkTools.Trial: 383 samples with 1 evaluation.\n Range (min … max):  12.089 ms … 38.311 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     12.873 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   13.052 ms ±  1.418 ms  ┊ GC (mean ± σ):  1.20% ± 3.41%\n\n       ▄▆▅ ▂▄▇█▇▅▅▄                                            \n  ▆▁▁▁▁████████████▆▆▁▄▁▁▄▁▄▁▆▆▇▄▇▄▇█▇▇▁▁▇▁▆▁▆▄▁▄▁▁▁▁▁▁▁▁▁▁▁▄ ▇\n  12.1 ms      Histogram: log(frequency) by time      15.8 ms <\n\n Memory estimate: 7.63 MiB, allocs estimate: 2.","category":"page"},{"location":"chap2/julia-array/","page":"Array and Broadcasting","title":"Array and Broadcasting","text":"The performance of a CPU is measured by the number of floating point operations per second (FLOPS) it can perform. The floating point operations include addition, subtraction, multiplication and division. The FLOPS can be related to multiple factors, such as the clock frequency, the number of cores, the number of instructions per cycle, and the number of floating point units. A simple way to measure the FLOPS is to benchmarking the speed of matrix multiplication. The number of FLOPS in a ntimes ntimes n matrix multiplication is 2n^3. The FLOPS can be calculated as: 2 times 1000^3  (12089 times 10^-3) approx 165rm GFLOPS.","category":"page"},{"location":"chap3/qr/#QR-Factorization:-Bottom-up","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"","category":"section"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"The QR factorization of a matrix A in mathbbR^mtimes n is a factorization of the form","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"A = QR","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"where Q in mathbbR^mtimes m is an orthogonal matrix and R in mathbbR^mtimes n is an upper triangular matrix.","category":"page"},{"location":"chap3/qr/#Householder-Reflection","page":"QR Factorization: Bottom-up","title":"Householder Reflection","text":"","category":"section"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"Let v in mathbbR^m be nonzero, An m-by-m matrix P of the form","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"P = 1-beta vv^T beta = frac2v^Tv","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"is a Householder reflection, which is both symmetric and orthogonal. Suppose we want to project a vector x to e_1, i.e. Px = beta e_1. Then we can choose","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"beginalign*\nv = x pm x_2 e_1\nH = I - beta vv^T\nendalign*","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"Let us define a Householder matrix in Julia.","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"struct HouseholderMatrix{T} <: AbstractArray{T, 2}\n    v::Vector{T}\n    β::T\nend\nfunction HouseholderMatrix(v::Vector{T}) where T\n    HouseholderMatrix(v, 2/norm(v, 2)^2)\nend\n\n# array interfaces\nBase.size(A::HouseholderMatrix) = (length(A.v), length(A.v))\nBase.size(A::HouseholderMatrix, i::Int) = i == 1 || i == 2 ? length(A.v) : 1\nfunction Base.getindex(A::HouseholderMatrix, i::Int, j::Int)\n    (i == j ? 1 : 0) - A.β * A.v[i] * conj(A.v[j])\nend\n\n# Householder matrix is unitary\nBase.inv(A::HouseholderMatrix) = A\n# Householder matrix is Hermitian\nBase.adjoint(A::HouseholderMatrix) = A\n\n# Left and right multiplication\nfunction left_mul!(B, A::HouseholderMatrix)\n    B .-= (A.β .* A.v) * (A.v' * B)\n    return B\nend\nfunction right_mul!(A, B::HouseholderMatrix)\n    A .= A .- (A * (B.β .* B.v)) * B.v'\n    return A\nend","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"In this example, we define a HouseholderMatrix type, which is a subtype of AbstractArray. The v field is the vector v and the β field is the scalar beta. To define the array interfaces, we need to define the size and getindex functions. Please check the Julia manual for more details.","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"using LinearAlgebra, Test\n@testset \"householder property\" begin\n    v = randn(3)\n    H = HouseholderMatrix(v)\n    # symmetric\n    @test H' ≈ H\n    # reflexive\n    @test H^2 ≈ I\n    # orthogonal\n    @test H' * H ≈ I\nend","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"Let us define a function to compute the Householder matrix that projects a vector to e_1.","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"function householder_e1(v::AbstractVector{T}) where T\n    v = copy(v)\n    v[1] -= norm(v, 2)\n    return HouseholderMatrix(v, 2/norm(v, 2)^2)\nend","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"A = Float64[1 2 2; 4 4 2; 4 6 4]\nhm = householder_e1(view(A,:,1))\nhm * A","category":"page"},{"location":"chap3/qr/#QR-factoriaztion-by-Householder-reflection.","page":"QR Factorization: Bottom-up","title":"QR factoriaztion by Householder reflection.","text":"","category":"section"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"Let H_k be a Householder reflection that zeros out the k-th column below the diagonal. Then we have","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"H_n ldots H_2H_1 A = R","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"where R is an upper triangular matrix. Then we can define the Q matrix as","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"Q = H_1^T H_2 ^Tldots H_n^T","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"which is a unitary matrix.","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"function householder_qr!(Q::AbstractMatrix{T}, a::AbstractMatrix{T}) where T\n    m, n = size(a)\n    @assert size(Q, 2) == m\n    if m == 1\n        return Q, a\n    else\n        # apply householder matrix\n        H = householder_e1(view(a, :, 1))\n        left_mul!(a, H)\n        # update Q matrix\n        right_mul!(Q, H')\n        # recurse\n        householder_qr!(view(Q, 1:m, 2:m), view(a, 2:m, 2:n))\n    end\n    return Q, a\nend","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"@testset \"householder QR\" begin\n    A = randn(3, 3)\n    Q = Matrix{Float64}(I, 3, 3)\n    R = copy(A)\n    householder_qr!(Q, R)\n    @info R\n    @test Q * R ≈ A\n    @test Q' * Q ≈ I\nend\n\nA = randn(3, 3)\ng = givens_matrix(A, 2, 3)\nleft_mul!(copy(A), g)","category":"page"},{"location":"chap3/qr/#Givens-Rotations","page":"QR Factorization: Bottom-up","title":"Givens Rotations","text":"","category":"section"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"G = left(beginmatrix\ncostheta  -sintheta\nsintheta  costheta\nendmatrixright)","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"rotation_matrix(angle) = [cos(angle) -sin(angle); sin(angle) cos(angle)]","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"angle = π/4\ninitial_vector = [1.0, 0.0]\nfinal_vector = rotation_matrix(angle) * initial_vector\n# eliminating the y element\natan(0.1, 0.5)\ninitial_vector = randn(2)\nangle = atan(initial_vector[2], initial_vector[1])\nfinal_vector = rotation_matrix(-angle) * initial_vector","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"left(\nbeginmatrix\n1  0  0  0  0\n0  c  0  s  0\n0  0  1  0  0\n0  -s  0  c  0\n0  0  0  0  1\nendmatrix\nright)\nleft(\nbeginmatrix\na_1a_2a_3a_4a_5\nendmatrix\nright)=\nleft(\nbeginmatrix\na_1alphaa_30a_5\nendmatrix\nright)","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"where s = sin(theta) and c = cos(theta).","category":"page"},{"location":"chap3/qr/#QR-Factorization-by-Givens-Rotations","page":"QR Factorization: Bottom-up","title":"QR Factorization by Givens Rotations","text":"","category":"section"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"struct GivensMatrix{T} <: AbstractArray{T, 2}\n    c::T\n    s::T\n    i::Int\n    j::Int\n    n::Int\nend\n\nBase.size(g::GivensMatrix) = (g.n, g.n)\nBase.size(g::GivensMatrix, i::Int) = i == 1 || i == 2 ? g.n : 1\nfunction Base.getindex(g::GivensMatrix{T}, i::Int, j::Int) where T\n    @boundscheck i <= g.n && j <= g.n\n    if i == j\n        return i == g.i || i == g.j ? g.c : one(T)\n    elseif i == g.i && j == g.j\n        return g.s\n    elseif i == g.j && j == g.i\n        return -g.s\n    else\n        return i == j ? one(T) : zero(T)\n    end\nend\n\nfunction left_mul!(A::AbstractMatrix, givens::GivensMatrix)\n    for col in 1:size(A, 2)\n        vi, vj = A[givens.i, col], A[givens.j, col]\n        A[givens.i, col] = vi * givens.c + vj * givens.s\n        A[givens.j, col] = -vi * givens.s + vj * givens.c\n    end\n    return A\nend\nfunction right_mul!(A::AbstractMatrix, givens::GivensMatrix)\n    for row in 1:size(A, 1)\n        vi, vj = A[row, givens.i], A[row, givens.j]\n        A[row, givens.i] = vi * givens.c + vj * givens.s\n        A[row, givens.j] = -vi * givens.s + vj * givens.c\n    end\n    return A\nend","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"function givens_matrix(A, i, j)\n    x, y = A[i, 1], A[j, 1]\n    norm = sqrt(x^2 + y^2)\n    c = x/norm\n    s = y/norm\n    return GivensMatrix(c, s, i, j, size(A, 1))\nend","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"function givens_qr!(Q::AbstractMatrix, A::AbstractMatrix)\n    m, n = size(A)\n    if m == 1\n        return Q, A\n    else\n        for k = m:-1:2\n            g = givens_matrix(A, k-1, k)\n            left_mul!(A, g)\n            right_mul!(Q, g)\n        end\n        givens_qr!(view(Q, :, 2:m), view(A, 2:m, 2:n))\n        return Q, A\n    end\nend","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"@testset \"givens QR\" begin\n    n = 3\n    A = randn(n, n)\n    R = copy(A)\n    Q, R = givens_qr!(Matrix{Float64}(I, n, n), R)\n    @test Q * R ≈ A\n    @test Q * Q' ≈ I\n    @info R\nend","category":"page"},{"location":"chap3/qr/#Gram-Schmidt-Orthogonalization","page":"QR Factorization: Bottom-up","title":"Gram-Schmidt Orthogonalization","text":"","category":"section"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"The Gram-Schmidt orthogonalization is a method to compute the QR factorization of a matrix A by constructing an orthogonal matrix Q and an upper triangular matrix R.","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"q_k = left(a_k - sum_i=1^k-1 r_ikq_iright)r_kk","category":"page"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"function classical_gram_schmidt(A::AbstractMatrix{T}) where T\n    m, n = size(A)\n    Q = zeros(T, m, n)\n    R = zeros(T, n, n)\n    R[1, 1] = norm(view(A, :, 1))\n    Q[:, 1] .= view(A, :, 1) ./ R[1, 1]\n    for k = 2:n\n        Q[:, k] .= view(A, :, k)\n        # project z to span(A[:, 1:k-1])⊥\n        for j = 1:k-1\n            R[j, k] = view(Q, :, j)' * view(A, :, k)\n            Q[:, k] .-= view(Q, :, j) .* R[j, k]\n        end\n        # normalize the k-th column\n        R[k, k] = norm(view(Q, :, k))\n        Q[:, k] ./= R[k, k]\n    end\n    return Q, R\nend\n\n@testset \"classical GS\" begin\n    n = 10\n    A = randn(n, n)\n    Q, R = classical_gram_schmidt(A)\n    @test Q * R ≈ A\n    @test Q * Q' ≈ I\n    @info R\nend","category":"page"},{"location":"chap3/qr/#Modified-Gram-Schmidt-Orthogonalization","page":"QR Factorization: Bottom-up","title":"Modified Gram-Schmidt Orthogonalization","text":"","category":"section"},{"location":"chap3/qr/","page":"QR Factorization: Bottom-up","title":"QR Factorization: Bottom-up","text":"function modified_gram_schmidt!(A::AbstractMatrix{T}) where T\n    m, n = size(A)\n    Q = zeros(T, m, n)\n    R = zeros(T, n, n)\n    for k = 1:n\n        R[k, k] = norm(view(A, :, k))\n        Q[:, k] .= view(A, :, k) ./ R[k, k]\n        for j = k+1:n\n            R[k, j] = view(Q, :, k)' * view(A, :, j)\n            A[:, j] .-= view(Q, :, k) .* R[k, j]\n        end\n    end\n    return Q, R\nend\n\n@testset \"modified GS\" begin\n    n = 10\n    A = randn(n, n)\n    Q, R = modified_gram_schmidt!(copy(A))\n    @test Q * R ≈ A\n    @test Q * Q' ≈ I\n    @info R\nend\n\nlet\n    n = 100\n    A = randn(n, n)\n    Q1, R1 = classical_gram_schmidt(A)\n    Q2, R2 = modified_gram_schmidt!(copy(A))\n    @info norm(Q1' * Q1 - I)\n    @info norm(Q2' * Q2 - I)\nend","category":"page"},{"location":"#Scientific-Computing-for-Physicists","page":"Home","title":"Scientific Computing for Physicists","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Who-should-read-this-book?","page":"Home","title":"Who should read this book?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is a book for those who aim to become professional scientific computing programmers. Before reading it, please make sure","category":"page"},{"location":"","page":"Home","title":"Home","text":"the problem you are trying to solve runs more than 10min,\nyou want to become a tool builder rather than tools user, and you are happy with writing the tool with the Julia programming language.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The demos in this book could be found in the GitHub repository: GiggleLiu/ScientificComputingDemos.","category":"page"}]
}
